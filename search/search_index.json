{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Home"},{"location":"#pet-python-ensemble-toolbox","title":"PET: Python Ensemble Toolbox","text":"<p>PET is a toolbox for ensemble-based Data Assimilation and Optimisation. It is developed and maintained by the eponymous group at NORCE Norwegian Research Centre AS.</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>Before installing ensure you have python3 pre-requisites. On a Debian system run:</p> <pre><code>sudo upt-get update\nsudo apt-get install python3\nsudo apt-get install python3-pip\nsudo apt-get install python3-venv\n</code></pre> <p>To install PET, first clone the repo (assuming you have added the SSH key)</p> <pre><code>git clone git@github.com:Python-Ensemble-Toolbox/PET.git PET\n</code></pre> <p>Make sure you have the latest version of <code>pip</code> and <code>setuptools</code>:</p> <pre><code>python3 -m pip install --upgrade pip setuptools\n</code></pre> <p>Optionally (but recommended): Create and activate a virtual environment:</p> <pre><code>python3 -m venv venv-PET\nsource venv-PET/bin/activate\n</code></pre> <p>Some additional features might be not part of your default installation and need to be set in the Python (virtual) environment manually:</p> <pre><code>python3 -m pip install wheel\npython3 setup.py bdist_wheel\n</code></pre> <p>If you do not install PET inside a virtual environment, you may have to include the <code>--user</code> option in the following (to install to your local Python site packages, usually located in <code>~/.local</code>).</p> <p>Inside the PET folder, run</p> <pre><code>python3 -m pip install -e .\n</code></pre> <ul> <li>The dot is needed to point to the current directory.</li> <li>The <code>-e</code> option installs PET such that changes to it take effect immediately   (without re-installation).</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>PET needs to be set up with a configuration file. See the example folder for inspiration.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li>A PIPT tutorial is found here</li> <li>A POPT tutorial is found here</li> </ul>"},{"location":"#suggested-readings","title":"Suggested readings:","text":"<p>If you use PET in a scientific publication, we would appreciate it if you cited one of the first papers where the PET was introduced. Each of them describes some of the PET's functionalities:</p>"},{"location":"#bayesian-data-assimilation-with-enrml-and-es-mda-for-history-matching-workflow-with-ai-geomodeling","title":"Bayesian data assimilation with EnRML and ES-MDA for History-Matching Workflow with AI-Geomodeling","text":""},{"location":"#cite-as","title":"Cite as","text":"<p>Fossum, Kristian, Sergey Alyaev, and Ahmed H. Elsheikh. \"Ensemble history-matching workflow using interpretable SPADE-GAN geomodel.\" First Break 42.2 (2024): 57-63. https://doi.org/10.3997/1365-2397.fb2024014</p> <pre><code>@article{fossum2024ensemble,\n  title={Ensemble history-matching workflow using interpretable SPADE-GAN geomodel},\n  author={Fossum, Kristian and Alyaev, Sergey and Elsheikh, Ahmed H},\n  journal={First Break},\n  volume={42},\n  number={2},\n  pages={57--63},\n  year={2024},\n  publisher={European Association of Geoscientists \\&amp; Engineers},\n  url = {https://doi.org/10.3997/1365-2397.fb2024014}\n}\n</code></pre>"},{"location":"#bayesian-inversion-technique-localization-and-data-compression-for-history-matching-of-the-edvard-grieg-field-using-4d-seismic-data","title":"Bayesian inversion technique, localization, and data compression for history matching of the Edvard Grieg field using 4D seismic data","text":""},{"location":"#cite-as_1","title":"Cite as","text":"<p>Lorentzen, R.J., Bhakta, T., Fossum, K. et al. Ensemble-based history matching of the Edvard Grieg field using 4D seismic data. Comput Geosci 28, 129\u2013156 (2024). https://doi.org/10.1007/s10596-024-10275-0</p> <pre><code>@article{lorentzen2024ensemble,\n  title={Ensemble-based history matching of the Edvard Grieg field using 4D seismic data},\n  author={Lorentzen, Rolf J and Bhakta, Tuhin and Fossum, Kristian and Haugen, Jon Andr{\\'e} and Lie, Espen Oen and Ndingwan, Abel Onana and Straith, Knut Richard},\n  journal={Computational Geosciences},\n  volume={28},\n  number={1},\n  pages={129--156},\n  year={2024},\n  publisher={Springer},\n  url={https://doi.org/10.1007/s10596-024-10275-0}\n}\n</code></pre>"},{"location":"#offshore-wind-farm-layout-optimization-using-ensemble-methods","title":"Offshore wind farm layout optimization using ensemble methods","text":""},{"location":"#cite-as_2","title":"Cite as","text":"<p>Eikrem, K.S., Lorentzen, R.J., Faria, R. et al. Offshore wind farm layout optimization using ensemble methods. Renewable Energy 216, 119061 (2023). https://www.sciencedirect.com/science/article/pii/S0960148123009758</p> <pre><code>@article{Eikrem2023offshore,\ntitle = {Offshore wind farm layout optimization using ensemble methods},\njournal = {Renewable Energy},\nvolume = {216},\npages = {119061},\nyear = {2023},\nissn = {0960-1481},\ndoi = {https://doi.org/10.1016/j.renene.2023.119061},\nurl = {https://www.sciencedirect.com/science/article/pii/S0960148123009758},\nauthor = {Kjersti Solberg Eikrem and Rolf Johan Lorentzen and Ricardo Faria and Andreas St{\\o}rksen Stordal and Alexandre Godard},\nkeywords = {Wind farm layout optimization, Ensemble optimization (EnOpt and EPF-EnOpt), Constrained optimization, Levelized cost of energy (LCOE), Floating offshore wind},\n}\n</code></pre>"},{"location":"dev_guide/","title":"Developer guide","text":""},{"location":"dev_guide/#writing-documentation","title":"Writing documentation","text":"<p>The documentation is built with <code>mkdocs</code>.</p> <ul> <li>It should be written in the syntax of markdown.</li> <li>The syntax is further augmented by several pymdown plugins.</li> <li>Docstrings are processed as above, but should also   declare parameters and return values in the style of numpy,   and <code>&gt;&gt;&gt;</code> markers must follow the \"Examples\" section.</li> </ul> <p>Note</p> <p>You can preview the rendered html docs by running <pre><code>mkdocs serve\n</code></pre></p> <ul> <li>Temporarily disable <code>mkdocs-jupyter</code> in <code>mkdocs.yml</code> to speed up build reloads.</li> <li>Set <code>validation: unrecognized_links: warn</code> to get warnings about linking issues.</li> </ul> <p>A summary of how to add cross-reference links is given below.</p>"},{"location":"dev_guide/#linking-to-pages","title":"Linking to pages","text":"<p>You should use relative page links, including the <code>.md</code> extension. For example, <code>[link label](sibling-page.md)</code>.</p> <p>The following works, but does not get validated! <code>[link label](../sibling-page)</code></p> <p>Why not absolute links?</p> <p>The downside of relative links is that if you move/rename source or destination, then they will need to be changed, whereas only the destination needs be watched when using absolute links.</p> <p>Previously, absolute links were not officially supported by MkDocs, meaning \"not modified at all\". Thus, if made like so <code>[label](/PET/references)</code>, i.e. without <code>.md</code> and including <code>/PET</code>, then they would work (locally with <code>mkdocs serve</code> and with GitHub hosting). Since #3485 you can instead use <code>[label](/references)</code> i.e. omitting <code>PET</code> (or whatever domain sub-dir is applied in <code>site_url</code>) by setting <code>mkdocs.yml: validation: absolute_links: relative_to_docs</code>. A different workaround is the <code>mkdocs-site-url</code> plugin.</p> <p>Either way</p> <p>It will not be link that your editor can follow to the relevant markdown file (unless you create a symlink in your file system root?) nor will GitHub's internal markdown rendering manage to make sense of it, so my advise is not to use absolute links.</p>"},{"location":"dev_guide/#linking-to-headersanchors","title":"Linking to headers/anchors","text":"<p>Thanks to the <code>autorefs</code> plugin, links to headings (including page titles) don't even require specifying the page path! Syntax: <code>[visible label][link]</code> i.e. double pairs of brackets. Shorthand: <code>[link][]</code>.</p> <p>Info</p> <ul> <li>Clearly, non-unique headings risk being confused with others in this way.</li> <li>The link (anchor) must be lowercase!</li> </ul> <p>This facilitates linking to</p> <ul> <li>API (code reference) items.   For example, <code>[`da_methods.ensemble`][]</code>,   where the backticks are optional (makes the link look like a code reference).</li> <li>References. For example <code>[`bocquet2016`][]</code>,</li> </ul>"},{"location":"dev_guide/#docstring-injection","title":"Docstring injection","text":"<p>Use the following syntax to inject the docstring of a code object.</p> <pre><code>::: da_methods.ensemble\n</code></pre> <p>But we generally don't do so manually. Instead it's taken care of by the reference generation via <code>docs/gen_ref_pages.py</code>.</p>"},{"location":"dev_guide/#including-other-files","title":"Including other files","text":"<p>The <code>pymdown</code> extension \"snippets\" enables the following syntax to include text from other files.</p> <p><code>--8&lt;-- \"/path/from/project/root/filename.ext\"</code></p>"},{"location":"dev_guide/#adding-to-the-examples","title":"Adding to the examples","text":"<p>Example scripts are very useful, and contributions are very desirable.  As well as showcasing some feature, new examples should make sure to reproduce some published literature results.  After making the example, consider converting the script to the Jupyter notebook format (or vice versa) so that the example can be run on Colab without users needing to install anything (see <code>docs/examples/README.md</code>). This should be done using the <code>jupytext</code> plug-in (with the <code>lightscript</code> format), so that the paired files can be kept in synch.</p>"},{"location":"dev_guide/#bibliography","title":"Bibliography","text":"<p>In order to add new references, insert their bibtex into <code>docs/bib/refs.bib</code>, then run <code>docs/bib/bib2md.py</code> which will format and add entries to <code>docs/references.md</code> that can be cited with regular cross-reference syntax, e.g. <code>[bocquet2010a][]</code>.</p>"},{"location":"dev_guide/#hosting","title":"Hosting","text":"<p>The above command is run by a GitHub Actions workflow whenever the <code>master</code> branch gets updated. The <code>gh-pages</code> branch is no longer being used. Instead actions/deploy-pages creates an artefact that is deployed to Github Pages.</p>"},{"location":"dev_guide/#tests","title":"Tests","text":"<p>The test suite is orchestrated using <code>pytest</code>. Both in CI and locally. I.e. you can run the tests simply by the command</p> <pre><code>pytest\n</code></pre> <p>It will discover all appropriately named tests in the source (see the <code>tests</code> dir).</p> <p>Use (for example) <code>pytest --doctest-modules some_file.py</code> to also run any example code within docstrings.</p> <p>We should also soon make use of a config file (for example <code>pyproject.toml</code>) for <code>pytest</code>.</p>"},{"location":"references/","title":"Bibliography","text":""},{"location":"references/#emerick2016a","title":"<code>emerick2016a</code>","text":"<p>Alexandre A. Emerick. <code>\"Analysis of the performance of ensemble-based assimilation of production and seismic data\"</code>. Journal of Petroleum Science and Engineering, 139:219\u2013239, 2016.</p>"},{"location":"references/#evensen2009a","title":"<code>evensen2009a</code>","text":"<p>Geir Evensen. Data Assimilation. Springer, 2 edition, 2009.</p>"},{"location":"references/#emerick2013a","title":"<code>emerick2013a</code>","text":"<p>Alexandre A. Emerick and Albert C. Reynolds. <code>\"Ensemble smoother with multiple data assimilation\"</code>. Computers &amp; Geosciences, 55:3\u201315, 2013.</p>"},{"location":"references/#rafiee2017","title":"<code>rafiee2017</code>","text":"<p>Javad Rafiee and Albert C Reynolds. <code>\"Theoretical and efficient practical procedures for the generation of inflation factors for es-mda\"</code>. Inverse Problems, 33(11):115003, 2017.</p>"},{"location":"references/#chen2013","title":"<code>chen2013</code>","text":"<p>Yan Chen and Dean S. Oliver. <code>\"Levenberg\u2013Marquardt forms of the iterative ensemble smoother for efficient history matching and uncertainty quantification\"</code>. Computational Geosciences, 17(4):689\u2013703, 2013.</p>"},{"location":"references/#raanes2019","title":"<code>raanes2019</code>","text":"<p>Patrick Nima Raanes, Andreas St\u00f8rksen Stordal, and Geir Evensen. <code>\"Revising the stochastic iterative ensemble smoother\"</code>. Nonlinear Processes in Geophysics, 26(3):325\u2013338, 2019. doi:10.5194/npg-26-325-2019.</p>"},{"location":"references/#evensen2019","title":"<code>evensen2019</code>","text":"<p>Geir Evensen, Patrick N. Raanes, Andreas S. Stordal, and Joakim Hove. <code>\"Efficient implementation of an iterative ensemble smoother for data assimilation and reservoir history matching\"</code>. Frontiers in Applied Mathematics and Statistics, 5:47, 2019. doi:10.3389/fams.2019.00047.</p>"},{"location":"references/#kingma2014","title":"<code>kingma2014</code>","text":"<p>Diederik P Kingma. <code>\"Adam: a method for stochastic optimization\"</code>. arXiv preprint arXiv:1412.6980, 2014.</p>"},{"location":"references/#hansen2006","title":"<code>hansen2006</code>","text":"<p>Nikolaus Hansen. <code>\"The CMA evolution strategy: a comparing review\"</code>. Towards a new evolutionary computation: Advances in the estimation of distribution algorithms, pages 75\u2013102, 2006.</p>"},{"location":"reference/SUMMARY/","title":"Code reference","text":"<p>Use links in sidebar to navigate the code docstrings.</p>"},{"location":"reference/ensemble/","title":"ensemble","text":"<p>Multiple realisations management.</p>"},{"location":"reference/ensemble/ensemble/","title":"ensemble","text":"<p>Package contains the basis for the PET ensemble based structure.</p>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble","title":"<code>Ensemble</code>","text":"<p>Class for organizing misc. variables and simulator for an ensemble-based inversion run. Here, the forecast step and prediction runs are performed. General methods that are useful in various ensemble loops have also been implemented here.</p>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble.__init__","title":"<code>__init__(keys_en, sim, redund_sim=None)</code>","text":"<p>Class extends the ReadInitFile class. First the PIPT init. file is passed to the parent class for reading and parsing. Rest of the initialization uses the keywords parsed in ReadInitFile (parent) class to set up observed, predicted data and data variance dictionaries. Also, the simulator to be used in forecast and/or predictions is initialized with keywords parsed in ReadInitFile (parent) class. Lastly, the initial ensemble is generated (if it has not been inputted), and some saving of variables can be done chosen in PIPT init. file.</p> Parameter <p>init_file : str             path to input file containing initiallization values</p>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble.calc_ml_prediction","title":"<code>calc_ml_prediction(input_state=None)</code>","text":"<p>Function for running the simulator over several levels. We assume that it is sufficient to provide the level integer to the setup of the forward run. This will initiate the correct simulator fidelity. The function then runs the set of state through the different simulator fidelities.</p> <p>Parameters:</p> Name Type Description Default <code>input_state</code> <p>If simulation is run stand-alone one can input any state.</p> <code>None</code>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble.calc_prediction","title":"<code>calc_prediction(input_state=None, save_prediction=None)</code>","text":"<p>Method for making predictions using the state variable. Will output the simulator response for all report steps and all data values provided to the simulator.</p> <p>Parameters:</p> Name Type Description Default <code>input_state</code> <p>Use an input state instead of internal state (stored in self) to run predictions</p> <code>None</code> <code>save_prediction</code> <p>Save the predictions as a .npz file (numpy compressed file) <code>None</code> <p>Returns:</p> Name Type Description <code>prediction</code> <p>List of dictionaries with keys equal to data types (in DATATYPE), containing the responses at each time step given in PREDICTION.</p>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble.gen_init_ensemble","title":"<code>gen_init_ensemble()</code>","text":"<p>Generate the initial ensemble of (joint) state vectors using the GeoStat class in the \"geostat\" package. TODO: Merge this function with the perturbation function _gen_state_ensemble in popt.</p>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble.get_list_assim_steps","title":"<code>get_list_assim_steps()</code>","text":"<p>Returns list of assimilation steps. Useful in a 'loop'-script.</p> <p>Returns:</p> Name Type Description <code>list_assim</code> <code>list</code> <p>List of total assimilation steps.</p>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble.load","title":"<code>load()</code>","text":"<p>Load a pickled file and save all info. in self.</p> Changelog <ul> <li>ST 28/2-17</li> </ul>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble.save","title":"<code>save()</code>","text":"<p>We use pickle to dump all the information we have in 'self'. Can be used, e.g., if some error has occurred.</p> Changelog <ul> <li>ST 28/2-17</li> </ul>"},{"location":"reference/input_output/","title":"input_output","text":"<p>File-based communication and storage.</p>"},{"location":"reference/input_output/get_ecl_key_val/","title":"get_ecl_key_val","text":"<p>Descriptive description.</p>"},{"location":"reference/input_output/organize/","title":"organize","text":"<p>Descriptive description.</p>"},{"location":"reference/input_output/organize/#input_output.organize.Organize_input","title":"<code>Organize_input</code>","text":""},{"location":"reference/input_output/read_config/","title":"read_config","text":"<p>Parse config files.</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.check_mand_keywords_da","title":"<code>check_mand_keywords_da(keys_da)</code>","text":"<p>Check for mandatory keywords in <code>DATAASSIM</code> part, and output error if they are not present</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.check_mand_keywords_en","title":"<code>check_mand_keywords_en(keys_en)</code>","text":"<p>Check for mandatory keywords in <code>ENSEMBLE</code> part, and output error if they are not present</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.check_mand_keywords_fwd","title":"<code>check_mand_keywords_fwd(keys_fwd)</code>","text":"<p>Check for mandatory keywords in <code>FWDSIM</code> part, and output error if they are not present</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.check_mand_keywords_opt","title":"<code>check_mand_keywords_opt(keys_opt)</code>","text":"<p>Check for mandatory keywords in <code>OPTIM</code> part, and output error if they are not present</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.parse_keywords","title":"<code>parse_keywords(lines)</code>","text":"<p>Here we parse the lines in the init. file to a Python dictionary. The keys of the dictionary is the keywords in the PIPT init. file, and the information in each keyword is stored in each key of the dictionary. To know how the keyword-information is organized in the keys of the dictionary, confront the manual located in the doc folder.</p> <p>Parameters:</p> Name Type Description Default <code>lines</code> <code>list</code> <p>List of (clean) lines from the PIPT init. file.</p> required <p>Returns:</p> Name Type Description <code>keys</code> <code>dict</code> <p>Dictionary with all info. from the init. file.</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.read_clean_file","title":"<code>read_clean_file(init_file)</code>","text":"<p>Read PIPT init. file and lines that are not comments (marked with octothorpe)</p> <p>Parameters:</p> Name Type Description Default <code>init_file</code> <code>str</code> <p>Name of file to remove all comments. WHOLE filename needed (with suffix!)</p> required <p>Returns:</p> Name Type Description <code>lines</code> <code>list</code> <p>Lines from init. file converted to list entries</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.read_toml","title":"<code>read_toml(init_file)</code>","text":"<p>Read .toml configuration file, parse and output dictionaries for PIPT/POPT</p> <p>Parameters:</p> Name Type Description Default <code>init_file</code> <code>str</code> <p>toml configuration file</p> required"},{"location":"reference/input_output/read_config/#input_output.read_config.read_txt","title":"<code>read_txt(init_file)</code>","text":"<p>Read a PIPT or POPT input file (.pipt or .popt), parse and output dictionaries for data assimilation or optimization,  and simulator classes.</p> <p>Parameters:</p> Name Type Description Default <code>init_file</code> <code>str</code> <p>PIPT init. file containing info. to run the inversion algorithm</p> required <p>Returns:</p> Name Type Description <code>keys_pr</code> <code>dict</code> <p>Parsed keywords from DATAASSIM or OPTIM</p> <code>keys_fwd</code> <code>dict</code> <p>Parsed keywords from FWDSSIM</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.read_yaml","title":"<code>read_yaml(init_file)</code>","text":"<p>Read .yaml input file, parse and return dictionaries for PIPT/POPT.</p> <p>Parameters:</p> Name Type Description Default <code>init_file</code> <code>str</code> <p>.yaml file</p> required <p>Returns:</p> Name Type Description <code>keys_da</code> <code>dict</code> <p>Parsed keywords from dataassim</p> <code>keys_fwd</code> <code>dict</code> <p>Parsed keywords from fwdsim</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.remove_empty_lines","title":"<code>remove_empty_lines(lines)</code>","text":"<p>Small method for finding empty lines in a read file.</p> <p>Parameters:</p> Name Type Description Default <code>lines</code> <code>list</code> <p>List of lines from a file</p> required <p>Returns:</p> Name Type Description <code>lines_clean</code> <code>list</code> <p>List of clean lines (without empty entries)</p>"},{"location":"reference/misc/","title":"misc","text":"<p>More tools.</p>"},{"location":"reference/misc/ecl/","title":"ecl","text":"<p>Read Schlumberger Eclipse output files.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase","title":"<code>EclipseCase</code>","text":"<p>               Bases: <code>object</code></p> <p>Read data for an Eclipse simulation case.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.__init__","title":"<code>__init__(casename)</code>","text":"<p>Initialize the Eclipse case.</p> <p>Parameters:</p> Name Type Description Default <code>casename</code> <code>str</code> <p>Path to the case, with or without extension.</p> required <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.at","title":"<code>at(when)</code>","text":"<p>Recurrent data for a certain timestep.</p> <p>The result of this method is usually passed to functions that need to calculate something using several properties.</p> <p>Parameters:</p> Name Type Description Default <code>when</code> <code>datetime or int</code> <p>Date of the property.</p> required <p>Returns:</p> Type Description <code>EclipseRestart</code> <p>Object containing properties for this timestep.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.atsm","title":"<code>atsm(when)</code>","text":"<p>Recurrent data from summary file for a certain timestep.</p> <p>The result of this method is usually passed to functions that need to calculate something using several properties.</p> <p>Parameters:</p> Name Type Description Default <code>when</code> <code>datetime or int</code> <p>Date of the property.</p> required <p>Returns:</p> Type Description <code>EclipseSummary</code> <p>Object containing summary properties for this timestep.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.cell_data","title":"<code>cell_data(prop, when=None)</code>","text":"<p>Read cell-wise data from case. This can be either static information or recurrent information for a certain date.</p> <p>Parameters:</p> Name Type Description Default <code>prop</code> <code>str</code> <p>Name of the property, e.g., 'SWAT'.</p> required <code>when</code> <code>datetime or int</code> <p>Date of the property, or None if static.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Loaded array for the property.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; case = EclipseCase(cmd_args.filename)\n&gt;&gt;&gt; zmf2 = case.cell_data('ZMF2', datetime.datetime(2054, 7, 1))\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.components","title":"<code>components()</code>","text":"<p>Components that exist in the restart file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; case = EclipseCase(filename)\n&gt;&gt;&gt; comps = case.components()\n&gt;&gt;&gt; print(\"Number of components is %d\" % (len(comps)))\n&gt;&gt;&gt; for num, name in enumerate(comps):\n&gt;&gt;&gt;     print(\"%d : %s\" % (num, name))\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.field_data","title":"<code>field_data(prop, when=None)</code>","text":"<p>Read field-wise data from case. This can be either static information or recurrent information for a certain date.</p> <p>Parameters:</p> Name Type Description Default <code>prop</code> <code>str</code> <p>Name of the property, e.g., 'ZPHASE'.</p> required <code>when</code> <code>datetime or int</code> <p>Date of the property, or None if static.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Loaded array for the property.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; case = EclipseCase(cmd_args.filename)\n&gt;&gt;&gt; zphase = case.cell_data('ZPHASE', datetime.datetime(2054, 7, 1))\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.grid","title":"<code>grid()</code>","text":"<p>Grid structure for simulation case.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.phases","title":"<code>phases()</code>","text":"<p>Phases that exist in the restart file.</p> <p>case = EclipseCase (filename) phs = case.phases () print (\"Number of phases is %d\" % (len (phs))) print (\"Keyword for first phase is %s\" % (\"S\" + phs [0])) if Phase.oil in phs:     print (\"Oil is present\") else:     print (\"Oil is not present\")</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.report_dates","title":"<code>report_dates()</code>","text":"<p>List of all the report dates that are available in this case.</p> <p>Items from this list can be used as a parameter to <code>at</code> to get the case for that particular report step.</p> <p>Returns:</p> Type Description <code>list of datetime.datetime</code> <p>List of available report dates in sequence.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; for rstp in case.report_dates():\n&gt;&gt;&gt;     print(case.at(rstp).this_date)\n</code></pre> See Also <p>ecl.EclipseCase.at</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.shape","title":"<code>shape()</code>","text":"<p>Get shape of returned field data.</p> <p>Returns:</p> Type Description <code>tuple of int</code> <p>Shape of the field data as (num_k, num_j, num_i).</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.start_date","title":"<code>start_date()</code>","text":"<p>Starting date of the simulation</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.summary_data","title":"<code>summary_data(prop, when)</code>","text":"<p>Read summary data from case. This is typically well data, but can also be, for example, newton iterations.</p> <p>Parameters:</p> Name Type Description Default <code>prop</code> <code>str</code> <p>Name of the property, e.g., 'WWPR PRO1'.</p> required <code>when</code> <code>datetime or int</code> <p>Date of the property.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Loaded array for the property.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; case = EclipseCase(cmd_args.filename)\n&gt;&gt;&gt; data = case.cell_data('WWIR INJ-5', datetime.datetime(2054, 7, 1))\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseData","title":"<code>EclipseData</code>","text":"<p>               Bases: <code>object</code></p> <p>Base class for both static and recurrent data.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseData.cell_data","title":"<code>cell_data(selector)</code>","text":"<p>Get a field property for every cell at this restart step.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>tuple</code> <p>Specification of the property to be loaded. This is a tuple starting with a Prop,  and then some context-dependent items.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of the data with inactive cells masked off.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pres = case.cell_data((Prop.pres,))\n&gt;&gt;&gt; swat = case.cell_data((Prop.sat, Phase.wat))\n&gt;&gt;&gt; xco2 = case.cell_data((Prop.mole, 'CO2', Phase.gas))\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseData.components","title":"<code>components()</code>","text":"<p>Components that exist in the restart file. Components used in the simulation are stored in all the restart files instead of once in the init file, since it is the restart file that hold component fields.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseData.field_data","title":"<code>field_data(propname)</code>","text":"<p>Get a property for the entire field at this restart step.</p> <p>Parameters:</p> Name Type Description Default <code>propname</code> <code>str</code> <p>Name of the property to be loaded.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of the data.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseData.summary_data","title":"<code>summary_data(propname)</code>","text":"<p>Get a property from the summary file at this restart step.</p> <p>Parameters:</p> Name Type Description Default <code>propname</code> <code>str</code> <p>Name of the property to be loaded. This is in the form 'mnemonic well',  e.g., 'WWIR I05'. Alternatively, propname can be either only well or  only mnemonic. Then the value for all mnemonics or all wells are given,  e.g., propname='WWIR' returns WWIR for all wells.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of the data.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseFile","title":"<code>EclipseFile</code>","text":"<p>               Bases: <code>object</code></p> <p>Low-level class to read records from binary files.</p> <p>Access to this object must be within a monitor (<code>with</code>-statement).</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseFile.__exit__","title":"<code>__exit__(typ, val, traceback)</code>","text":"<p>Close the underlaying file object when we go out of scope.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseFile.__init__","title":"<code>__init__(root, ext)</code>","text":"<p>Initialize file object from a path in the filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Stem of the file name (including directory).</p> required <code>ext</code> <code>str</code> <p>Extension of the file to read from.</p> required <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseFile.dump","title":"<code>dump(positional=False, fileobj=sys.stdout)</code>","text":"<p>Dump catalog contents of records in the datafile.</p> <p>Parameters:</p> Name Type Description Default <code>positional</code> <code>bool</code> <p>If True, the keywords should be sorted on position in the file.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; f = EclipseFile(\"foo\", 'INIT')\n&gt;&gt;&gt; f.dump()\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseFile.get","title":"<code>get(kwd, seq=0)</code>","text":"<p>Read a (potentially indexed) keyword from file.</p> <p>Parameters:</p> Name Type Description Default <code>kwd</code> <code>str</code> <p>Keyword to read.</p> required <code>seq</code> <code>int</code> <p>Sequence number, starting from zero.</p> <code>0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with EclipseFile('foo', 'EGRID') as grid:\n&gt;&gt;&gt;     zcorn = grid.get('ZCORN')\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseGrid","title":"<code>EclipseGrid</code>","text":"<p>               Bases: <code>object</code></p> <p>Corner-point geometry data from an Eclipse Extensive Grid file.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseGrid.grid","title":"<code>grid()</code>","text":"<p>Create a grid structure from the information read from file.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Grid structure.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # convert from .EGRID to .grdecl:\n&gt;&gt;&gt; import pyresito.io.ecl as ecl\n&gt;&gt;&gt; import pyresito.io.grdecl as grdecl\n&gt;&gt;&gt; grdecl.write('foo.grdecl', ecl.EclipseGrid('FOO').grid())\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseInit","title":"<code>EclipseInit</code>","text":"<p>               Bases: <code>EclipseData</code></p> <p>Read information from static data (init) file.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseRFT","title":"<code>EclipseRFT</code>","text":"<p>               Bases: <code>object</code></p> <p>Read data from an Eclipse RFT file.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseRFT.__init__","title":"<code>__init__(casename)</code>","text":"<p>Initialize the Eclipse case.</p> <p>Parameters:</p> Name Type Description Default <code>casename</code> <code>str</code> <p>Path to the case, with or without extension.</p> required <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseRFT.rft_data","title":"<code>rft_data(well, prop)</code>","text":"<p>Read the RFT data for the requested well.</p> <p>Parameters:</p> Name Type Description Default <code>well</code> <code>str</code> <p>Name of the well, e.g., 'PRO-1'.</p> required <code>prop</code> <code>str</code> <p>Type of property (depth, pressure, swat, or sgas).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Loaded array for the property.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; case = EclipseRFT(cmd_args.filename)\n&gt;&gt;&gt; data = case.rft_data(well='INJ-5', prop='PRESSURE')\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseRestart","title":"<code>EclipseRestart</code>","text":"<p>               Bases: <code>EclipseData</code></p> <p>Read information from a recurrent data (restart) file.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseRestart.__init__","title":"<code>__init__(grid, seq)</code>","text":"<p>Initialize the restart file reader.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>EclipseInit or EclipseGrid</code> <p>Initialization file which contains grid dimensions.</p> required <code>seq</code> <code>int</code> <p>Run number.</p> required <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseRestart.date","title":"<code>date()</code>","text":"<p>Simulation date the restart file is created for.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseSummary","title":"<code>EclipseSummary</code>","text":"<p>               Bases: <code>EclipseData</code></p> <p>Read information from a recurrent data (summary) file.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseSummary.__init__","title":"<code>__init__(grid, seq)</code>","text":"<p>Initialize the restart file reader.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>EclipseInit or EclipseGrid</code> <p>Initialization file which contains grid dimensions.</p> required <code>seq</code> <code>int</code> <p>Run number.</p> required <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseSummary.date","title":"<code>date()</code>","text":"<p>Simulation date the restart file is created for.</p>"},{"location":"reference/misc/ecl/#misc.ecl.main","title":"<code>main(*args)</code>","text":"<p>Read a data file to see if it parses OK.</p>"},{"location":"reference/misc/ecl_common/","title":"ecl_common","text":"<p>Common definitions for all import filters</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from .ecl_common import Phase, Prop\n</code></pre>"},{"location":"reference/misc/read_input_csv/","title":"read_input_csv","text":"<p>File for reading CSV files and returning a 2D list</p>"},{"location":"reference/misc/read_input_csv/#misc.read_input_csv.read_data_csv","title":"<code>read_data_csv(filename, datatype, truedataindex)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>filename</code> <p>Name of csv-file</p> required <code>datatype</code> <p>List of data types as strings</p> required <code>truedataindex</code> <p>List of where the \"TRUEDATA\" has been extracted (e.g., at which time, etc)</p> required <p>Returns:</p> Type Description <code>some-type:</code> <p>List of observed data</p>"},{"location":"reference/misc/read_input_csv/#misc.read_input_csv.read_var_csv","title":"<code>read_var_csv(filename, datatype, truedataindex)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of the CSV file.</p> required <code>datatype</code> <code>list</code> <p>List of data types as strings.</p> required <code>truedataindex</code> <code>list</code> <p>List of indices where the \"TRUEDATA\" has been extracted.</p> required <p>Returns:</p> Name Type Description <code>imported_var</code> <code>list</code> <p>List of variances.</p>"},{"location":"reference/misc/grid/","title":"grid","text":"<p>Generic read module which determines format from extension.</p>"},{"location":"reference/misc/grid/#misc.grid.read_grid","title":"<code>read_grid(filename, cache_dir=None)</code>","text":"<p>Read a grid file and optionally cache it for faster future reads.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of the grid file to read, including path.</p> required <code>cache_dir</code> <code>str</code> <p>Path to a directory where a cache of the grid may be stored to ensure faster read next time.</p> <code>None</code>"},{"location":"reference/misc/grid/cornerpoint/","title":"cornerpoint","text":"<p>Common functionality for visualization of corner-point grids.</p> <p>import pyresito.grid.cornerpoint as cp</p>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.bounding_box","title":"<code>bounding_box(corn, filtr)</code>","text":"<p>Calculate the bounding box of the grid.</p> <p>This function assumes that the grid is aligned with the geographical axes.</p> <p>Parameters:</p> Name Type Description Default <code>corn</code> <code>ndarray</code> <p>Coordinate values for each corner. This matrix can be constructed with the <code>corner_coordinates</code> function. Shape: (3, nk*2*nj*2*ni*2)</p> required <code>filtr</code> <code>ndarray</code> <p>Active corners; use scatter of ACTNUM if no filtering. Shape: (nk, 2, nj, 2, ni, 2), dtype: numpy.bool</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Bottom front right corner and top back left corner. Since the matrix is returned with C ordering, it is specified the opposite way of what is natural for mathematical matrices. Shape: (2, 3), dtype: numpy.float64</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; corn = corner_coordinates(grid['COORD'], grid['ZCORN'])\n&gt;&gt;&gt; bbox = bounding_box(corn, scatter(grid['ACTNUM']))\n&gt;&gt;&gt; p, q = bbox[0, :], bbox[1, :]\n&gt;&gt;&gt; diag = np.sqrt(np.dot(q - p, q - p))\n</code></pre>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.cell_filter","title":"<code>cell_filter(grid, func)</code>","text":"<p>Create a filter for a specific grid.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Grid structure that should be filtered.</p> required <code>func</code> <code>Callable[(int, int, int), bool]</code> <p>Lambda function that takes i, j, k indices (one-based) and returns boolean for whether the cells should be included or not. The function should be vectorized.</p> required <p>Returns:</p> Name Type Description <code>layr</code> <code>ndarray</code> <p>Filtered grid layer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; layr = cell_filter(grid, lambda i, j, k: np.greater_equal(k, 56))\n</code></pre>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.corner_coordinates","title":"<code>corner_coordinates(coord, zcorn)</code>","text":"<p>Generate (x, y, z) coordinates for each corner-point.</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>ndarray</code> <p>Pillar geometrical information. Shape: (nj+1, ni+1, 2, 3)</p> required <code>zcorn</code> <code>ndarray</code> <p>Depth values along each pillar. Shape: (nk, 2, nj, 2, ni, 2)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Coordinate values for each corner. Shape: (3, nk*2*nj*2*ni*2)</p>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.cp_cells","title":"<code>cp_cells(grid, face)</code>","text":"<p>Make a cell array from a cornerpoint grid. The cells will be in the same order as the Cartesian enumeration of the grid.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Pillar coordinates and corner depths. Must contain 'coord' and 'zcorn' properties.</p> required <code>face</code> <code>Face enum</code> <p>Which face that should be extracted, e.g. Face.UP.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Set of geometrical objects that can be sent to rendering. Contains: - 'points': ndarray, shape (nverts, 3) - 'cells': ndarray, shape (nelems, ncorns), where ncorns is either 8    (hexahedron volume) or 4 (quadrilateral face), depending on the face parameter.</p>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.elem_vtcs_ndcs","title":"<code>elem_vtcs_ndcs(nk, nj, ni)</code>","text":"<p>List zcorn indices used by every element in an nk*nj*ni grid.</p> <p>Parameters:</p> Name Type Description Default <code>nk</code> <code>int</code> <p>Number of layers in Z direction.</p> required <code>nj</code> <code>int</code> <p>Number of elements in the Y direction.</p> required <code>ni</code> <code>int</code> <p>Number of elements in the X direction.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Zero-based indices for the hexahedral element corners,  with shape (nk*nj*ni, 8) and dtype int.</p>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.face_coords","title":"<code>face_coords(grid)</code>","text":"<p>Get (x, y, z) coordinates for each corner-point.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Cornerpoint-grid.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Coordinate values for each corner. Use the Face enum to index the first dimension, k, j, i coordinates to index the next three, and Dim enum to index the last dimension. Note that the first point in a face is not necessarily the point that is closest to the origin of the grid. Shape = (8, nk, nj, ni, 3).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pyresito.io.ecl as ecl\n&gt;&gt;&gt; import pyresito.grid.cornerpoint as cp\n&gt;&gt;&gt; case = ecl.EclipseCase(\"FOO\")\n&gt;&gt;&gt; coord_fijkd = cp.face_coords(case.grid())\n&gt;&gt;&gt; # get the midpoint of the upper face in each cell\n&gt;&gt;&gt; up_mid = np.average(coord_fijkd[cp.Face.UP, :, :, :, :], axis=0)\n</code></pre>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.horizon","title":"<code>horizon(grid, layer=0, top=True)</code>","text":"<p>Extract the points that are in a certain horizon and average them so that the result is per cell and not per pillar.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Grid structure.</p> required <code>layer</code> <code>int</code> <p>The K index of the horizon. Default is the layer at the top.</p> <code>0</code> <code>top</code> <code>bool</code> <p>Whether the top face should be exported. If this is False, then the bottom face is exported instead.</p> <code>True</code> <p>Returns:</p> Type Description <code>array</code> <p>Depth at the specified horizon for each cell center, with shape (nk, nj, ni) and dtype numpy.float64.</p>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.horizon_pillars","title":"<code>horizon_pillars(grid, layer=0, top=True)</code>","text":"<p>Extract heights where a horizon crosses the pillars.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Grid structure, containing both COORD and ZCORN properties.</p> required <code>layer</code> <code>int</code> <p>The K index of the horizon. Default is the layer at the top.</p> <code>0</code> <code>top</code> <code>bool</code> <p>Whether the top face should be exported. If False, then the bottom face is exported instead.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Heights of the horizon at each pillar, in the same format as the COORD matrix. Shape: (nj+1, ni+1, 2, 3)</p>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.inner_dup","title":"<code>inner_dup(pillar_field)</code>","text":"<p>Duplicate all inner items in both dimensions.</p> <p>Use this method to duplicate values associated with each pillar to the corners on each side(s) of the pillar; four corners for all interior pillars, two corners for all pillars on the rim and only one (element) corner for those pillars that are on the grid corners.</p> <p>Parameters:</p> Name Type Description Default <code>pillar_field</code> <code>ndarray</code> <p>Property per pillar in the grid, shape = (m+1, n+1)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Property per corner in a grid plane, shape = (2*m, 2*n)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; inner_dup(np.array([[1, 2, 3],\n                        [4, 5, 6],\n                        [7, 8, 9]]))\narray([[1, 2, 2, 3],\n       [4, 5, 5, 6],\n       [4, 5, 5, 6],\n       [7, 8, 8, 9]])\n</code></pre>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.mass_center","title":"<code>mass_center(corn, filtr)</code>","text":"<p>Mass center of the grid.</p> <p>This function will always assume that the density is equal throughout the field.</p> <p>Parameters:</p> Name Type Description Default <code>corn</code> <code>ndarray</code> <p>Coordinate values for each corner. This matrix can be constructed with the  <code>corner_coordinates</code> function. Shape = (3, nk*2*nj*2*ni*2).</p> required <code>filtr</code> <code>ndarray</code> <p>Active corners; use scatter of ACTNUM if no filtering. Shape = (nk, 2, nj, 2, ni, 2),  dtype = numpy.bool.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Center of mass. This should be the focal point of the grid. Shape = (3,), dtype = np.float64.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; corn = cp.corner_coordinates(grid['COORD'], grid['ZCORN'])\n&gt;&gt;&gt; focal_point = cp.mass_center(corn, cp.scatter(grid['ACTNUM']))\n</code></pre>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.scatter","title":"<code>scatter(cell_field)</code>","text":"<p>Duplicate all items in every dimension.</p> <p>Use this method to duplicate values associated with each cell to each of the corners in the cell.</p> <p>Parameters:</p> Name Type Description Default <code>cell_field</code> <code>ndarray</code> <p>Property per cell in the grid, shape = (nk, nj, ni)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Property per corner in the cube, shape = (nk, 2, nj, 2, ni, 2)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; scatter(np.array([[[1, 2],\n                       [3, 4]],\n                      [[5, 6],\n                       [7, 8]]]))\narray([[[[[[1, 1], [2, 2]],\n          [[1, 1], [2, 2]]],\n         [[[3, 3], [4, 4]],\n          [[3, 3], [4, 4]]]],\n</code></pre> <pre><code>    [[[[1, 1], [2, 2]],\n      [[1, 1], [2, 2]]],\n     [[[3, 3], [4, 4]],\n      [[3, 3], [4, 4]]]]],\n\n   [[[[[5, 5], [6, 6]],\n      [[5, 5], [6, 6]]],\n     [[[7, 7], [8, 8]],\n      [[7, 7], [8, 8]]]],\n\n    [[[[5, 5], [6, 6]],\n      [[5, 5], [6, 6]]],\n     [[[7, 7], [8, 8]],\n      [[7, 7], [8, 8]]]]]])\n</code></pre>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.snugfit","title":"<code>snugfit(grid)</code>","text":"<p>Create coordinate pillars that match exactly the top and bottom horizon of the grid cells.</p> <p>This version assumes that the pillars in the grid are all strictly vertical, i.e., the x- and y-coordinates will not be changed.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Grid structure, containing both COORD and ZCORN properties.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Pillar coordinates, in the same format as the COORD matrix. Note that a new matrix is returned, the original grid is not altered/updated. Shape: (nj+1, ni+1, 2, 3)</p>"},{"location":"reference/misc/grid/sector/","title":"sector","text":"<p>Extract a sector from an existing cornerpoint grid.</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.extract_cell_prop","title":"<code>extract_cell_prop(prop, least, most)</code>","text":"<p>Extract the property values for a submodel.</p> <p>Parameters:</p> Name Type Description Default <code>prop</code> <code>ndarray</code> <p>Property values for each cell in the entire grid with shape (nk, nj, ni).</p> required <code>least</code> <code>tuple of int</code> <p>Lower, left-most, back corner of submodel, (k1, j1, i1).</p> required <code>most</code> <code>tuple of int</code> <p>Upper, right-most, front corner of submodel, (k2, j2, i2).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Property values for each cell in the submodel with shape (k2-k1+1, j2-j1+1, i2-i1+1).</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.extract_coord","title":"<code>extract_coord(coord, least, most)</code>","text":"<p>Extract the coordinate pillars for a submodel.</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>ndarray</code> <p>Coordinate pillars for the entire grid with shape (nj+1, ni+1, 2, 3).</p> required <code>least</code> <code>tuple of int</code> <p>Lower, left-most, back corner of submodel, (k1, j1, i1).</p> required <code>most</code> <code>tuple of int</code> <p>Upper, right-most, front corner of submodel, (k2, j2, i2).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Coordinate pillars for the submodel with shape (j2-j1+2, i2-i1+2, 2, 3).</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.extract_dimens","title":"<code>extract_dimens(least, most)</code>","text":"<p>Build a new dimension tuple for a submodel.</p> <p>Parameters:</p> Name Type Description Default <code>least</code> <code>tuple of int</code> <p>Lower, left-most, back corner of submodel, (k1, j1, i1).</p> required <code>most</code> <code>tuple of int</code> <p>Upper, right-most, front corner of submodel, (k2, j2, i2).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Dimensions of the submodel.</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.extract_grid","title":"<code>extract_grid(grid, least, most)</code>","text":"<p>Extract a submodel from a full grid.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Attributes of the full grid, such as COORD, ZCORN, ACTNUM.</p> required <code>least</code> <code>tuple of int</code> <p>Lower, left-most, back corner of the submodel, (k1, j1, i1).</p> required <code>most</code> <code>tuple of int</code> <p>Upper, right-most, front corner of the submodel, (k2, j2, i2).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Attributes of the sector model.</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.extract_zcorn","title":"<code>extract_zcorn(zcorn, least, most)</code>","text":"<p>Extract hinge depth values for a submodel from the entire grid.</p> <p>Parameters:</p> Name Type Description Default <code>zcorn</code> <code>ndarray</code> <p>Hinge depth values for the entire grid with shape (nk, 2, nj, 2, ni, 2).</p> required <code>least</code> <code>tuple of int</code> <p>Lower, left-most, back corner of submodel, (k1, j1, i1).</p> required <code>most</code> <code>tuple of int</code> <p>Upper, right-most, front corner of submodel, (k2, j2, i2).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Hinge depth values for the submodel with shape (k2-k1+1, 2, j2-j1+1, 2, i2-i1+1).</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.main","title":"<code>main(*args)</code>","text":"<p>Read a data file to see if it parses OK.</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.parse_tuple","title":"<code>parse_tuple(corner)</code>","text":"<p>Parse a coordinate specification string into a tuple of zero-based coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>corner</code> <code>str</code> <p>Coordinate specification in the format \"(i1,j1,k1)\".</p> required <p>Returns:</p> Type Description <code>tuple of int</code> <p>The parsed tuple, converted into zero-based coordinates and in Python-matrix order: (k, j, i).</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.sort_tuples","title":"<code>sort_tuples(corner, opposite)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>corner</code> <code>tuple of int</code> <p>Coordinates of one corner.</p> required <code>opposite</code> <code>tuple of int</code> <p>Coordinates of the opposite corner.</p> required <p>Returns:</p> Type Description <code>tuple of tuple of int</code> <p>The two tuples, but with coordinates interchanged so that one corner is always in the lower, left, back and the other is in the upper, right, front.</p>"},{"location":"reference/misc/grid/unstruct/","title":"unstruct","text":"<p>Convert cornerpoint grids to unstructured grids.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pyresito.grid.unstruct as us\n&gt;&gt;&gt; import pyresito.io.grdecl as grdecl\n&gt;&gt;&gt; g = grdecl.read('~/proj/cmgtools/bld/overlap.grdecl')\n</code></pre>"},{"location":"reference/misc/grid/unstruct/#misc.grid.unstruct.Face","title":"<code>Face</code>","text":"<p>               Bases: <code>object</code></p> <p>A (vertical) face consists of two ridges, because all of the faces in a hexahedron can be seen as (possibly degenerate) quadrilaterals.</p>"},{"location":"reference/misc/grid/unstruct/#misc.grid.unstruct.Face.is_above","title":"<code>is_above(other)</code>","text":"<p>Weak ordering of faces based on vertical placement.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Face</code> <p>Face to be compared to this object.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if all points in face self are above all points in face other, False otherwise.</p>"},{"location":"reference/misc/grid/unstruct/#misc.grid.unstruct.Ridge","title":"<code>Ridge</code>","text":"<p>               Bases: <code>object</code></p> <p>A ridge consists of two points, anchored in each their pillar. We only need to store the z-values, because the x- and y- values are determined by the pillar themselves.</p>"},{"location":"reference/misc/grid/unstruct/#misc.grid.unstruct.Ridge.is_not_below","title":"<code>is_not_below(other)</code>","text":"<p>Weak ordering of ridges based on vertical placement.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Ridge</code> <p>Ridge to be compared to this object.</p> required <p>Returns:</p> Type Description <code>bool or None</code> <p>True if no point on self is below any on the other, None if the ridges cross, and False if there is a point on the other ridge that is above any on self.</p>"},{"location":"reference/misc/grid/unstruct/#misc.grid.unstruct.conv","title":"<code>conv(grid)</code>","text":"<p>Convert a cornerpoint grid to an unstructured grid.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Cornerpoint grid to be converted. Should contain 'COORD', 'ZCORN', 'ACTNUM'.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Unstructured grid.</p>"},{"location":"reference/misc/system_tools/","title":"system_tools","text":"<p>Multiprocessing and environment management.</p>"},{"location":"reference/misc/system_tools/environ_var/","title":"environ_var","text":"<p>Descriptive description.</p>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.CmgRunEnvironment","title":"<code>CmgRunEnvironment</code>","text":"<p>A context manager class to run CMG simulators with correct environmental variables.</p>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.CmgRunEnvironment.__enter__","title":"<code>__enter__()</code>","text":"<p>Method that is run when class is initiated by a 'with'-statement</p> Changelog <ul> <li>ST 25/10-18</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.CmgRunEnvironment.__exit__","title":"<code>__exit__(exc_typ, exc_val, exc_trb)</code>","text":"<p>Method that is run when 'with'-statement closes. Here, we reset environment variables and Process context to what is was set to before the 'with'-statement. Input here in this method are required to work in 'with'-statement.</p> Changelog <ul> <li>ST 25/10-18</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.CmgRunEnvironment.__init__","title":"<code>__init__(root, simulator, version, license)</code>","text":"<p>We initialize the context manager by setting up correct paths and environment variable names that we set in enter.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root folder where CMG simulator(s) are installed.</p> required <code>simulator</code> <code>str</code> <p>Simulator name.</p> required <code>version</code> <code>str</code> <p>Version of the simulator.</p> required <code>license</code> <code>str</code> <p>License server name.</p> required Changelog <ul> <li>ST 25/10-18</li> </ul> Notes <p>'version' is the release version of CMG, e.g., 2017.101.G.</p>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.EclipseRunEnvironment","title":"<code>EclipseRunEnvironment</code>","text":"<p>A context manager class to run eclipse simulators with correct environmental variables.</p>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.EclipseRunEnvironment.__enter__","title":"<code>__enter__()</code>","text":"<p>Method that is run when class is initiated by a 'with'-statement</p> Changelog <ul> <li>KF 30/10-19</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.EclipseRunEnvironment.__exit__","title":"<code>__exit__(exc_typ, exc_val, exc_trb)</code>","text":"<p>Method that is run when 'with'-statement closes. Here, we reset environment variables and Process context to what is was set to before the 'with'-statement. Input here in this method are required to work in 'with'-statement.</p> Changelog <ul> <li>ST 25/10-18</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.EclipseRunEnvironment.__init__","title":"<code>__init__(filename)</code>","text":"<p>input filename: eclipse run file, needed to check for errors (string)</p> Changelog <ul> <li>KF 30/10-19</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.FlowRockRunEnvironment","title":"<code>FlowRockRunEnvironment</code>","text":"<p>A context manager class to run flowRock simulators with correct environmental variables.</p>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.FlowRockRunEnvironment.__enter__","title":"<code>__enter__()</code>","text":"<p>Method that is run when class is initiated by a 'with'-statement</p> Changelog <ul> <li>KF 30/10-19</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.FlowRockRunEnvironment.__exit__","title":"<code>__exit__(exc_typ, exc_val, exc_trb)</code>","text":"<p>Method that is run when 'with'-statement closes. Here, we reset environment variables and Process context to what is was set to before the 'with'-statement. Input here in this method are required to work in 'with'-statement.</p> Changelog <ul> <li>ST 25/10-18</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.FlowRockRunEnvironment.__init__","title":"<code>__init__(filename)</code>","text":"<ul> <li>filename: dummy run file</li> </ul> Changelog <ul> <li>KF 30/10-19</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OPMRunEnvironment","title":"<code>OPMRunEnvironment</code>","text":"<p>A context manager class to run OPM simulators with correct environmental variables.</p>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OPMRunEnvironment.__enter__","title":"<code>__enter__()</code>","text":"<p>Method that is run when class is initiated by a 'with'-statement</p> Changelog <ul> <li>KF 30/10-19</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OPMRunEnvironment.__exit__","title":"<code>__exit__(exc_typ, exc_val, exc_trb)</code>","text":"<p>Method that is run when 'with'-statement closes. Here, we reset environment variables and Process context to what it was set to before the 'with'-statement. Input here in this method are required to work in 'with'-statement.</p> Changelog <ul> <li>ST 25/10-18</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OPMRunEnvironment.__init__","title":"<code>__init__(filename, suffix, matchstring)</code>","text":"<ul> <li>filename: OPM run file, needed to check for errors (string)</li> <li>suffix: What file to search for complete sign</li> <li>matchstring: what is the complete sign</li> </ul> Changelog <ul> <li>KF 30/10-19</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OpenBlasSingleThread","title":"<code>OpenBlasSingleThread</code>","text":"<p>A context manager class to set OpenBLAS multi threading environment variable to 1 (i.e., single threaded). The class is used in a 'with'-statement to ensure that everything inside the statement is run single threaded, and outside the statement is run using whatever the environment variable was set before the 'with'-statement. The environment variable setting threading in OpenBLAS is OMP_NUM_THREADS.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from system_tools.environ_var import OpenBlasSingleThread\n... import ctypes\n... import multiprocessing as mp\n...\n... def justdoit():\n...     # Load OpenBLAS library and print number of threads\n...     openblas_lib = ctypes.cdll.LoadLibrary('/scratch/openblas/lib/libopenblas.so')\n...     print(openblas_lib.openblas_get_num_threads())\n...\n... if __name__ == \"__main__\":\n...     # Load OpenBLAS library and print number of threads before the with-statement\n...     openblas_lib = ctypes.cdll.LoadLibrary('/scratch/openblas/lib/libopenblas.so')\n...     print(openblas_lib.openblas_get_num_threads())\n...\n...     # Run a Process inside the with-statement with the OpenBlasSingleThread class.\n...     with OpenBlasSingleThread ():\n...         p = mp.Process (target=justdoit)\n...         p.start ()\n...         p.join ()\n...\n... # Load OpenBLAS library and print number of threads before the with-statement\n... openblas_lib = ctypes.cdll.LoadLibrary('/scratch/openblas/lib/libopenblas.so')\n... print(openblas_lib.openblas_get_num_threads())\n</code></pre>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OpenBlasSingleThread.__enter__","title":"<code>__enter__()</code>","text":"<p>Method that is run when class is initiated by a 'with'-statement</p> Changelog <ul> <li>ST 31/10-17</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OpenBlasSingleThread.__exit__","title":"<code>__exit__(exc_typ, exc_val, exc_trb)</code>","text":"<p>Method that is run when 'with'-statement closes. Here, we reset OMP_NUM_THREADS and Process context to what is was set to before the 'with'-statement. Input here in this method are required to work in 'with'-statement.</p> Changelog <ul> <li>ST 31/10-17</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OpenBlasSingleThread.__init__","title":"<code>__init__()</code>","text":"<p>Init. the class with no inputs. Use this to initialize internal variables for storing number of threads an the Process context manager before the change to single thread.</p> <p>Attributes:</p> Name Type Description <code>num_threads</code> <p>String with number of OpenBLAS threads before change to single threaded (it is the content of OMP_NUM_THREADS)</p> <code>ctx</code> <p>The context variable from Process (default is 'fork' context, but we want to use 'spawn')</p> Changelog <ul> <li>ST 31/10-17</li> </ul>"},{"location":"reference/pipt/","title":"pipt","text":"<p>Inversion (estimation, data assimilation)</p>"},{"location":"reference/pipt/#pipt--python-inverse-problem-toolbox-pipt","title":"Python Inverse Problem Toolbox (PIPT)","text":"<p>PIPT is one part of the PET application. Here we solve Data-Assimilation and inverse problems.</p>"},{"location":"reference/pipt/pipt_init/","title":"pipt_init","text":"<p>Descriptive description.</p>"},{"location":"reference/pipt/pipt_init/#pipt.pipt_init.init_da","title":"<code>init_da(da_input, en_input, sim)</code>","text":"<p>initialize the ensemble object based on the DA inputs</p>"},{"location":"reference/pipt/loop/","title":"loop","text":"<p>Main loop for running data assimilation.</p>"},{"location":"reference/pipt/loop/assimilation/","title":"assimilation","text":"<p>Descriptive description.</p>"},{"location":"reference/pipt/loop/assimilation/#pipt.loop.assimilation.Assimilate","title":"<code>Assimilate</code>","text":"<p>Class for iterative ensemble-based methods. This loop is similar/equal to a deterministic/optimization loop, but since we use ensemble-based method, we need to invoke <code>pipt.fwd_sim.ensemble.Ensemble</code> to get correct hierarchy of classes. The iterative loop will go until the max. iterations OR convergence has been met. Parameters for both these stopping criteria have to be given by the user through methods in their <code>pipt.update_schemes</code> class. Note that only iterative ensemble smoothers can be implemented with this loop (at the moment). Methods needed to be provided by user in their update_schemes class:  </p> <p><code>calc_analysis</code> <code>check_convergence</code> </p> <p>% Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/loop/assimilation/#pipt.loop.assimilation.Assimilate.__init__","title":"<code>__init__(ensemble)</code>","text":"<p>Initialize by passing the PIPT init. file up the hierarchy.</p>"},{"location":"reference/pipt/loop/assimilation/#pipt.loop.assimilation.Assimilate.calc_forecast","title":"<code>calc_forecast()</code>","text":"<p>Calculate the forecast step.</p> <p>Run the forward simulator, generating predicted data for the analysis step. First input to the simulator instances is the ensemble of (joint) state to be run and how many to run in parallel. The forward runs are done in a while-loop consisting of the following steps:</p> <pre><code>    1. Run the simulator for each ensemble member in the background.\n    2. Check for errors during run (if error, correct and run again or abort).\n    3. Check if simulation has ended; if yes, run simulation for the next ensemble members.\n    4. Get results from successfully ended simulations.\n</code></pre> <p>The procedure here is general, hence a simulator used here must contain the initial step of setting up the parameters and steps i-iv, if not an error will be outputted. Initialization of the simulator is done when initializing the Ensemble class (see init). The names of the mandatory methods in a simulator are:</p> <pre><code>    &gt; setup_fwd_sim\n    &gt; run_fwd_sim\n    &gt; check_sim_end\n    &gt; get_sim_results\n</code></pre> Notes <p>Parallel run in \"ampersand\" mode means that it will be started in the background and run independently of the Python script. Hence, check for simulation finished or error must be conducted!</p> <p>Info</p> <p>It is only necessary to get the results from the forward simulations that corresponds to the observed data at the particular assimilation step. That is, results from all data types are not necessary to extract at step iv; if they are not present in the obs_data (indicated by a None type) then this result does not need to be extracted.</p> <p>Info</p> <p>It is assumed that no underscore is inputted in DATATYPE. If there are underscores in DATATYPE entries, well, then we may have a problem when finding out which response to extract in get_sim_results below.</p>"},{"location":"reference/pipt/loop/assimilation/#pipt.loop.assimilation.Assimilate.post_process_forecast","title":"<code>post_process_forecast()</code>","text":"<p>Post processing of predicted data after a forecast run</p>"},{"location":"reference/pipt/loop/assimilation/#pipt.loop.assimilation.Assimilate.run","title":"<code>run()</code>","text":"<p>The general loop implemented here is:</p> <ol> <li>Forecast/forward simulation</li> <li>Check for convergence</li> <li>If convergence have not been achieved, do analysis/update</li> </ol> <p>% Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/loop/ensemble/","title":"ensemble","text":"<p>Descriptive description.</p>"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble","title":"<code>Ensemble</code>","text":"<p>               Bases: <code>Ensemble</code></p> <p>Class for organizing/initializing misc. variables and simulator for an ensemble-based inversion run. Inherits the PET ensemble structure</p>"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.__init__","title":"<code>__init__(keys_da, keys_en, sim)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>keys_da</code> <code>dict</code> <p>Options for the data assimilation class</p> <ul> <li>daalg: spesification of the method, first the main type (e.g., \"enrml\"), then the solver (e.g., \"gnenrml\")</li> <li>analysis: update flavour (\"approx\", \"full\" or \"subspace\")</li> <li>energy: percent of singular values kept after SVD</li> <li>obsvarsave: save the observations as a file (default false)</li> <li>restart: restart optimization from a restart file (default false)</li> <li>restartsave: save a restart file after each successful iteration (defalut false)</li> <li>analysisdebug: specify which class variables to save to the result files</li> <li>truedataindex: order of the simulated data (for timeseries this is points in time)</li> <li>obsname: unit for truedataindex (for timeseries this is days or hours or seconds, etc.)</li> <li>truedata: the data, e.g., provided as a .csv file</li> <li>assimindex: index for the data that will be used for assimilation</li> <li>datatype: list with the name of the datatypes</li> <li>staticvar: name of the static variables</li> <li>dynamicvar: name of the dynamic variables</li> <li>datavar: data variance, e.g., provided as a .csv file</li> </ul> required <code>keys_en</code> <code>dict</code> <p>Options for the ensemble class</p> <ul> <li>ne: number of perturbations used to compute the gradient</li> <li>state: name of state variables passed to the .mako file</li> <li>prior_: the prior information the state variables, including mean, variance and variable limits <p>NB: If keys_en is empty dict, it is assumed that the prior info is contained in keys_da. The merged dict keys_da|keys_en is what is sent to the parent class.</p> required <code>sim</code> <code>callable</code> <p>The forward simulator (e.g. flow)</p> required"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.check_assimindex_sequential","title":"<code>check_assimindex_sequential()</code>","text":"<p>Check if assim. indices is given as a 2D list as is needed in sequential updating. If not, make it a 2D list</p>"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.check_assimindex_simultaneous","title":"<code>check_assimindex_simultaneous()</code>","text":"<p>Check if assim. indices is given as a 1D list as is needed in simultaneous updating. If not, make it a 2D list with one row.</p>"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.compress","title":"<code>compress(data=None, vintage=0, aug_coeff=None)</code>","text":"<p>Compress the input data using wavelets.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>data to be compressed If data is <code>None</code>, all data (true and simulated) is re-compressed (used if leading indices are updated)</p> <code>None</code> <code>vintage</code> <code>int</code> <p>the time index for the data</p> <code>0</code> <code>aug_coeff</code> <code>bool</code> <ul> <li>False: in this case the leading indices for wavelet coefficients are computed</li> <li>True: in this case the leading indices are augmented using information from the ensemble</li> <li>None: in this case simulated data is compressed</li> </ul> <code>None</code>"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.local_analysis_update","title":"<code>local_analysis_update()</code>","text":"<p>Function for updates that can be used by all algorithms. Do this once to avoid duplicate code for local analysis.</p>"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.save_temp_state_assim","title":"<code>save_temp_state_assim(ind_save)</code>","text":"<p>Method to save the state variable during the assimilation. It is stored in a list with length = tot. no. assim. steps + 1 (for the init. ensemble). The list of temporary states are also stored as a .npz file.</p> <p>Parameters:</p> Name Type Description Default <code>ind_save</code> <code>int</code> <p>Assim. step to save (0 = prior)</p> required"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.save_temp_state_iter","title":"<code>save_temp_state_iter(ind_save, max_iter)</code>","text":"<p>Save a snapshot of state at current iteration. It is stored in a list with length equal to max. iteration length + 1 (due to prior state being 0). The list of temporary states are also stored as a .npz file.</p> <p>Warning</p> <p>Max. iterations must be defined before invoking this method.</p> <p>Parameters:</p> Name Type Description Default <code>ind_save</code> <code>int</code> <p>Iteration step to save (0 = prior)</p> required"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.save_temp_state_mda","title":"<code>save_temp_state_mda(ind_save)</code>","text":"<p>Save a snapshot of the state during a MDA loop. The temporary state will be stored as a list with length equal to the tot. no. of assimilations + 1 (init. ensemble saved in 0 entry). The list of temporary states are also stored as a .npz file.</p> <p>Warning</p> <p>Tot. no. of assimilations must be defined before invoking this method.</p> Parameter <p>ind_save : int     Assim. step to save (0 = prior)</p>"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.save_temp_state_ml","title":"<code>save_temp_state_ml(ind_save)</code>","text":"<p>Save a snapshot of the state during a ML loop. The temporary state will be stored as a list with length equal to the tot. no. of assimilations + 1 (init. ensemble saved in 0 entry). The list of temporary states are also stored as a .npz file.</p> <p>Warning</p> <p>Tot. no. of assimilations must be defined before invoking this method.</p> <p>Parameters:</p> Name Type Description Default <code>ind_save</code> <code>int</code> <p>Assim. step to save (0 = prior)</p> required"},{"location":"reference/pipt/misc_tools/","title":"misc_tools","text":"<p>More tools.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/","title":"analysis_tools","text":"<p>Collection of tools that can be used in update/analysis schemes.</p> <p>Only put tools here that are so general that they can be used by several update/analysis schemes. If some method is only applicable to the update scheme you are implementing, leave it in that class.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.aug_obs_pred_data","title":"<code>aug_obs_pred_data(obs_data, pred_data, assim_index, list_data)</code>","text":"<p>Augment the observed and predicted data to an array at an assimilation step. The observed data will be an augemented vector and the predicted data will be an ensemble matrix.</p> <p>Parameters:</p> Name Type Description Default <code>obs_data</code> <code>list</code> <p>List of dictionaries containing observed data</p> required <code>pred_data</code> <code>list</code> <p>List of dictionaries where each entry of the list is the forward simulation results at an assimilation step. The dictionary has keys equal to the data type (given in <code>OBSNAME</code>).</p> required <p>Returns:</p> Name Type Description <code>obs</code> <code>ndarray</code> <p>Augmented vector of observed data</p> <code>pred</code> <code>ndarray</code> <p>Ensemble matrix of predicted data</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.aug_state","title":"<code>aug_state(state, list_state, cell_index=None)</code>","text":"<p>Augment the state variables to an array.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary of initial ensemble of (joint) state variables (static parameters and dynamic variables) to be assimilated.</p> required <code>list_state</code> <code>list</code> <p>Fixed list of keys in state dict.</p> required <code>cell_index</code> <code>list of vector indexes to be extracted</code> <code>None</code> <p>Returns:</p> Name Type Description <code>aug</code> <code>ndarray</code> <p>Ensemble matrix of augmented state variables</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.block_diag_cov","title":"<code>block_diag_cov(cov, list_state)</code>","text":"<p>Block diagonalize a covariance matrix dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>cov</code> <code>dict</code> <p>Dict. with cov. matrices</p> required <code>list_state</code> <code>list</code> <p>Fixed list of keys in state dict.</p> required <p>Returns:</p> Name Type Description <code>cov_out</code> <code>ndarray</code> <p>Block diag. matrix with prior covariance matrices for each state.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.calc_autocov","title":"<code>calc_autocov(pert)</code>","text":"<p>Calculate sample auto-covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>pert</code> <code>ndarray</code> <p>Perturbation matrix (matrix of variables perturbed with their mean)</p> required <p>Returns:</p> Name Type Description <code>cov_auto</code> <code>ndarray</code> <p>Sample auto-covariance matrix</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.calc_crosscov","title":"<code>calc_crosscov(pert1, pert2)</code>","text":"<p>Calculate sample cross-covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>pert1</code> <p>Perturbation matrices (matrix of variables perturbed with their mean).</p> required <code>pert2</code> <p>Perturbation matrices (matrix of variables perturbed with their mean).</p> required <p>Returns:</p> Name Type Description <code>cov_cross</code> <code>ndarray</code> <p>Sample cross-covariance matrix</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.calc_kalman_filter_eq","title":"<code>calc_kalman_filter_eq(aug_state, kalman_gain, obs_data, pred_data)</code>","text":"<p>Calculate the updated augment state using the Kalman filter equations</p> <p>Parameters:</p> Name Type Description Default <code>aug_state</code> <code>ndarray</code> <p>Augmented state variable (all the parameters defined in <code>STATICVAR</code> augmented in one array)</p> required <code>kalman_gain</code> <code>ndarray</code> <p>Kalman gain</p> required <code>obs_data</code> <code>ndarray</code> <p>Augmented observed data vector (all <code>OBSNAME</code> augmented in one array)</p> required <code>pred_data</code> <code>ndarray</code> <p>Augmented predicted data vector (all <code>OBSNAME</code> augmented in one array)</p> required <p>Returns:</p> Name Type Description <code>aug_state_upd</code> <code>ndarray</code> <p>Updated augmented state variable using the Kalman filter equations</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.calc_kalmangain","title":"<code>calc_kalmangain(cov_cross, cov_auto, cov_data, opt=None)</code>","text":"<p>Calculate the Kalman gain</p> <p>Parameters:</p> Name Type Description Default <code>cov_cross</code> <code>ndarray</code> <p>Cross-covariance matrix between state and predicted data</p> required <code>cov_auto</code> <code>ndarray</code> <p>Auto-covariance matrix of predicted data</p> required <code>cov_data</code> <code>ndarray</code> <p>Variance on observed data (diagonal matrix)</p> required <code>opt</code> <code>str</code> <p>Which method should we use to calculate Kalman gain</p> <ul> <li>'lu': LU decomposition (default)</li> <li>'chol': Cholesky decomposition</li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>kalman_gain</code> <code>ndarray</code> <p>Kalman gain</p> Notes <p>In the following Kalman gain is \\(K\\), cross-covariance is \\(C_{mg}\\), predicted data auto-covariance is \\(C_{g}\\), and data covariance is \\(C_{d}\\).</p> <p>With <code>'lu'</code> option, we solve the transposed linear system: $$     K^T = (C_{g} + C_{d})<sup>{-T}C_{mg}</sup>T $$</p> <p>With <code>'chol'</code> option we use Cholesky on auto-covariance matrix, $$    L L^T = (C_{g} + C_{d})^T $$ and solve linear system with the square-root matrix from Cholesky: $$     L^T Y = C_{mg}^T\\     LK = Y $$</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.calc_objectivefun","title":"<code>calc_objectivefun(pert_obs, pred_data, Cd)</code>","text":"<p>Calculate the objective function.</p> <p>Parameters:</p> Name Type Description Default <code>pert_obs</code> <code>array - like</code> <p>NdxNe array containing perturbed observations.</p> required <code>pred_data</code> <code>array - like</code> <p>NdxNe array containing ensemble of predictions.</p> required <code>Cd</code> <code>array - like</code> <p>NdxNd array containing data covariance, or Ndx1 array containing data variance.</p> required <p>Returns:</p> Name Type Description <code>data_misfit</code> <code>array - like</code> <p>Nex1 array containing objective function values.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.calc_scaling","title":"<code>calc_scaling(state, list_state, prior_info)</code>","text":"<p>Form the scaling to be used in svd related algoritms. Scaling consist of standard deviation for each <code>STATICVAR</code> It is important that this is formed in the same manner as the augmentet state vector is formed. Hence, with the same list of states.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary containing the state</p> required <code>list_state</code> <code>list</code> <p>List of states for augmenting</p> required <code>prior_info</code> <code>dict</code> <p>Nested dictionary containing prior information</p> required <p>Returns:</p> Name Type Description <code>scaling</code> <code>numpy array</code> <p>scaling</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.calc_subspace_kalmangain","title":"<code>calc_subspace_kalmangain(cov_cross, data_pert, cov_data, energy)</code>","text":"<p>Compute the Kalman gain in a efficient subspace determined by how much energy (i.e. percentage of singluar values) to retain. For more info regarding the implementation, see Chapter 14 in <code>evensen2009a</code>.</p> <p>Parameters cov_cross : ndarray     Cross-covariance matrix between state and predicted data data_pert : ndarray         Predicted data - mean of predicted data cov_data : ndarray     Variance on observed data (diagonal matrix)</p> <p>Returns:</p> Name Type Description <code>k_g</code> <code>ndarray</code> <p>Subspace Kalman gain</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.compute_x","title":"<code>compute_x(pert_preddata, cov_data, keys_da, alfa=None)</code>","text":"<p>INSERT DESCRIPTION</p> <p>Parameters:</p> Name Type Description Default <code>pert_preddata</code> <code>ndarray</code> <p>Perturbed predicted data</p> required <code>cov_data</code> <code>ndarray</code> <p>Data covariance matrix</p> required <code>keys_da</code> <code>dict</code> <p>Dictionary with every input in <code>DATAASSIM</code></p> required <code>alfa</code> <code>None</code> <p>INSERT DESCRIPTION</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X</code> <code>ndarray</code> <p>INSERT DESCRIPTION</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.extract_tot_empirical_cov","title":"<code>extract_tot_empirical_cov(data_var, assim_index, list_data, ne)</code>","text":"<p>Extract realizations of noise from data_var (if imported), or generate realizations if only variance is specified (assume uncorrelated)</p> <p>Parameters:</p> Name Type Description Default <code>data_var</code> <code>list</code> <p>List of dictionaries containing the varianse as read from the input</p> required <code>assim_index</code> <code>int</code> <p>Index of the assimilation</p> required <code>list_data</code> <code>list</code> <p>List of data types</p> required <code>ne</code> <code>int</code> <p>Ensemble size</p> required <p>Returns:</p> Name Type Description <code>E</code> <code>ndarray</code> <p>Sorted (according to assim_index and list_data) matrix of data realization noise.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.gen_covdata","title":"<code>gen_covdata(datavar, assim_index, list_data)</code>","text":"<p>Generate the data covariance matrix at current assimilation step. Note here that the data covariance may be a diagonal matrix with only variance entries, or an empirical covariance matrix, or both if in combination. For diagonal data covariance we only store vector of variance values.</p> <p>Parameters:</p> Name Type Description Default <code>datavar</code> <code>list</code> <p>List of dictionaries containing variance for the observed data. The structure of this list is the same as for <code>obs_data</code></p> required <code>assim_index</code> <code>int</code> <p>Current assimilation index</p> required <code>list_data</code> <code>list</code> <p>List of the data types</p> required <p>Returns:</p> Name Type Description <code>cd</code> <code>ndarray</code> <p>Data auto-covariance matrix</p> Notes <p>For empirical covariance generation, the datavar entry must be a 2D array, arranged as a standard ensemble matrix (N x Ns, where Ns is the number of samples).</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.get_list_data_types","title":"<code>get_list_data_types(obs_data, assim_index)</code>","text":"<p>Extract the list of all and active data types </p> <p>Parameters:</p> Name Type Description Default <code>obs_data</code> <code>dict</code> <p>Observed data</p> required <code>assim_index</code> <code>int</code> <p>Current assimilation index</p> required <p>Returns:</p> Name Type Description <code>l_all</code> <code>list</code> <p>List of all data types</p> <code>l_act</code> <code>list</code> <p>List of the data types that are active (that are not <code>None</code>)</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.init_local_analysis","title":"<code>init_local_analysis(init, state)</code>","text":"<p>Initialize local analysis.</p> <p>Initialize the local analysis by reading the input variables, defining the parameter classes and search ranges. Build the map of data/parameter positions.</p> Args <p>init : dictionary containing the parsed information form the input file. state : list of states that will be updated</p> <p>Returns:</p> Name Type Description <code>local</code> <code>dictionary of initialized values.</code>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.limits","title":"<code>limits(state, prior_info)</code>","text":"<p>Check if any state variables overshoots the limits given by the prior info. If so, modify these values</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary containing the states</p> required <code>prior_info</code> <code>dict</code> <p>Dictionary containing prior information for all the states.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>dict</code> <p>Valid state</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.parallel_upd","title":"<code>parallel_upd(list_state, prior_info, states_dict, X, local_mask_info, obs_data, pred_data, parallel, actnum=None, field_dim=None, act_data_list=None, scale_data=None, num_states=1, emp_d_cov=False)</code>","text":"<p>Script to initialize and control a parallel update of the ensemble state following <code>emerick2016a</code>.</p> <p>Parameters:</p> Name Type Description Default <code>list_state</code> <code>list</code> <p>List of state names</p> required <code>prior_info</code> <code>dict</code> <p>INSERT DESCRIPTION</p> required <code>states_dict</code> <code>dict</code> <p>Dict. of state arrays</p> required <code>X</code> <code>ndarray</code> <p>INSERT DESCRIPTION</p> required <code>local_mask_info</code> <code>dict</code> <p>INSERT DESCRIPTION</p> required <code>obs_data</code> <code>ndarray</code> <p>Observed data</p> required <code>pred_data</code> <code>ndarray</code> <p>Predicted data</p> required <code>parallel</code> <code>int</code> <p>Number of parallel runs</p> required <code>actnum</code> <code>ndarray</code> <p>Active cells</p> <code>None</code> <code>field_dim</code> <code>list</code> <p>Number of grid cells in each direction</p> <code>None</code> <code>act_data_list</code> <code>list</code> <p>List of active data names</p> <code>None</code> <code>scale_data</code> <code>ndarray</code> <p>Scaling array for data</p> <code>None</code> <code>num_states</code> <code>int</code> <p>Number of states</p> <code>1</code> <code>emp_d_cov</code> <code>bool</code> <p>INSERT DESCRIPTION</p> <code>False</code> Notes <p>Since the localization matrix is to large for evaluation, we instead calculate it row for row.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.resample_state","title":"<code>resample_state(aug_state, state, list_state, new_en_size)</code>","text":"<p>Extract the seperate state variables from an augmented state matrix. Calculate the mean and covariance, and resample this.</p> <p>Parameters:</p> Name Type Description Default <code>aug_state</code> <code>ndarray</code> <p>Augmented matrix of state variables</p> required <code>state</code> <code>dict</code> <p>Dict. af state variables</p> required <code>list_state</code> <code>list</code> <p>List of state variable</p> required <code>new_en_size</code> <code>int</code> <p>Size of the new ensemble</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>dict</code> <p>Dict. of resampled members</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.save_analysisdebug","title":"<code>save_analysisdebug(ind_save, **kwargs)</code>","text":"<p>Save variables in analysis step for debugging purpose</p> <p>Parameters:</p> Name Type Description Default <code>ind_save</code> <code>int</code> <p>Index of analysis step</p> required <code>**kwargs</code> <code>dict</code> <p>Variables that will be saved to npz file</p> <code>{}</code> Notes <p>Use kwargs here because the input will be a dictionary with names equal the variable names to store, and when this is passed to np.savez (kwargs) the variable will be stored with their original name.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.screen_data","title":"<code>screen_data(cov_data, pred_data, obs_data_vector, keys_da, iteration)</code>","text":"<p>INSERT DESCRIPTION</p> <p>Parameters:</p> Name Type Description Default <code>cov_data</code> <code>ndarray</code> <p>Data covariance matrix</p> required <code>pred_data</code> <code>ndarray</code> <p>Predicted data</p> required <code>obs_data_vector</code> <p>Observed data (1D array)</p> required <code>keys_da</code> <code>dict</code> <p>Dictionary with every input in <code>DATAASSIM</code></p> required <code>iteration</code> <code>int</code> <p>Current iteration</p> required <p>Returns:</p> Name Type Description <code>cov_data</code> <code>ndarray</code> <p>Updated data covariance matrix</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.store_ensemble_sim_information","title":"<code>store_ensemble_sim_information(saveinfo, member)</code>","text":"<p>Here, we can either run a unique python script or do some other post-processing routines. The function should not return anything, but provide a method for storing revevant information. Input the current member for easy storage</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.subsample_state","title":"<code>subsample_state(index, aug_state, pert_state)</code>","text":"<p>Draw a subsample from the original state, given by the index</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>ndarray</code> <p>Index of parameters to draw.</p> required <code>aug_state</code> <code>ndarray</code> <p>Original augmented state.</p> required <code>pert_state</code> <code>ndarray</code> <p>Perturbed augmented state, for error covariance.</p> required <p>Returns:</p> Name Type Description <code>new_state</code> <code>dict</code> <p>Subsample of state.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.update_datavar","title":"<code>update_datavar(cov_data, datavar, assim_index, list_data)</code>","text":"<p>Extract the separate variance from an augmented vector. It is assumed that the augmented variance is made gen_covdata, hence this is the reverse method of gen_covdata.</p> <p>Parameters:</p> Name Type Description Default <code>cov_data</code> <code>array - like</code> <p>Augmented vector of variance.</p> required <code>datavar</code> <code>dict</code> <p>Dictionary of separate variances.</p> required <code>assim_index</code> <code>list</code> <p>Assimilation order as a list.</p> required <code>list_data</code> <code>list</code> <p>List of data keys.</p> required <p>Returns:</p> Name Type Description <code>datavar</code> <code>dict</code> <p>Updated dictionary of separate variances.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.update_state","title":"<code>update_state(aug_state, state, list_state, cell_index=None)</code>","text":"<p>Extract the separate state variables from an augmented state array. It is assumed that the augmented state array is made in <code>aug_state</code>, hence this is the reverse method of <code>aug_state</code>.</p> <p>Parameters:</p> Name Type Description Default <code>aug_state</code> <code>ndarray</code> <p>Augmented array of UPDATED state variables</p> required <code>state</code> <code>dict</code> <p>Dict. of state variables NOT updated.</p> required <code>list_state</code> <code>list</code> <p>List of state keys that have been updated</p> required <code>cell_index</code> <code>list</code> <p>List of indexes that gives the where the aug state should be placed</p> <code>None</code> <p>Returns:</p> Name Type Description <code>state</code> <code>dict</code> <p>Dict. of UPDATED state variables</p>"},{"location":"reference/pipt/misc_tools/cov_regularization/","title":"cov_regularization","text":"<p>Scripts used for localization in the fwd_sim step of Bayes.</p> Changelog <ul> <li>28/6-16: Initialise major reconstruction of the covariance regularization script.</li> </ul> <p>Main outline is:</p> <ul> <li>make this a collection of support functions, not a class</li> <li> <p>initialization will be performed at the initialization of the ensemble class, not at the analysis step. This will   return a dictionary of dictionaries, with a triple as key (data_type, assim_time, parameter). From this key the info   for a unique localization function can be found as a new dictionary with keys:   <code>taper_func</code>, <code>position</code>, <code>anisotropi</code>, <code>range</code>.   This is, potentially, a substantial amount of data which should be imported   as a npz file. For small cases, it can be defined in the init file in csv   form:</p> <p>LOCALIZATION   FIELD    10 10   fb 2 2 1 5 1 0 WBHP PRO-1 10 PERMX,fb 7 7 1 5 1 0 WBHP PRO-2 10 PERMX,fb 5 5 1 5 1 0 WBHP INJ-1 10 PERMX   (taper_func pos(x) pos(y) pos(z) range range(z) anisotropi(ratio) anisotropi(angel) data well assim_time parameter)</p> </li> <li> <p>Generate functions that return the correct localization function.</p> </li> </ul>"},{"location":"reference/pipt/misc_tools/cov_regularization/#pipt.misc_tools.cov_regularization.localization","title":"<code>localization</code>","text":""},{"location":"reference/pipt/misc_tools/cov_regularization/#pipt.misc_tools.cov_regularization.localization.__init__","title":"<code>__init__(parsed_info, assimIndex, data_typ, free_parameter, ne)</code>","text":"<p>Format the parsed info from the input file, and generate the unique localization masks</p>"},{"location":"reference/pipt/misc_tools/qaqc_tools/","title":"qaqc_tools","text":"<p>Quality Assurance of the forecast (QA) and analysis (QC) step.</p>"},{"location":"reference/pipt/misc_tools/qaqc_tools/#pipt.misc_tools.qaqc_tools.QAQC","title":"<code>QAQC</code>","text":"<p>Perform Quality Assurance of the forecast (QA) and analysis (QC) step. Available functions:    1) calc_coverage: check forecast data coverage    2) calc_mahalanobis: evaluate \"higher-order\" data coverage    3) calc_kg: check/write individual gain for parameters;                flag data which have conflicting updates    4) calc_da_stat: compute statistics for updated parameters</p> <p>Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/misc_tools/qaqc_tools/#pipt.misc_tools.qaqc_tools.QAQC.calc_coverage","title":"<code>calc_coverage(line=None, field_dim=None)</code>","text":"<p>Calculate the Data coverage for production and seismic data. For seismic data the plotting is based on the importance-scaled coverage developed by Espen O. Lie from GeoCore.</p> <p>Input:     line: if not None, plot 1d coverage     field_dim: if None, must import utm coordinates. Else give the grid</p> <p>Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/misc_tools/qaqc_tools/#pipt.misc_tools.qaqc_tools.QAQC.calc_da_stat","title":"<code>calc_da_stat(options=None)</code>","text":"<p>Calculate statistics for the updated parameters. The persentage of parameters that have updates larger than one, two and three standard deviations (calculated from the initial ensemble) are flagged.</p> <p>Input: options: Settings for statistics     - write_to_file: write results to .grdecl file (default False)</p> <p>Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/misc_tools/qaqc_tools/#pipt.misc_tools.qaqc_tools.QAQC.calc_kg","title":"<code>calc_kg(options=None)</code>","text":"<p>Check/write individual gain for parameters. Note form ES gain with an identity Cd... This can be improved</p> <p>Visualization of the many of these parameters is problem-specific. In reservoir simulation cases, it is necessary to write this to the simulation grid. While for other applications, one might want other visualization. Hence, the method also depends on a simulator specific writer.</p> <p>Input: options: Settings for the kalman gain computations     - num_store: number of elements to store (default 10)     - unique_time: calculate for each time instance (default False)     - plot_all_kg: plot all the kalman gains for the field parameters, if not plot the num_store (default False)     - only_log: only write to logger; no plotting (default True)     - auto_ada_loc: use localization in computations (default True)     - write_to_resinsight: pipe results to ResInsight (default False)       (Note: this requires that ResInsight is open on the computer)</p> <p>Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/misc_tools/qaqc_tools/#pipt.misc_tools.qaqc_tools.QAQC.calc_mahalanobis","title":"<code>calc_mahalanobis(combi_list=(1, None))</code>","text":"<p>Calculate the mahalanobis distance as described in \"Oliver, D. S. (2020). Diagnosing reservoir model deficiency for model improvement. Journal of Petroleum Science and Engineering, 193(February). https://doi.org/10.1016/j.petrol.2020.107367\"</p> <p>Input: combi_list: list of levels and possible combination of datatypes. The list must be given as a tuple with pairs:     level int: defines which level. default = 1     combi_typ: defines how data are combined: Default is no combine.</p> <p>Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/misc_tools/wavelet_tools/","title":"wavelet_tools","text":"<p>Sparse representation of seismic data using wavelet compression.</p> <p>Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/update_schemes/","title":"update_schemes","text":"<p>Ensemble analysis/conditioning/inversion schemes.</p>"},{"location":"reference/pipt/update_schemes/enkf/","title":"enkf","text":"<p>EnKF type schemes</p>"},{"location":"reference/pipt/update_schemes/enkf/#pipt.update_schemes.enkf.enkfMixIn","title":"<code>enkfMixIn</code>","text":"<p>               Bases: <code>Ensemble</code></p> <p>Straightforward EnKF analysis scheme implementation. The sequential updating can be done with general grouping and ordering of data. If only one-step EnKF is to be done, use <code>es</code> instead.</p>"},{"location":"reference/pipt/update_schemes/enkf/#pipt.update_schemes.enkf.enkfMixIn.__init__","title":"<code>__init__(keys_da, keys_en, sim)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p>"},{"location":"reference/pipt/update_schemes/enkf/#pipt.update_schemes.enkf.enkfMixIn.calc_analysis","title":"<code>calc_analysis()</code>","text":"<p>Calculate the analysis step of the EnKF procedure. The updating is done using the Kalman filter equations, using svd for numerical stability. Localization is available.</p>"},{"location":"reference/pipt/update_schemes/enkf/#pipt.update_schemes.enkf.enkfMixIn.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Calculate the \"convergence\" of the method. Important to</p>"},{"location":"reference/pipt/update_schemes/enkf/#pipt.update_schemes.enkf.enkf_approx","title":"<code>enkf_approx</code>","text":"<p>               Bases: <code>enkfMixIn</code>, <code>approx_update</code></p> <p>MixIn the main EnKF update class with the standard analysis scheme.</p>"},{"location":"reference/pipt/update_schemes/enkf/#pipt.update_schemes.enkf.enkf_full","title":"<code>enkf_full</code>","text":"<p>               Bases: <code>enkfMixIn</code>, <code>approx_update</code></p> <p>MixIn the main EnKF update class with the standard analysis scheme. Note that this class is only included for completness. The EnKF does not iterate, and the standard scheme is therefor always applied.</p>"},{"location":"reference/pipt/update_schemes/enkf/#pipt.update_schemes.enkf.enkf_subspace","title":"<code>enkf_subspace</code>","text":"<p>               Bases: <code>enkfMixIn</code>, <code>subspace_update</code></p> <p>MixIn the main EnKF update class with the subspace analysis scheme.</p>"},{"location":"reference/pipt/update_schemes/enrml/","title":"enrml","text":"<p>EnRML type schemes</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.co_lm_enrml","title":"<code>co_lm_enrml</code>","text":"<p>               Bases: <code>lmenrmlMixIn</code>, <code>approx_update</code></p> <p>This is the implementation of the approximative LM-EnRML algorithm as described in <code>chen2013</code>.</p> <p>This algorithm is quite similar to the lm_enrml as provided above, and will therefore inherit most of its methods. We only change the calc_analysis part...</p> <p>% Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.co_lm_enrml.__init__","title":"<code>__init__(keys_da)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.co_lm_enrml.calc_analysis","title":"<code>calc_analysis()</code>","text":"<p>Calculate the update step in approximate LM-EnRML code.</p> <p>Attributes:</p> Name Type Description <code>iteration</code> <code>int</code> <p>Iteration number</p> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>True if data mismatch is decreasing, False if increasing</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gn_enrml","title":"<code>gn_enrml</code>","text":"<p>               Bases: <code>lmenrmlMixIn</code></p> <p>This is the implementation of the stochastig IES as  described in <code>raanes2019</code>.</p> <p>More information about the method is found in <code>evensen2019</code>. This implementation is the Gauss-Newton version.</p> <p>This algorithm is quite similar to the <code>lm_enrml</code> as provided above, and will therefore inherit most of its methods. We only change the calc_analysis part...</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gn_enrml.__init__","title":"<code>__init__(keys_da)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gn_enrml.calc_analysis","title":"<code>calc_analysis()</code>","text":"Changelog <ul> <li>KF 25/2-20</li> </ul>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gn_enrml.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Check if GN-EnRML have converged based on evaluation of change sizes of objective function, state and damping parameter. Very similar to original function, but exit if there is no reduction in obj. function.</p> <p>Returns:</p> Name Type Description <code>conv</code> <code>bool</code> <p>Logic variable indicating if the algorithm has converged.</p> <code>status</code> <code>bool</code> <p>Indicates whether the objective function has reduced.</p> <code>why_stop</code> <code>dict</code> <p>Dictionary with keys corresponding to convergence criteria, with logical variables indicating which of them has been met.</p> Changelog <ul> <li>ST 3/6-16</li> <li>ST 6/6-16: Added LM damping param. check</li> <li>KF 16/11-20: Modified for GN-EnRML</li> <li>KF 10/3-21: Output whether the method reduced the objective function</li> </ul>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gnenrmlMixIn","title":"<code>gnenrmlMixIn</code>","text":"<p>               Bases: <code>Ensemble</code></p> <p>This is an implementation of EnRML using the Gauss-Newton approach. The update scheme is selected by a MixIn with multiple update_methods_ns. This class must therefore facititate many different update schemes.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gnenrmlMixIn.__init__","title":"<code>__init__(keys_da, keys_en, sim)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gnenrmlMixIn.calc_analysis","title":"<code>calc_analysis()</code>","text":"<p>Calculate the update step in LM-EnRML, which is just the Levenberg-Marquardt update algorithm with the sensitivity matrix approximated by the ensemble.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gnenrmlMixIn.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Check if LM-EnRML have converged based on evaluation of change sizes of objective function, state and damping parameter.</p> <p>Returns:</p> Name Type Description <code>conv</code> <code>bool</code> <p>Logic variable telling if algorithm has converged</p> <code>why_stop</code> <code>dict</code> <p>Dict. with keys corresponding to conv. criteria, with logical variable telling which of them that has been met</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gnenrml_margis","title":"<code>gnenrml_margis</code>","text":"<p>               Bases: <code>gnenrmlMixIn</code>, <code>margIS_update</code></p> <p>The marg-IS scheme is currently not available in this version of PIPT. To utilize the scheme you have to import the margIS_update class from a standalone repository.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.lmenrmlMixIn","title":"<code>lmenrmlMixIn</code>","text":"<p>               Bases: <code>Ensemble</code></p> <p>This is an implementation of EnRML using Levenberg-Marquardt. The update scheme is selected by a MixIn with multiple update_methods_ns. This class must therefore facititate many different update schemes.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.lmenrmlMixIn.__init__","title":"<code>__init__(keys_da, keys_en, sim)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.lmenrmlMixIn.calc_analysis","title":"<code>calc_analysis()</code>","text":"<p>Calculate the update step in LM-EnRML, which is just the Levenberg-Marquardt update algorithm with the sensitivity matrix approximated by the ensemble.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.lmenrmlMixIn.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Check if LM-EnRML have converged based on evaluation of change sizes of objective function, state and damping parameter. </p> <p>Returns:</p> Name Type Description <code>conv</code> <code>bool</code> <p>Logic variable telling if algorithm has converged</p> <code>why_stop</code> <code>dict</code> <p>Dict. with keys corresponding to conv. criteria, with logical variable telling which of them that has been met</p>"},{"location":"reference/pipt/update_schemes/es/","title":"es","text":"<p>ES type schemes</p>"},{"location":"reference/pipt/update_schemes/es/#pipt.update_schemes.es.esMixIn","title":"<code>esMixIn</code>","text":"<p>This is the straightforward ES analysis scheme. We treat this as a all-data-at-once EnKF step, hence the calc_analysis method here is identical to that in the <code>enkf</code> class. Since, for the moment, ASSIMINDEX is parsed in a specific manner (or more precise, single rows and columns in the PIPT init. file is parsed to a 1D list), a <code>Simultaneous</code> 'loop' had to be implemented, and <code>es</code> will use this to do the inversion. Maybe in the future, we can make the <code>enkf</code> class do simultaneous updating also. The consequence of all this is that we inherit BOTH <code>enkf</code> and <code>Simultaneous</code> classes, which is convenient. The <code>Simultaneous</code> class is inherited to set up the correct inversion structure and <code>enkf</code> is inherited to get <code>calc_analysis</code>, so we do not have to implement it again.</p>"},{"location":"reference/pipt/update_schemes/es/#pipt.update_schemes.es.esMixIn.__init__","title":"<code>__init__(keys_da, keys_en, sim)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p>"},{"location":"reference/pipt/update_schemes/es/#pipt.update_schemes.es.esMixIn.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Calculate the \"convergence\" of the method. Important to</p>"},{"location":"reference/pipt/update_schemes/es/#pipt.update_schemes.es.es_approx","title":"<code>es_approx</code>","text":"<p>               Bases: <code>esMixIn</code>, <code>enkf_approx</code></p> <p>Mixin of ES class and approximate update</p>"},{"location":"reference/pipt/update_schemes/es/#pipt.update_schemes.es.es_full","title":"<code>es_full</code>","text":"<p>               Bases: <code>esMixIn</code>, <code>enkf_full</code></p> <p>mixin of ES class and full update. Note that since we do not iterate there is no difference between is full and approx.</p>"},{"location":"reference/pipt/update_schemes/es/#pipt.update_schemes.es.es_subspace","title":"<code>es_subspace</code>","text":"<p>               Bases: <code>esMixIn</code>, <code>enkf_subspace</code></p> <p>mixin of ES class and subspace update.</p>"},{"location":"reference/pipt/update_schemes/esmda/","title":"esmda","text":"<p>ES-MDA type schemes</p>"},{"location":"reference/pipt/update_schemes/esmda/#pipt.update_schemes.esmda.esmdaMixIn","title":"<code>esmdaMixIn</code>","text":"<p>               Bases: <code>Ensemble</code></p> <p>This is the implementation of the ES-MDA algorithm given in <code>emerick2013a</code>. This algorithm have been implemented mostly to illustrate how a algorithm using the Mda loop can be implemented.</p>"},{"location":"reference/pipt/update_schemes/esmda/#pipt.update_schemes.esmda.esmdaMixIn.__init__","title":"<code>__init__(keys_da, keys_en, sim)</code>","text":"<p>The class is initialized by passing the keywords and simulator object upwards in the hierarchy.</p> <p>Parameters:</p> Name Type Description Default <code>keys_da</code> <ul> <li>tot_assim_steps: total number of iterations in MDA, e.g., 3</li> <li>inflation_param: covariance inflation factors, e.g., [2, 4, 4]</li> </ul> required <code>keys_en</code> <code>dict</code> required <code>sim</code> <code>callable</code> required"},{"location":"reference/pipt/update_schemes/esmda/#pipt.update_schemes.esmda.esmdaMixIn.calc_analysis","title":"<code>calc_analysis()</code>","text":"<p>Analysis step of ES-MDA. The analysis algorithm is similar to EnKF analysis, only difference is that the data covariance matrix is inflated with an inflation parameter alpha. The update is done as an iterative smoother where all data is assimilated at once.</p> Notes <p>ES-MDA is an iterative ensemble smoother with a predefined number of iterations, where the updates is done with the EnKF update equations but where the data covariance matrix have been inflated:</p> \\[ \\begin{align} d_{obs} &amp;= d_{true} + \\sqrt{\\alpha}C_d^{1/2}Z \\\\ m &amp;= m_{prior} + C_{md}(C_g + \\alpha C_d)^{-1}(g(m) - d_{obs}) \\end{align} \\] <p>where \\(d_{true}\\) is the true observed data, \\(\\alpha\\) is the inflation factor, \\(C_d\\) is the data covariance matrix, \\(Z\\) is a standard normal random variable, \\(C_{md}\\) and \\(C_{g}\\) are sample covariance matrices, \\(m\\) is the model parameter, and \\(g(\\)\\) is the predicted data. Note that \\(\\alpha\\) can have a different value in each assimilation step and must fulfill:</p> \\[ \\sum_{i=1}^{N_a} \\frac{1}{\\alpha} = 1 \\] <p>where \\(N_a\\) being the total number of assimilation steps.</p>"},{"location":"reference/pipt/update_schemes/esmda/#pipt.update_schemes.esmda.esmdaMixIn.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Check if LM-EnRML have converged based on evaluation of change sizes of objective function, state and damping parameter.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Logic variable telling if algorithm has converged</p> <code>dict</code> <p>Dict. with keys corresponding to conv. criteria, with logical variable telling which of them that has been met</p>"},{"location":"reference/pipt/update_schemes/esmda/#pipt.update_schemes.esmda.esmda_geo","title":"<code>esmda_geo</code>","text":"<p>               Bases: <code>esmda_approx</code></p> <p>This is the implementation of the ES-MDA-GEO algorithm from [1]. The main analysis step in this algorithm is the same as the standard ES-MDA algorithm (implemented in the <code>es_mda</code> class). The difference between this and the standard algorithm is the calculation of the inflation factor. Also see <code>rafiee2017</code>.</p>"},{"location":"reference/pipt/update_schemes/esmda/#pipt.update_schemes.esmda.esmda_geo.__init__","title":"<code>__init__(keys_da)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p>"},{"location":"reference/pipt/update_schemes/multilevel/","title":"multilevel","text":"<p>Here we place the classes that are required to run the multilevel schemes developed in the 4DSeis project. All methods inherit the ensemble class, hence the main loop is inherited. These classes will consider the analysis step.</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.esmda_h","title":"<code>esmda_h</code>","text":"<p>               Bases: <code>multilevel</code>, <code>hybrid_update</code>, <code>esmdaMixIn</code></p> <p>A multilevel implementation of the ES-MDA algorithm with the hybrid gain</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.esmda_h.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Check ESMDA objective function for logging purposes.</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.esmda_seq_h","title":"<code>esmda_seq_h</code>","text":"<p>               Bases: <code>multilevel</code>, <code>esmda_approx</code></p> <p>A multilevel implementation of the Sequeontial ES-MDA algorithm with the hybrid gain</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.esmda_seq_h.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Check ESMDA objective function for logging purposes.</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.mlhs_full","title":"<code>mlhs_full</code>","text":"<p>               Bases: <code>multilevel</code></p> <p>Multilevel Hybrid Ensemble Smoother</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.mlhs_full.__init__","title":"<code>__init__(keys_da, keys_fwd, sim)</code>","text":"<p>Standard initiallization</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.mlhs_full.calc_analysis","title":"<code>calc_analysis()</code>","text":"<p>This class has been written based on the enkf class. It is designed for simultaneous assimilation of seismic data with multilevel spatial data. Using this calc_analysis tool we generate a seperate level-based covariance matrix for every level and update them seperate from each other This scheme updates each level based on the covariance and cross-covariance matrix which are generated based on all the levels which are up/down-scaled to that specific level. In short Modified Kristian's Idea</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.mlhs_full.calc_kalmangain","title":"<code>calc_kalmangain(cov_cross, cov_auto, cov_data, opt=None)</code>","text":"<p>Calculate the Kalman gain Using mainly two options: linear soultion and pseudo inverse of the matrix MN 04/2020</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.mlhs_full.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Check if LM-EnRML have converged based on evaluation of change sizes of objective function, state and damping parameter.</p> <p>Returns:</p> Name Type Description <code>conv</code> <code>bool</code> <p>Logic variable telling if algorithm has converged</p> <code>why_stop</code> <code>dict</code> <p>Dict. with keys corresponding to conv. criteria, with logical variable telling which of them that has been met</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.mlhs_full.check_fault","title":"<code>check_fault()</code>","text":"<p>Checks if there is a statement for generating a fault in the input file and if so generates a synthetic fault based on the given input in there.</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.mlhs_full.efficient_real_gen","title":"<code>efficient_real_gen(mean, var, number, level, original_size=False, limits=None, return_chol=False)</code>","text":"<p>This function is added to prevent additional computational cost if var is diagonal MN 04/20</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.multilevel","title":"<code>multilevel</code>","text":"<p>               Bases: <code>Ensemble</code></p> <p>Inititallize the multilevel class. Similar for all ML schemes, hence make one class for all.</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.multilevel.init_ml_prior","title":"<code>init_ml_prior()</code>","text":"<p>This function changes the structure of prior from independent ensembles to a nested structure.</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.smlses_s","title":"<code>smlses_s</code>","text":"<p>               Bases: <code>multilevel</code>, <code>esmda_approx</code></p> <p>The Sequential multilevel ensemble smoother with the \"straightforward\" flavour as descibed in Nezhadali, M., Bhakta, T., Fossum, K., &amp; Mannseth, T. (2023). Sequential multilevel assimilation of inverted seismic data. Computational Geosciences, 27(2), 265\u2013287. https://doi.org/10.1007/s10596-023-10191-9</p> <p>Since the update schemes are basically a esmda update we inherit the esmda_approx method. Hence, we only have to care about handling the multi-level features.</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.smlses_s.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Check convergence for the smlses-s method</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/","title":"update_methods_ns","text":"<p>Descriptive description.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/approx_update/","title":"approx_update","text":"<p>EnRML (IES) without the prior increment term.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/approx_update/#pipt.update_schemes.update_methods_ns.approx_update.approx_update","title":"<code>approx_update</code>","text":"<p>Approximate LM Update scheme as defined in \"Chen, Y., &amp; Oliver, D. S. (2013). Levenberg\u2013Marquardt forms of the iterative ensemble smoother for efficient history matching and uncertainty quantification. Computational Geosciences, 17(4), 689\u2013703. https://doi.org/10.1007/s10596-013-9351-5\". Note that for a EnKF or ES update, or for update within GN scheme, lambda = 0.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/full_update/","title":"full_update","text":"<p>EnRML (IES) as in 2013.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/full_update/#pipt.update_schemes.update_methods_ns.full_update.full_update","title":"<code>full_update</code>","text":"<p>Full LM Update scheme as defined in \"Chen, Y., &amp; Oliver, D. S. (2013). Levenberg\u2013Marquardt forms of the iterative ensemble smoother for efficient history matching and uncertainty quantification. Computational Geosciences, 17(4), 689\u2013703. https://doi.org/10.1007/s10596-013-9351-5\". Note that for a EnKF or ES update, or for update within GN scheme, lambda = 0.</p> <p>Note</p> <p>no localization is implemented for this method yet.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/full_update/#pipt.update_schemes.update_methods_ns.full_update.full_update.ext_Am","title":"<code>ext_Am(*args, **kwargs)</code>","text":"<p>The class is initialized by calculating the required Am matrix.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/hybrid_update/","title":"hybrid_update","text":"<p>ES, and Iterative ES updates with hybrid update matrix calculated from multi-fidelity runs.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/hybrid_update/#pipt.update_schemes.update_methods_ns.hybrid_update.hybrid_update","title":"<code>hybrid_update</code>","text":"<p>Class for hybrid update schemes as described in: Fossum, K., Mannseth, T., &amp; Stordal, A. S. (2020). Assessment of multilevel ensemble-based data assimilation for reservoir history matching. Computational Geosciences, 24(1), 217\u2013239. https://doi.org/10.1007/s10596-019-09911-x</p> <p>Note that the scheme is slightly modified to be inline with the standard (I)ES approximate update scheme. This enables the scheme to efficiently be coupled with multiple updating strategies via class MixIn</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/subspace_update/","title":"subspace_update","text":"<p>Stochastic iterative ensemble smoother (IES, i.e. EnRML) with subspace implementation.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/subspace_update/#pipt.update_schemes.update_methods_ns.subspace_update.subspace_update","title":"<code>subspace_update</code>","text":"<p>Ensemble subspace update, as described in  Raanes, P. N., Stordal, A. S., &amp; Evensen, G. (2019). Revising the stochastic iterative ensemble smoother. Nonlinear Processes in Geophysics, 26(3), 325\u2013338. https://doi.org/10.5194/npg-26-325-2019 More information about the method is found in Evensen, G., Raanes, P. N., Stordal, A. S., &amp; Hove, J. (2019). Efficient Implementation of an Iterative Ensemble Smoother for Data Assimilation and Reservoir History Matching. Frontiers in Applied Mathematics and Statistics, 5(October), 114. https://doi.org/10.3389/fams.2019.00047</p>"},{"location":"reference/popt/","title":"popt","text":"<p>Optimisation methods.</p>"},{"location":"reference/popt/#popt--python-optimization-problem-toolbox-popt","title":"Python Optimization Problem Toolbox (POPT)","text":"<p>POPT is one part of the PET application. Here we solve the optimization problem.  Currently, the following methods are implemented: </p> <ul> <li>EnOpt: The standard ensemble optimization method</li> <li>GenOpt: Generalized ensemble optimization (using non-Gaussian distributions)</li> <li>SmcOpt: Gradient-free optimization based on sequential Monte Carlo</li> <li>LineSearch: Gradient based method satisfying the strong Wolfie conditions</li> </ul> <p>The gradient and Hessian methods are compatible with SciPy, and can be used as input to scipy.optimize.minimize.  A POPT tutorial is found here.</p>"},{"location":"reference/popt/cost_functions/","title":"cost_functions","text":""},{"location":"reference/popt/cost_functions/ecalc_npv/","title":"ecalc_npv","text":"<p>Net present value.</p>"},{"location":"reference/popt/cost_functions/ecalc_npv/#popt.cost_functions.ecalc_npv.ecalc_npv","title":"<code>ecalc_npv(pred_data, **kwargs)</code>","text":"<p>Net present value cost function using eCalc to calculate emmisions</p> <p>Parameters:</p> Name Type Description Default <code>pred_data</code> <code>array_like</code> <p>Ensemble of predicted data.</p> required <code>**kwargs</code> <code>dict</code> <p>Other arguments sent to the npv function</p> <p>keys_opt : list     Keys with economic data.</p> <p>report : list     Report dates.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>objective_values</code> <code>array_like</code> <p>Objective function values (NPV) for all ensemble members.</p>"},{"location":"reference/popt/cost_functions/ecalc_npv/#popt.cost_functions.ecalc_npv.results_as_df","title":"<code>results_as_df(yaml_model, results, getter)</code>","text":"<p>Extract relevant values, as well as some meta (<code>attrs</code>).</p>"},{"location":"reference/popt/cost_functions/ecalc_pareto_npv/","title":"ecalc_pareto_npv","text":"<p>Net present value.</p>"},{"location":"reference/popt/cost_functions/ecalc_pareto_npv/#popt.cost_functions.ecalc_pareto_npv.ecalc_pareto_npv","title":"<code>ecalc_pareto_npv(pred_data, kwargs)</code>","text":"<p>Net present value cost function using eCalc to calculate emmisions</p> <p>Parameters:</p> Name Type Description Default <code>pred_data</code> <code>array_like</code> <p>Ensemble of predicted data.</p> required <code>**kwargs</code> <code>dict</code> <p>Other arguments sent to the npv function</p> <p>keys_opt : list     Keys with economic data.</p> <p>report : list     Report dates.</p> required <p>Returns:</p> Name Type Description <code>objective_values</code> <code>array_like</code> <p>Objective function values (NPV) for all ensemble members.</p>"},{"location":"reference/popt/cost_functions/ecalc_pareto_npv/#popt.cost_functions.ecalc_pareto_npv.results_as_df","title":"<code>results_as_df(yaml_model, results, getter)</code>","text":"<p>Extract relevant values, as well as some meta (<code>attrs</code>).</p>"},{"location":"reference/popt/cost_functions/npv/","title":"npv","text":"<p>Net present value.</p>"},{"location":"reference/popt/cost_functions/npv/#popt.cost_functions.npv.npv","title":"<code>npv(pred_data, **kwargs)</code>","text":"<p>Net present value cost function</p> <p>Parameters:</p> Name Type Description Default <code>pred_data</code> <code>array_like</code> <p>Ensemble of predicted data.</p> required <code>**kwargs</code> <code>dict</code> <p>Other arguments sent to the npv function</p> <p>keys_opt : list     Keys with economic data.</p> <pre><code>    - wop: oil price\n    - wgp: gas price\n    - wwp: water production cost\n    - wwi: water injection cost\n    - disc: discount factor\n    - obj_scaling: used to scale the objective function (negative since all methods are minimizers)\n</code></pre> <p>report : list     Report dates.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>objective_values</code> <code>ndarray</code> <p>Objective function values (NPV) for all ensemble members.</p>"},{"location":"reference/popt/cost_functions/quadratic/","title":"quadratic","text":"<p>Descriptive description.</p>"},{"location":"reference/popt/cost_functions/quadratic/#popt.cost_functions.quadratic.quadratic","title":"<code>quadratic(state, *args, **kwargs)</code>","text":"<p>Quadratic objective function</p> \\[ f(x) = ||x - b||^2_A \\]"},{"location":"reference/popt/cost_functions/ren_npv/","title":"ren_npv","text":"<p>Net present value cost function with injection from RENewable energy</p>"},{"location":"reference/popt/cost_functions/ren_npv/#popt.cost_functions.ren_npv.ren_npv","title":"<code>ren_npv(pred_data, kwargs)</code>","text":"<p>Net present value cost function with injection from RENewable energy</p> <p>Parameters:</p> Name Type Description Default <code>pred_data</code> <code>ndarray</code> <p>Ensemble of predicted data.</p> required <code>**kwargs</code> <code>dict</code> <p>Other arguments sent to the npv function</p> <ul> <li> <p>keys_opt (list)     Keys with economic data.</p> </li> <li> <p>report (list)     Report dates.</p> </li> </ul> required <p>Returns:</p> Name Type Description <code>objective_values</code> <code>ndarray</code> <p>Objective function values (NPV) for all ensemble members.</p>"},{"location":"reference/popt/cost_functions/ren_npv_co2/","title":"ren_npv_co2","text":"<p>Net Present Value with Renewable Power and co2 emissions</p>"},{"location":"reference/popt/cost_functions/ren_npv_co2/#popt.cost_functions.ren_npv_co2.ren_npv_co2","title":"<code>ren_npv_co2(pred_data, keys_opt, report, save_emissions=False)</code>","text":"<p>Net Present Value with Renewable Power and co2 emissions (with eCalc)</p> <p>Parameters:</p> Name Type Description Default <code>pred_data</code> <code>array_like</code> <p>Ensemble of predicted data.</p> required <code>keys_opt</code> <code>list</code> <p>Keys with economic data.</p> required <code>report</code> <code>list</code> <p>Report dates.</p> required <p>Returns:</p> Name Type Description <code>objective_values</code> <code>array_like</code> <p>Objective function values (NPV) for all ensemble members.</p>"},{"location":"reference/popt/cost_functions/rosenbrock/","title":"rosenbrock","text":"<p>Rosenbrock objective function.</p>"},{"location":"reference/popt/cost_functions/rosenbrock/#popt.cost_functions.rosenbrock.rosenbrock","title":"<code>rosenbrock(state, *args, **kwargs)</code>","text":"<p>Rosenbrock: http://en.wikipedia.org/wiki/Rosenbrock_function</p>"},{"location":"reference/popt/loop/","title":"loop","text":"<p>Main loop for running optimization.</p>"},{"location":"reference/popt/misc_tools/","title":"misc_tools","text":"<p>Optimisation helpers.</p>"},{"location":"reference/popt/misc_tools/basic_tools/","title":"basic_tools","text":"<p>Collection of simple, yet useful Python tools</p>"},{"location":"reference/popt/misc_tools/basic_tools/#popt.misc_tools.basic_tools.index2d","title":"<code>index2d(list2d, value)</code>","text":"<p>Search in a 2D list for pattern or value and return is (i, j) index. If the pattern/value is not found, (None, None) is returned</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; l = [['string1', 1], ['string2', 2]]\n&gt;&gt;&gt; print index2d(l, 'string1')\n(0, 0)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>list2d</code> <code>list of lists</code> <p>2D list.</p> required <code>value</code> <code>object</code> <p>Pattern or value to search for.</p> required <p>Returns:</p> Name Type Description <code>ind</code> <code>tuple</code> <p>Indices (i, j) of the value.</p>"},{"location":"reference/popt/misc_tools/basic_tools/#popt.misc_tools.basic_tools.read_file","title":"<code>read_file(val_type, filename)</code>","text":"<p>Read an eclipse file with specified keyword.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; read_file('PERMX','filename.permx')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>val_type</code> <p>keyword or property</p> required <code>filename</code> <p>the file that is read</p> required <p>Returns:</p> Name Type Description <code>values</code> <p>a vector with values for each cell</p>"},{"location":"reference/popt/misc_tools/basic_tools/#popt.misc_tools.basic_tools.write_file","title":"<code>write_file(filename, val_type, data)</code>","text":"<p>Write an eclipse file with specified keyword.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; write_file('filename.permx','PERMX',data_vec)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>the file that is read</p> required <code>val_type</code> <p>keyword or property</p> required <code>data</code> <p>data written to file</p> required"},{"location":"reference/popt/misc_tools/optim_tools/","title":"optim_tools","text":"<p>Collection of tools that can be used in optimization schemes. Only put tools here that are so general that they can be used by several optimization schemes. If some method is only applicable to the update scheme you are implementing, leave it in that class.</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.aug_optim_state","title":"<code>aug_optim_state(state, list_state)</code>","text":"<p>Augment the state variables to get one augmented array.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary of state variables for optimization. OBS: 1D arrays!</p> required <code>list_state</code> <code>list</code> <p>Fixed list of keys in the state dictionary.</p> required <p>Returns:</p> Name Type Description <code>aug_state</code> <code>ndarray</code> <p>Augmented 1D array of state variables.</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.clip_state","title":"<code>clip_state(x, bounds)</code>","text":"<p>Clip a state vector according to the bounds</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>The input state</p> required <code>bounds</code> <code>array_like</code> <p>(min, max) pairs for each element in x. None is used to specify no bound.</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>The state after truncation</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.corr2BlockDiagonal","title":"<code>corr2BlockDiagonal(state, corr)</code>","text":"<p>Makes the correlation matrix block diagonal. The blocks are the state varible types.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <p>Current control state, including state names</p> required <code>corr</code> <code>array_like</code> <p>Correlation matrix, of shape (d, d)</p> required <p>Returns:</p> Name Type Description <code>corr_blocks</code> <code>list</code> <p>block matrices, one for each variable type</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.corr2cov","title":"<code>corr2cov(corr, std)</code>","text":"<p>Transfroms a correlation matrix to a covaraince matrix</p> <p>Parameters:</p> Name Type Description Default <code>corr</code> <code>array_like</code> <p>The correlation matrix, of shape (d,d).</p> required <code>std</code> <code>array_like</code> <p>Array of the standard deviations, of shape (d, ).</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>The covaraince matrix, of shape (d,d)</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.cov2corr","title":"<code>cov2corr(cov)</code>","text":"<p>Transfroms a covaraince matrix to a correlation matrix</p> <p>Parameters:</p> Name Type Description Default <code>cov</code> <code>array_like</code> <p>The covaraince matrix, of shape (d,d).</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>The correlation matrix, of shape (d,d)</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.get_list_element","title":"<code>get_list_element(list, element)</code>","text":"<p>Retrieve the value associated with a given element in a list of tuples.</p> <p>Parameters:</p> Name Type Description Default <code>list</code> <code>list</code> <p>A list of tuples, where each tuple contains two elements.</p> required <code>element</code> <code>any</code> <p>The element to search for in the first position of the tuples.</p> required <p>Returns:</p> Type Description <code>any</code> <p>The value associated with the given element in the list of tuples, or None if the element is not found.</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.get_optimize_result","title":"<code>get_optimize_result(obj)</code>","text":"<p>Collect optimize results based on requested</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Optimize</code> <p>An instance of an optimization class</p> required <p>Returns:</p> Name Type Description <code>save_dict</code> <code>OptimizeResult</code> <p>The requested optimization results</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.get_sym_pos_semidef","title":"<code>get_sym_pos_semidef(a)</code>","text":"<p>Force matrix to positive semidefinite</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>array_like</code> <p>The input matrix, of shape (d,d)</p> required <p>Returns:</p> Name Type Description <code>a</code> <code>ndarray</code> <p>The positive semidefinite matrix, of shape (d,d)</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.save_optimize_results","title":"<code>save_optimize_results(intermediate_result)</code>","text":"<p>Save optimize results</p> <p>Parameters:</p> Name Type Description Default <code>intermediate_result</code> <code>OptimizeResult</code> <p>An instance of an OptimizeResult class</p> required"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.time_correlation","title":"<code>time_correlation(a, state, n_timesteps, dt=1.0)</code>","text":"<p>Constructs correlation matrix with time correlation using an autoregressive model.</p> \\[ Corr(t_1, t_2) = a^{|t_1 - t_2|} \\] <p>Assumes that each varaible in state is time-order such that <code>x = [x1, x2,..., xi,..., xn]</code>, where <code>i</code> is the time index,  and <code>xi</code> is d-dimensional.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Correlation coef, in range (0, 1).</p> required <code>state</code> <code>dict</code> <p>Control state (represented in a dict).</p> required <code>n_timesteps</code> <code>int</code> <p>Number of time-steps to correlate for each component.</p> required <code>dt</code> <code>float or int</code> <p>Duration between each time-step. Default is 1.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>Correlation matrix with time correlation</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.toggle_ml_state","title":"<code>toggle_ml_state(state, ml_ne)</code>","text":"<p>Toggle the state from a dictionary to a list of levels, or from a list of levels to a dictionary. This is necessary when we are using multi-level ensembles.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict or list</code> <p>The current state, either as a dictionary or a list of levels.</p> required <code>ml_ne</code> <code>list</code> <p>List of ensemble sizes for each level.</p> required <p>Returns:</p> Name Type Description <code>new_state</code> <code>dict or list</code> <p>The toggled state, either as a list of levels or a dictionary.</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.update_optim_state","title":"<code>update_optim_state(aug_state, state, list_state)</code>","text":"<p>Extract the separate state variables from an augmented state array.</p> <p>It is assumed that the augmented state array is made in the aug_optim_state method, hence this is the reverse method.</p> <p>Parameters:</p> Name Type Description Default <code>aug_state</code> <code>ndarray</code> <p>Augmented state array.</p> required <code>state</code> <code>dict</code> <p>Dictionary of state variables for optimization.</p> required <code>list_state</code> <code>list</code> <p>Fixed list of keys in the state dictionary.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>dict</code> <p>State dictionary updated with aug_state.</p>"},{"location":"reference/popt/update_schemes/","title":"update_schemes","text":"<p>Iterative steppers.</p>"},{"location":"reference/popt/update_schemes/enopt/","title":"enopt","text":"<p>Ensemble optimisation algorithm.</p>"},{"location":"reference/popt/update_schemes/enopt/#popt.update_schemes.enopt.EnOpt","title":"<code>EnOpt</code>","text":"<p>               Bases: <code>Optimize</code></p> <p>This is an implementation of the ensemble steepest descent ensemble optimization algorithm - EnOpt. The update of the control variable is done with the simple steepest (or gradient) descent algorithm:</p> \\[ x_l = x_{l-1} - \\alpha \\times C \\times G \\] <p>where \\(x\\) is the control variable, \\(l\\) is the iteration index, \\(\\alpha\\) is the step size, \\(C\\) is a smoothing matrix (e.g., covariance matrix for \\(x\\)), and \\(G\\) is the ensemble gradient.</p> <p>Methods:</p> Name Description <code>calc_update</code> <p>Update using steepest descent method with ensemble gradient</p> References <p>Chen et al., 2009, 'Efficient Ensemble-Based Closed-Loop Production Optimization', SPE Journal, 14 (4): 634-645.</p> <p>TODO: Implement getter for optimize_result</p>"},{"location":"reference/popt/update_schemes/enopt/#popt.update_schemes.enopt.EnOpt.__init__","title":"<code>__init__(fun, x, args, jac, hess, bounds=None, **options)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fun</code> <code>callable</code> <p>objective function</p> required <code>x</code> <code>ndarray</code> <p>Initial state</p> required <code>args</code> <code>tuple</code> <p>Initial covariance</p> required <code>jac</code> <code>callable</code> <p>Gradient function</p> required <code>hess</code> <code>callable</code> <p>Hessian function</p> required <code>bounds</code> <code>list</code> <p>(min, max) pairs for each element in x. None is used to specify no bound.</p> <code>None</code> <code>options</code> <code>dict</code> <p>Optimization options</p> <ul> <li>maxiter: maximum number of iterations (default 10)</li> <li>restart: restart optimization from a restart file (default false)</li> <li>restartsave: save a restart file after each successful iteration (defalut false)</li> <li>tol: convergence tolerance for the objective function (default 1e-6)</li> <li>alpha: step size for the steepest decent method (default 0.1)</li> <li>beta: momentum coefficient for running accelerated optimization (default 0.0)</li> <li>alpha_maxiter: maximum number of backtracing trials (default 5)</li> <li>resample: number indicating how many times resampling is tried if no improvement is found</li> <li>optimizer: 'GA' (gradient accent) or Adam (default 'GA')</li> <li>nesterov: use Nesterov acceleration if true (default false)</li> <li>hessian: use Hessian approximation (if the algorithm permits use of Hessian) (default false)</li> <li>normalize: normalize the gradient if true (default true)</li> <li>cov_factor: factor used to shrink the covariance for each resampling trial (defalut 0.5)</li> <li>savedata: specify which class variables to save to the result files (state, objective             function value, iteration number, number of function evaluations, and number             of gradient evaluations, are always saved)</li> </ul> <code>{}</code>"},{"location":"reference/popt/update_schemes/enopt/#popt.update_schemes.enopt.EnOpt.calc_update","title":"<code>calc_update()</code>","text":"<p>Update using steepest descent method with ensemble gradients</p>"},{"location":"reference/popt/update_schemes/genopt/","title":"genopt","text":"<p>Non-Gaussian generalisation of EnOpt.</p>"},{"location":"reference/popt/update_schemes/genopt/#popt.update_schemes.genopt.GenOpt","title":"<code>GenOpt</code>","text":"<p>               Bases: <code>Optimize</code></p>"},{"location":"reference/popt/update_schemes/genopt/#popt.update_schemes.genopt.GenOpt.__init__","title":"<code>__init__(fun, x, args, jac, jac_mut, corr_adapt=None, bounds=None, **options)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fun</code> <code>callable</code> <p>objective function</p> required <code>x</code> <code>ndarray</code> <p>Initial state</p> required <code>args</code> <code>tuple</code> <p>Initial covariance</p> required <code>jac</code> <code>callable</code> <p>Gradient function</p> required <code>jac_mut</code> <code>callable</code> <p>Mutation gradient function</p> required <code>corr_adapt</code> <code>callable</code> <p>Function for correalation matrix adaption</p> <code>None</code> <code>bounds</code> <code>list</code> <p>(min, max) pairs for each element in x. None is used to specify no bound.</p> <code>None</code> <code>options</code> <code>dict</code> <p>Optimization options</p> <code>{}</code>"},{"location":"reference/popt/update_schemes/genopt/#popt.update_schemes.genopt.GenOpt.calc_update","title":"<code>calc_update()</code>","text":"<p>Update using steepest descent method with ensemble gradients</p>"},{"location":"reference/popt/update_schemes/smcopt/","title":"smcopt","text":"<p>Stochastic Monte-Carlo optimisation.</p>"},{"location":"reference/popt/update_schemes/smcopt/#popt.update_schemes.smcopt.SmcOpt","title":"<code>SmcOpt</code>","text":"<p>               Bases: <code>Optimize</code></p> <p>TODO: Write docstring ala EnOpt</p>"},{"location":"reference/popt/update_schemes/smcopt/#popt.update_schemes.smcopt.SmcOpt.__init__","title":"<code>__init__(fun, x, args, sens, bounds=None, **options)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fun</code> <code>callable</code> <p>objective function</p> required <code>x</code> <code>ndarray</code> <p>Initial state</p> required <code>sens</code> <code>callable</code> <p>Ensemble sensitivity</p> required <code>bounds</code> <code>list</code> <p>(min, max) pairs for each element in x. None is used to specify no bound.</p> <code>None</code> <code>options</code> <code>dict</code> <p>Optimization options</p> <ul> <li>maxiter: maximum number of iterations (default 10)</li> <li>restart: restart optimization from a restart file (default false)</li> <li>restartsave: save a restart file after each successful iteration (defalut false)</li> <li>tol: convergence tolerance for the objective function (default 1e-6)</li> <li>alpha: weight between previous and new step (default 0.1)</li> <li>alpha_maxiter: maximum number of backtracing trials (default 5)</li> <li>resample: number indicating how many times resampling is tried if no improvement is found</li> <li>cov_factor: factor used to shrink the covariance for each resampling trial (defalut 0.5)</li> <li>inflation_factor: term used to weight down prior influence (defalult 1)</li> <li>survival_factor: fraction of surviving samples</li> <li>savedata: specify which class variables to save to the result files (state, objective function             value, iteration number, number of function evaluations, and number of gradient             evaluations, are always saved)</li> </ul> <code>{}</code>"},{"location":"reference/popt/update_schemes/smcopt/#popt.update_schemes.smcopt.SmcOpt.calc_update","title":"<code>calc_update()</code>","text":"<p>Update using sequential monte carlo method</p>"},{"location":"reference/popt/update_schemes/subroutines/cma/","title":"cma","text":"<p>Covariance matrix adaptation (CMA).</p>"},{"location":"reference/popt/update_schemes/subroutines/cma/#popt.update_schemes.subroutines.cma.CMA","title":"<code>CMA</code>","text":""},{"location":"reference/popt/update_schemes/subroutines/cma/#popt.update_schemes.subroutines.cma.CMA.__call__","title":"<code>__call__(cov, step, X, J)</code>","text":"<p>Performs the CMA update.</p> <p>Parameters:</p> Name Type Description Default <code>cov</code> <code>array_like, of shape (d, d)</code> <p>Current covariance or correlation matrix.</p> required <code>step</code> <code>array_like, of shape (d,)</code> <p>New step of control vector. Used to update the evolution path.</p> required <code>X</code> <code>array_like, of shape (n, d)</code> <p>Control ensemble of size n.</p> required <code>J</code> <code>array_like, of shape (n,)</code> <p>Objective ensemble of size n.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>array_like, of shape (d, d)</code> <p>CMA updated covariance (correlation) matrix.</p>"},{"location":"reference/popt/update_schemes/subroutines/cma/#popt.update_schemes.subroutines.cma.CMA.__init__","title":"<code>__init__(ne, dim, alpha_mu=None, n_mu=None, alpha_1=None, alpha_c=None, corr_update=False, equal_weights=True)</code>","text":"<p>This is a rather simple simple CMA class <code>hansen2006</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ne</code> <code>int</code> <p>Ensemble size</p> required <code>dim</code> <code>int</code> <p>Dimensions of control vector</p> required <code>alpha_mu</code> <code>float</code> <p>Learning rate for rank-mu update. If None, value proposed in [1] is used.</p> <code>None</code> <code>n_mu</code> <code>int, `n_mu &lt; ne`</code> <p>Number of best samples of ne, to be used for rank-mu update. Default is int(ne/2).</p> <code>None</code> <code>alpha_1</code> <code>float</code> <p>Learning rate fro rank-one update. If None, value proposed in [1] is used.</p> <code>None</code> <code>alpha_c</code> <code>float</code> <p>Parameter (inverse if backwards time horizen)for evolution path update  in the rank-one update. See [1] for more info. If None, value proposed in [1] is used.</p> <code>None</code> <code>corr_update</code> <code>bool</code> <p>If True, CMA is used to update a correlation matrix. Default is False.</p> <code>False</code> <code>equal_weights</code> <code>bool</code> <p>If True, all n_mu members are assign equal weighting, <code>w_i = 1/n_mu</code>. If False, the weighting scheme proposed in [1], where <code>w_i = log(n_mu + 1)-log(i)</code>, and normalized such that they sum to one. Defualt is True.</p> <code>True</code>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/","title":"optimizers","text":"<p>Gradient acceleration.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.AdaMax","title":"<code>AdaMax</code>","text":"<p>               Bases: <code>Adam</code></p> <p>AdaMax optimizer <code>kingma2014</code></p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Adam","title":"<code>Adam</code>","text":"<p>A class implementing the Adam optimizer for gradient-based optimization <code>kingma2014</code>.</p> <p>The Adam update equation for the control x using gradient g, iteration t, and small constants \u03b5 is given by:</p> <pre><code>m_t = \u03b21 * m_{t-1} + (1 - \u03b21) * g\n\nv_t = \u03b22 * v_{t-1} + (1 - \u03b22) * g^2\n\nm_t_hat = m_t / (1 - \u03b21^t)\n\nv_t_hat = v_t / (1 - \u03b22^t)\n\nx_{t+1} = x_t - \u03b1 * m_t_hat / (sqrt(v_t_hat) + \u03b5)\n</code></pre> <p>Attributes:</p> Name Type Description <code>step_size</code> <code>float</code> <p>The initial step size provided during initialization.</p> <code>beta1</code> <code>float</code> <p>The exponential decay rate for the first moment estimates.</p> <code>beta2</code> <code>float</code> <p>The exponential decay rate for the second moment estimates.</p> <code>vel1</code> <code>1-D array_like</code> <p>First moment estimate.</p> <code>vel2</code> <code>1-D array_like</code> <p>Second moment estimate.</p> <code>eps</code> <code>float</code> <p>Small constant to prevent division by zero.</p> <code>_step_size</code> <code>float</code> <p>Private attribute for temporarily modifying step size.</p> <code>temp_vel1</code> <code>1-D array_like</code> <p>Temporary first moment estimate.</p> <code>temp_vel2</code> <code>1-D array_like</code> <p>Temporary Second moment estimate.</p> <p>Methods:</p> Name Description <code>apply_update</code> <p>Apply an Adam update to the control parameter.</p> <code>apply_backtracking</code> <p>Apply backtracking by reducing step size temporarily.</p> <code>restore_parameters</code> <p>Restore the original step size.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Adam.__init__","title":"<code>__init__(step_size, beta1=0.9, beta2=0.999)</code>","text":"<p>A class implementing the Adam optimizer for gradient-based optimization. The Adam update equation for the control x using gradient g,  iteration t, and small constants \u03b5 is given by:</p> <pre><code>m_t = \u03b21 * m_{t-1} + (1 - \u03b21) * g\n\nv_t = \u03b22 * v_{t-1} + (1 - \u03b22) * g^2\n\nm_t_hat = m_t / (1 - \u03b21^t)\n\nv_t_hat = v_t / (1 - \u03b22^t)\n\nx_{t+1} = x_t - \u03b1 * m_t_hat / (sqrt(v_t_hat) + \u03b5)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>step_size</code> <code>float</code> <p>The step size (learning rate) for the optimization.</p> required <code>beta1</code> <code>float</code> <p>The exponential decay rate for the first moment estimates (default is 0.9).</p> <code>0.9</code> <code>beta2</code> <code>float</code> <p>The exponential decay rate for the second moment estimates (default is 0.999).</p> <code>0.999</code>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Adam.apply_backtracking","title":"<code>apply_backtracking()</code>","text":"<p>Apply backtracking by reducing step size temporarily.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Adam.apply_update","title":"<code>apply_update(control, gradient, **kwargs)</code>","text":"<p>Apply a gradient update to the control parameter.</p> <p>Note</p> <p>This is the steepest decent update: x_new = x_old - x_step.</p> <p>Parameters:</p> Name Type Description Default <code>control</code> <code>array_like</code> <p>The current value of the parameter being optimized.</p> required <code>gradient</code> <code>array_like</code> <p>The gradient of the objective function with respect to the control parameter.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments, including 'iter' for the current iteration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>new_control, temp_velocity: tuple</code> <p>The new value of the control parameter after the update, and the current state step.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Adam.restore_parameters","title":"<code>restore_parameters()</code>","text":"<p>Restore the original step size.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.GradientAscent","title":"<code>GradientAscent</code>","text":"<p>A class for performing gradient ascent optimization with momentum and backtracking. The gradient descent update equation with momentum is given by:</p> \\[ \\begin{align}     v_t &amp;= \\beta * v_{t-1} + \\alpha * gradient \\\\     x_t &amp;= x_{t-1} - v_t \\end{align} \\] <p>Attributes:</p> Name Type Description <code>step_size</code> <code>float</code> <p>The initial step size provided during initialization.</p> <code>momentum</code> <code>float</code> <p>The initial momentum factor provided during initialization.</p> <code>velocity</code> <code>array_like</code> <p>Current velocity of the optimization process.</p> <code>temp_velocity</code> <code>array_like</code> <p>Temporary velocity</p> <code>_step_size</code> <code>float</code> <p>Private attribute for temporarily modifying step size.</p> <code>_momentum</code> <code>float</code> <p>Private attribute for temporarily modifying momentum.</p> <p>Methods:</p> Name Description <code>apply_update</code> <p>Apply a gradient update to the control parameter.</p> <code>apply_backtracking</code> <p>Apply backtracking by reducing step size and momentum temporarily.</p> <code>restore_parameters</code> <p>Restore the original step size and momentum values.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.GradientAscent.__init__","title":"<code>__init__(step_size, momentum)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>step_size</code> <code>float</code> <p>The step size (learning rate) for the gradient ascent.</p> required <code>momentum</code> <code>float</code> <p>The momentum factor to apply during updates.</p> required"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.GradientAscent.apply_backtracking","title":"<code>apply_backtracking()</code>","text":"<p>Apply backtracking by reducing step size and momentum temporarily.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.GradientAscent.apply_smc_update","title":"<code>apply_smc_update(control, gradient, **kwargs)</code>","text":"<p>Apply a gradient update to the control parameter.</p> <p>Parameters:</p> Name Type Description Default <code>control</code> <code>array_like</code> <p>The current value of the parameter being optimized.</p> required <code>gradient</code> <code>array_like</code> <p>The gradient of the objective function with respect to the control parameter.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>new_control</code> <code>ndarray</code> <p>The new value of the control parameter after the update.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.GradientAscent.apply_update","title":"<code>apply_update(control, gradient, **kwargs)</code>","text":"<p>Apply a gradient update to the control parameter.</p> <p>Note</p> <p>This is the steepest decent update: x_new = x_old - x_step.</p> <p>Parameters:</p> Name Type Description Default <code>control</code> <code>array_like</code> <p>The current value of the parameter being optimized.</p> required <code>gradient</code> <code>array_like</code> <p>The gradient of the objective function with respect to the control parameter.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>new_control, temp_velocity: tuple</code> <p>The new value of the control parameter after the update, and the current state step.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.GradientAscent.restore_parameters","title":"<code>restore_parameters()</code>","text":"<p>Restore the original step size and momentum value.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Steihaug","title":"<code>Steihaug</code>","text":"<p>A class implementing the Steihaug conjugate-gradient trust region optimizer. This code is based on the minfx optimisation library, https://gna.org/projects/minfx</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Steihaug.__init__","title":"<code>__init__(maxiter=1000000.0, epsilon=1e-08, delta_max=100000.0, delta0=1.0)</code>","text":"<p>Page 75 from 'Numerical Optimization' by Jorge Nocedal and Stephen J. Wright, 1999, 2<sup>nd</sup> ed.  The CG-Steihaug algorithm is:</p> <ul> <li>epsilon &gt; 0</li> <li>p0 = 0, r0 = g, d0 = -r0</li> <li>if ||r0|| &lt; epsilon:<ul> <li>return p = p0</li> </ul> </li> <li>while 1:<ul> <li>if djT.B.dj &lt;= 0:<ul> <li>Find tau such that p = pj + tau.dj minimises m(p) in (4.9) and satisfies ||p|| = delta</li> <li>return p</li> </ul> </li> <li>aj = rjT.rj / djT.B.dj</li> <li>pj+1 = pj + aj.dj</li> <li>if ||pj+1|| &gt;= delta:<ul> <li>Find tau such that p = pj + tau.dj satisfies ||p|| = delta</li> <li>return p</li> </ul> </li> <li>rj+1 = rj + aj.B.dj</li> <li>if ||rj+1|| &lt; epsilon.||r0||:<ul> <li>return p = pj+1</li> </ul> </li> <li>bj+1 = rj+1T.rj+1 / rjT.rj</li> <li>dj+1 = rj+1 + bj+1.dj</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>maxiter</code> <code>float</code> <p>Maximum number of iterations.</p> <code>1000000.0</code> <code>epsilon</code> <code>float</code> <p>Tolerance for iterations.</p> <code>1e-08</code> <code>delta_max</code> <code>float</code> <p>Maximum thrust region size.</p> <code>100000.0</code> <code>delta0</code> <code>float</code> <p>Initial thrust region size.</p> <code>1.0</code>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Steihaug.apply_backtracking","title":"<code>apply_backtracking()</code>","text":"<p>Apply backtracking by reducing step size temporarily.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Steihaug.apply_update","title":"<code>apply_update(xk, dfk, **kwargs)</code>","text":"<p>Apply a Steihaug update to the control vector.</p> <p>Parameters:</p> Name Type Description Default <code>xk</code> <code>array_like</code> <p>The current value of the parameter being optimized.</p> required <code>dfk</code> <code>array_like</code> <p>The gradient of the objective function with respect to the control parameter.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments, including the hessian of the objective function with respect to the control parameter.</p> <code>{}</code> <p>Returns:</p> Type Description <code>new_control, step: tuple</code> <p>The new value of the control parameter after the update, and the current state step.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Steihaug.get_tau","title":"<code>get_tau(pj, dj)</code>","text":"<p>Function to find tau such that p = pj + tau.dj, and ||p|| = delta.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Steihaug.restore_parameters","title":"<code>restore_parameters()</code>","text":"<p>Restore the original step size.</p>"},{"location":"reference/simulator/","title":"simulator","text":"<p>Model wrappers.</p>"},{"location":"reference/simulator/#simulator--simulator-package","title":"Simulator package","text":"<p>Here you can put the wrappers for simulator used with PET. </p> <p>You can build your wrapper (almost) any way you like, but you must follow the following rules:</p> <ol> <li>The wrapper must be a Class</li> <li>The Class must contain the following three methods:</li> </ol> <pre><code>__init__(self,input_dict):\n    # parse information from the input. \n    # Needs to get datatype, reporttype and reportpoint\n</code></pre> <pre><code>Setup_fwd_run(self)\n   # do whatever initiallization you need.\n   # Useful to initiallize the self.pred_data variable.\n   # self.pred_data is a list of dictionaries. Where each list element represents\n   # a reportpoint and the dictionary should have the datatypes as keys. \n   # Entries in the dictionary are numpy arrays.\n</code></pre> <pre><code>run_fwd_sim(self, state, member)\n  # run simulator. Called from the main function using p_map from p_tqdm package.\n  # Return pred_data if run is successfull, False if run failed.\n</code></pre>"},{"location":"reference/simulator/eclipse/","title":"eclipse","text":"<p>Wrap Eclipse</p>"},{"location":"reference/simulator/eclipse/#simulator.eclipse.ecl_100","title":"<code>ecl_100</code>","text":"<p>               Bases: <code>eclipse</code></p> <p>ecl_100 class</p>"},{"location":"reference/simulator/eclipse/#simulator.eclipse.ecl_100.call_sim","title":"<code>call_sim(path=None, wait_for_proc=False)</code>","text":"<p>Method for calling the ecl_100 simulator.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Alternative folder for the ecl_100.data file.</p> <code>None</code> <code>wait_for_proc</code> <code>bool</code> <p>Logical variable to wait for the simulator to finish. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>.RSM : str</code> <p>Run summary file in the standard ECL format. Well data are collected from this file.</p> <code>.RST : str</code> <p>Restart file in the standard ECL format. Pressure and saturation data are collected from this file.</p> <code>.PRT : str</code> <p>Info file to be used for checking errors in the run.</p> Changelog <ul> <li>KF 14/9-2015</li> </ul>"},{"location":"reference/simulator/eclipse/#simulator.eclipse.ecl_300","title":"<code>ecl_300</code>","text":"<p>               Bases: <code>eclipse</code></p> <p>eclipse 300 class</p>"},{"location":"reference/simulator/eclipse/#simulator.eclipse.ecl_300.call_sim","title":"<code>call_sim(path=None, wait_for_proc=False)</code>","text":"<p>Method for calling the ecl_300 simulator.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Alternative folder for the ecl_100.data file.</p> <code>None</code> <code>wait_for_proc</code> <code>bool</code> <p>Logical variable to wait for the simulator to finish. Default is False.</p> <p>Note</p> <p>For now, this option is only utilized in a single localization option.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>RSM</code> <code>str</code> <p>Run summary file in the standard ECL format. Well data are collected from this file.</p> <code>RST</code> <code>str</code> <p>Restart file in the standard ECL format. Pressure and saturation data are collected from this file.</p> <code>PRT</code> <code>str</code> <p>Info file to be used for checking errors in the run.</p> Changelog <ul> <li>KF 8/10-2015</li> </ul>"},{"location":"reference/simulator/eclipse/#simulator.eclipse.eclipse","title":"<code>eclipse</code>","text":"<p>Class for running the Schlumberger eclipse 100 black oil reservoir simulator. For more information see  GeoQuest: ECLIPSE reference manual 2009.1. Schlumberger, GeoQuest (2009).</p> <p>To run this class, eclipse must be installed and elcrun must be in the system path!</p>"},{"location":"reference/simulator/eclipse/#simulator.eclipse.eclipse.__init__","title":"<code>__init__(input_dict=None, filename=None, options=None)</code>","text":"<p>The inputs are all optional, but in the same fashion as the other simulators a system must be followed. The input_dict can be utilized as a single input. Here all nescessary info is stored. Alternatively, if input_dict is not defined, all the other input variables must be defined.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>dict</code> <p>Dictionary containing all information required to run the simulator.</p> <pre><code>- parallel: number of forward simulations run in parallel\n- simoptions: options for the simulations\n    - mpi: option to use mpi (always use &gt; 2 cores)\n    - sim_path: Path to the simulator\n    - sim_flag: Flags sent to the simulator (see simulator documentation for all possibilities)\n- sim_limit: maximum number of seconds a simulation can run before being killed\n- runfile: name of the simulation input file\n- reportpoint: these are the dates the simulator reports results\n- reporttype: this key states that the report poins are given as dates\n- datatype: the data types the simulator reports\n- replace: replace failed simulations with randomly selected successful ones\n- rerun: in case of failure, try to rerun the simulator the given number of times\n- startdate: simulaton start date\n- saveforecast: save the predicted measurements for each iteration\n</code></pre> <code>None</code> <code>filename</code> <code>str</code> <p>Name of the .mako file utilized to generate the ECL input .DATA file. Must be in uppercase for the ECL simulator.</p> <code>None</code> <code>options</code> <code>dict</code> <p>Dictionary with options for the simulator.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>initial_object</code> <code>object</code> <p>Initial object from the class ecl_100.</p>"},{"location":"reference/simulator/eclipse/#simulator.eclipse.eclipse.coarsen","title":"<code>coarsen(folder, ensembleMember=None)</code>","text":"<p>This method utilized one field parameter to upscale the computational grid. A coarsening file is written to the ensemble folder, and the eclipse calculates the upscaled permeabilities, porosities and transmissibilities based on the new grid and the original parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Path to the ecl_100 run folder.</p> required <code>ensembleMember</code> <code>int</code> <p>Index of the ensemble member to run.</p> <code>None</code> Changelog <ul> <li>KF 17/9-2015 Added uniform upscaling as an option</li> <li>KF 6/01-17</li> </ul>"},{"location":"reference/simulator/eclipse/#simulator.eclipse.eclipse.get_sim_results","title":"<code>get_sim_results(whichResponse, ext_data_info=None, member=None)</code>","text":"<p>Read the output from simulator and store as an array. Optionally, if the DA method is based on an ensemble method, the output must be read inside a folder.</p> <p>Parameters:</p> Name Type Description Default <code>whichResponse</code> <code>str</code> <p>Which of the responses is to be outputted (e.g., WBHP PRO-1, WOPR, PRESS, OILSAT, etc).</p> required <code>ext_data_info</code> <code>tuple</code> <p>Tuple containing the assimilation step information, including the place of assimilation (e.g., which TIME) and the index of this assimilation place.</p> <code>None</code> <code>member</code> <code>int</code> <p>Ensemble member that is finished.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yFlow</code> <code>array - like</code> <p>Array containing the response from ECL 100. The response type is chosen by the user in options['data_type'].</p> Notes <ul> <li>Modified the ecl package to allow reading the summary data directly, hence, we get cell, summary, and field data from the ecl package.</li> <li>KF 29/10-2015</li> <li>Modified the ecl package to read RFT files. To obtain, e.g,. RFT pressures form well 'PRO-1\" whichResponse must be rft_PRESSURE PRO-1</li> </ul>"},{"location":"reference/simulator/eclipse/#simulator.eclipse.eclipse.run_fwd_sim","title":"<code>run_fwd_sim(state, member_i, del_folder=True, nosim=False)</code>","text":"<p>Setup and run the ecl_100 forward simulator. All the parameters are defined as attributes, and the name of the parameters are initialized in setupFwdRun. This method sets up and runs all the individual ensemble members. This method is based on writing .DATA file for each ensemble member using the mako template, for more info regarding mako see http://www.makotemplates.org/</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary containing the ensemble state.</p> required <code>member_i</code> <code>int</code> <p>Index of the ensemble member.</p> required <code>del_folder</code> <code>bool</code> <p>Boolean to determine if the ensemble folder should be deleted. Default is False.</p> <code>True</code> <code>nosim</code> <code>bool</code> <p>Boolean to determine if the simulation should be run. Default is False.</p> <code>False</code>"},{"location":"reference/simulator/eclipse/#simulator.eclipse.eclipse.setup_fwd_run","title":"<code>setup_fwd_run(**kwargs)</code>","text":"<p>Setup the simulator.</p> <p>Attributes:</p> Name Type Description <code>assimIndex</code> <code>int</code> <p>Gives the index-type (e.g. step,time,etc.) and the index for the data to be assimilated</p> <code>trueOrder</code> <p>Gives the index-type (e.g. step,time,etc.) and the index of the true data</p>"},{"location":"reference/simulator/eclipse/#simulator.eclipse.eclipse.write_coarse","title":"<code>write_coarse(folder, whgt, image)</code>","text":"<p>This function writes the include file coarsen to the ecl run. This file tels ECL to coarsen the grid.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Path to the ecl_100 run.</p> required <code>whgt</code> <code>float</code> <p>Weight of the transformed cells.</p> required <code>image</code> <code>array - like</code> <p>Original image.</p> required Changelog <ul> <li>KF 17/9-2015</li> </ul>"},{"location":"reference/simulator/flow_rock/","title":"flow_rock","text":"<p>Descriptive description.</p>"},{"location":"reference/simulator/flow_rock/#simulator.flow_rock.flow_barycenter","title":"<code>flow_barycenter</code>","text":"<p>               Bases: <code>flow</code></p> <p>Couple the OPM-flow simulator with a rock-physics simulator such that both reservoir quantities and petro-elastic quantities can be calculated. Inherit the flow class, and use super to call similar functions. In the end, the barycenter and moment of interia for the bulkimpedance objects, are returned as observations. The objects are identified using k-means clustering, and the number of objects are determined using and elbow strategy.</p>"},{"location":"reference/simulator/flow_rock/#simulator.flow_rock.flow_rock","title":"<code>flow_rock</code>","text":"<p>               Bases: <code>flow</code></p> <p>Couple the OPM-flow simulator with a rock-physics simulator such that both reservoir quantities and petro-elastic quantities can be calculated. Inherit the flow class, and use super to call similar functions.</p>"},{"location":"reference/simulator/flow_rock/#simulator.flow_rock.flow_sim2seis","title":"<code>flow_sim2seis</code>","text":"<p>               Bases: <code>flow</code></p> <p>Couple the OPM-flow simulator with a sim2seis simulator such that both reservoir quantities and petro-elastic quantities can be calculated. Inherit the flow class, and use super to call similar functions.</p>"},{"location":"reference/simulator/opm/","title":"opm","text":"<p>Wrap OPM-flow</p>"},{"location":"reference/simulator/opm/#simulator.opm.ebos","title":"<code>ebos</code>","text":"<p>               Bases: <code>eclipse</code></p> <p>Class for running OPM ebos with Eclipse input files. Inherits eclipse parent class for setting up and running simulations, and reading the results.</p>"},{"location":"reference/simulator/opm/#simulator.opm.ebos.call_sim","title":"<code>call_sim(folder=None, wait_for_proc=False)</code>","text":"<p>Call OPM flow simulator via shell.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Folder with runfiles.</p> <code>None</code> <code>wait_for_proc</code> <code>bool</code> <p>Determines whether to wait for the process to be done or not.</p> <code>False</code> Changelog <ul> <li>RJL 27/08-19</li> </ul>"},{"location":"reference/simulator/opm/#simulator.opm.ebos.check_sim_end","title":"<code>check_sim_end(finished_member=None)</code>","text":"<p>Check in RPT file for \"End of simulation\" to see if OPM ebos is done.</p> Changelog <ul> <li>RJL 27/08-19</li> </ul>"},{"location":"reference/simulator/opm/#simulator.opm.flow","title":"<code>flow</code>","text":"<p>               Bases: <code>eclipse</code></p> <p>Class for running OPM flow with Eclipse input files. Inherits eclipse parent class for setting up and running simulations, and reading the results.</p>"},{"location":"reference/simulator/opm/#simulator.opm.flow.SLURM_ARRAY_HPC_run","title":"<code>SLURM_ARRAY_HPC_run(n_e, venv, filename, **kwargs)</code>","text":"<p>HPC run manager for Slurm array jobs. Each ensemble member runs independently in its own task.</p> <p>Parameters:</p> Name Type Description Default <code>n_e</code> <code>list[int]</code> <p>Indices of ensemble members to simulate.</p> required <code>venv</code> <code>str</code> <p>Path to Python virtual environment activate script.</p> required <code>filename</code> <code>str</code> <p>Simulation input file.</p> required <code>kwargs</code> <code>dict</code> <p>Extra simulation options. Recognized: - sim_limit (float seconds or str HH:MM:SS) - mem (default \"4G\") - cpus_per_task (default 2)</p> <code>{}</code>"},{"location":"reference/simulator/opm/#simulator.opm.flow.SLURM_HPC_run","title":"<code>SLURM_HPC_run(n_e, venv, filename, **kwargs)</code>","text":"<p>HPC run manager for SLURM.</p> <p>This function will start num_runs of sim.call_sim() using job arrays in SLURM.</p>"},{"location":"reference/simulator/opm/#simulator.opm.flow.are_jobs_done","title":"<code>are_jobs_done(job_id)</code>","text":"<p>Check if all job array tasks are completed using sacct.</p>"},{"location":"reference/simulator/opm/#simulator.opm.flow.call_sim","title":"<code>call_sim(folder=None, wait_for_proc=False)</code>","text":"<p>Call OPM flow simulator via shell.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Folder with runfiles.</p> <code>None</code> <code>wait_for_proc</code> <code>bool</code> <p>Boolean determining if we wait for the process to be done or not.</p> <code>False</code> Changelog <ul> <li>ST 18/10-18</li> </ul>"},{"location":"reference/simulator/opm/#simulator.opm.flow.check_sim_end","title":"<code>check_sim_end(finished_member=None)</code>","text":"<p>Check in RPT file for \"End of simulation\" to see if OPM flow is done.</p> Changelog <ul> <li>ST 19/10-18</li> </ul>"},{"location":"reference/simulator/opm/#simulator.opm.flow.wait_for_jobs","title":"<code>wait_for_jobs(job_id, wait_time=10)</code>","text":"<p>Wait until all job array tasks are completed.</p>"},{"location":"reference/simulator/simple_models/","title":"simple_models","text":"<p>A collection of trivial toy models.</p>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.lin_1d","title":"<code>lin_1d</code>","text":"<p>linear 1x150 model (or whatever), just make observations of the state at given positions.</p>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.lin_1d.__init__","title":"<code>__init__(input_dict=None, m=None)</code>","text":"<p>Two inputs here. A dictionary of keys, or parameter directly.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>dict</code> <p>Dictionary containing all information required to run the simulator. It may come from, for example, an init file.</p> <code>None</code> <code>m</code> <code>int</code> <p>Parameter to make predicted data.</p> <code>None</code> Changelog <ul> <li>ST 7/9-15</li> </ul>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.nonlin_onedimmodel","title":"<code>nonlin_onedimmodel</code>","text":"<p>Class of simple 1D forward model for testing purposes.</p>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.nonlin_onedimmodel.__init__","title":"<code>__init__(input_dict=None)</code>","text":"<p>Two inputs here. A dictionary of keys, or parameter directly.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <p>contains all information the run the simulator (may come from, e.g., an init file)</p> <code>None</code>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.sevenmountains","title":"<code>sevenmountains</code>","text":"<p>The objective function is the elevations of the seven mountains around bergen, to test optimization algorithm</p>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.sevenmountains.__init__","title":"<code>__init__(input_dict=None, state=None)</code>","text":"<p>Two inputs here. A dictionary of keys, or parameter directly.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>dict</code> <p>contains all information the run the simulator (may come from, e.g., an init file)</p> <code>None</code> <code>state</code> <code>any</code> <p>Parameter to make predicted data</p> <code>None</code> Changelog <ul> <li>ST 9/5-18</li> </ul>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.sevenmountains.call_sim","title":"<code>call_sim(path=None)</code>","text":"<p>Run the simple 1D forward model</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Alternative folder where the MARE2DEM input files are located.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>d</code> <code>object</code> <p>Predicted data.</p> Changelog <ul> <li>ST 3/6-16</li> </ul>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.sevenmountains.check_sim_end","title":"<code>check_sim_end(current_run)</code>","text":"<p>Check if a simulation that has run in the background is finished. For ensemble-based methods, there can possibly be several folders with simulations, and each folder must be checked for finished runs. To check if simulation is done we search for .resp file which is the output in a successful  run.</p> <p>Parameters:</p> Name Type Description Default <code>current_run</code> <code>list</code> <p>List of ensemble members currently running simulation.</p> required <p>Returns:</p> Name Type Description <code>member</code> <code>int</code> <p>Ensemble member that is finished.</p> Changelog <ul> <li>ST 9/5-18</li> </ul>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.sevenmountains.get_sim_results","title":"<code>get_sim_results(which_resp, ext_data_info=None, member=None)</code>","text":"<p>Get forward simulation results. Simply load the numpy array...</p> <p>Parameters:</p> Name Type Description Default <code>which_resp</code> <code>str</code> <p>Specifies which of the responses is to be outputted (just one data type in this case).</p> required <code>member</code> <code>int</code> <p>Ensemble member that is finished.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>Array containing the predicted data (response).</p> Changelog <ul> <li>ST 3/6-16</li> </ul>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.sevenmountains.run_fwd_sim","title":"<code>run_fwd_sim(en_member=None, folder=os.getcwd(), wait_for_proc=False)</code>","text":"<p>Set up and run a forward simulation in an fwd_sim. The parameters for the forward simulation is set in setup_fwd_run. All the steps to set up and run a forward simulation is done in this object.</p> <p>Parameters:</p> Name Type Description Default <code>en_member</code> <code>int</code> <p>Index of the ensemble member to be run.</p> <code>None</code> <code>folder</code> <code>str</code> <p>Folder where the forward simulation is run.</p> <code>getcwd()</code> Changelog <ul> <li>ST 3/6-16</li> </ul>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.sevenmountains.setup_fwd_run","title":"<code>setup_fwd_run(state, assim_ind=None, true_ind=None)</code>","text":"<p>Set input parameters from an fwd_sim in the simulation to get predicted data. Parameters can be an ensemble or a single array.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary of input parameter. It can be either a single 'state' or an ensemble of 'state'.</p> required <p>Other Parameters:</p> Name Type Description <code>true_ind</code> <code>list</code> <p>The list of observed data assimilation indices.</p> <code>assim_ind</code> <code>list</code> <p>List with information on assimilation order for ensemble-based methods.</p> Changelog <ul> <li>ST 3/6-16</li> </ul>"},{"location":"reference/simulator/rockphysics/","title":"rockphysics","text":"<p>Compute elastic properties.</p>"},{"location":"reference/simulator/rockphysics/standardrp/","title":"standardrp","text":"<p>Descriptive description.</p>"},{"location":"reference/simulator/rockphysics/standardrp/#simulator.rockphysics.standardrp.elasticproperties","title":"<code>elasticproperties</code>","text":"<p>Calculate elastic properties from standard rock-physics models, specifically following Batzle and Wang, Geophysics, 1992, for fluid properties, and Report 1 in Abul Fahimuddin's thesis at Universty of Bergen (2010) for other properties.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; porosity = 0.2\n... pressure = 5\n... phases = [\"Oil\",\"Water\"]\n... saturations = [0.3, 0.5]\n...\n... satrock = Elasticproperties()\n... satrock.calc_props(phases, saturations, pressure, porosity)\n</code></pre>"},{"location":"reference/simulator/rockphysics/standardrp/#simulator.rockphysics.standardrp.elasticproperties.setup_fwd_run","title":"<code>setup_fwd_run(state)</code>","text":"<p>Setup the input parameters to be used in the PEM simulator. Parameters can be a an ensemble or a single array. State is set as an attribute of the simulator, and the correct value is determined in self.pem.calc_props()</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary of input parameters or states.</p> required Changelog <ul> <li>KF 11/12-2018</li> </ul>"},{"location":"tutorials/","title":"Examples","text":"<p>Here are some tutorials.</p> <ul> <li><code>tutorial_pipt.ipynb</code>: Tutorial for running PIPT</li> <li><code>tutorial_pipt.ipynb</code>: Tutorial for running POPT</li> </ul>"},{"location":"tutorials/pipt/tutorial_pipt/","title":"Tutorial for running the Python Inverse Problem Toolbox (PIPT)","text":"In\u00a0[1]: Copied! <pre># Set width\nfrom IPython.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.container { width:50% !important; }&lt;/style&gt;\"))\n\n# Import global modules\nimport numpy as np\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt  \n\n# Import local modules\nfrom pipt.loop.ensemble import Ensemble # this class contains the data\nfrom pipt.loop.assimilation import Assimilate # this class contains the iterative assimilation loop\nfrom subsurface.multphaseflow.opm import flow # the simulator we want to use\nfrom input_output import read_config # functions for reading input\nfrom pipt import pipt_init # script for initializing the module with the data assimilation method \nfrom plot_objective_function import combined # plot the data mismatch\nfrom plot_parameters import plot_layer, export_to_grid # plot the parameters\nfrom plot_data import plot_prod # plot the production data\n</pre> # Set width from IPython.display import display, HTML display(HTML(\"\"))  # Import global modules import numpy as np from glob import glob import os import matplotlib.pyplot as plt    # Import local modules from pipt.loop.ensemble import Ensemble # this class contains the data from pipt.loop.assimilation import Assimilate # this class contains the iterative assimilation loop from subsurface.multphaseflow.opm import flow # the simulator we want to use from input_output import read_config # functions for reading input from pipt import pipt_init # script for initializing the module with the data assimilation method  from plot_objective_function import combined # plot the data mismatch from plot_parameters import plot_layer, export_to_grid # plot the parameters from plot_data import plot_prod # plot the production data <p>Set the random seed:</p> In\u00a0[2]: Copied! <pre>np.random.seed(10)\n</pre> np.random.seed(10)     <p>Remove old results and folders, if present:</p> In\u00a0[3]: Copied! <pre>for folder in glob('En_*'):\n    shutil.rmtree(folder)\nfor file in glob('debug_analysis_step_*'):\n    os.remove(file)\n</pre> for folder in glob('En_*'):     shutil.rmtree(folder) for file in glob('debug_analysis_step_*'):     os.remove(file) <p>Read inputfile. In this tutorial the input file is written as a .toml file, and consists of two main keys: dataassim and fwdsim. The first part contains the options for the data assimilation algorithm and the second part are options related to the forward simulation model. The description of all keys are provided in the printouts of method docstrings below.</p> In\u00a0[4]: Copied! <pre>!cat 3D_ESMDA.toml\nkd, kf, ke = read_config.read_toml('3D_ESMDA.toml')\n</pre> !cat 3D_ESMDA.toml kd, kf, ke = read_config.read_toml('3D_ESMDA.toml') <pre>[ensemble]\r\nne = 50.0\r\nstate = \"permx\"\r\nprior_permx = [[\"vario\", \"sph\"], [\"mean\", \"priormean.npz\"], [\"var\", 1.0], [\"range\", 10.0], [\"aniso\", 1.0],\r\n               [\"angle\", 0.0], [\"grid\", [10.0, 10.0, 2.0]]]\r\n               \r\n[dataassim]\r\ndaalg = [\"esmda\", \"esmda\"]\r\nanalysis = \"approx\"\r\nenergy = 98.0\r\nobsvarsave = \"yes\"\r\nrestartsave = \"no\"\r\nanalysisdebug = [\"pred_data\", \"state\", \"data_misfit\", \"prev_data_misfit\"]\r\nrestart = \"no\"\r\nobsname = \"days\"\r\ntruedataindex = [400, 800, 1200, 1600, 2000, 2400, 2800, 3200, 3600, 4000]\r\ntruedata = \"true_data.csv\"\r\nassimindex = [0,1,2,3,4,5,6,7,8,9]\r\ndatatype = [\"WOPR PRO1\", \"WOPR PRO2\", \"WOPR PRO3\", \"WWPR PRO1\", \"WWPR PRO2\",\r\n            \"WWPR PRO3\", \"WWIR INJ1\", \"WWIR INJ2\", \"WWIR INJ3\"]\r\nstaticvar = \"permx\"\r\ndatavar = \"var.csv\"\r\nmda = [ [\"tot_assim_steps\", 3], ['inflation_param', [2, 4, 4]] ]\r\n\r\n[fwdsim]\r\nreporttype = \"days\"\r\nreportpoint = [400, 800, 1200, 1600, 2000, 2400, 2800, 3200, 3600, 4000]\r\nreplace = \"yes\"\r\nsaveforecast = \"yes\"\r\nsim_limit = 300.0\r\nrerun = 1\r\nrunfile = \"runfile\"\r\ndatatype = [\"WOPR PRO1\", \"WOPR PRO2\", \"WOPR PRO3\", \"WWPR PRO1\", \"WWPR PRO2\",\r\n            \"WWPR PRO3\", \"WWIR INJ1\", \"WWIR INJ2\", \"WWIR INJ3\"]\r\nparallel = 4\r\nstartdate = \"1/1/2022\"\r\n</pre> <p>Initialize the simulator with simulator keys.</p> In\u00a0[5]: Copied! <pre>sim = flow(kf)\nprint(flow.__init__.__doc__)\n</pre> sim = flow(kf) print(flow.__init__.__doc__) <pre>\n        The inputs are all optional, but in the same fashion as the other simulators a system must be followed.\n        The input_dict can be utilized as a single input. Here all nescessary info is stored. Alternatively,\n        if input_dict is not defined, all the other input variables must be defined.\n\n        Parameters\n        ----------\n        input_dict : dict, optional\n            Dictionary containing all information required to run the simulator.\n\n                - parallel: number of forward simulations run in parallel\n                - simoptions: options for the simulations\n                    - mpi: option to use mpi (always use &gt; 2 cores)\n                    - sim_path: Path to the simulator\n                    - sim_flag: Flags sent to the simulator (see simulator documentation for all possibilities)\n                - sim_limit: maximum number of seconds a simulation can run before being killed\n                - runfile: name of the simulation input file\n                - reportpoint: these are the dates the simulator reports results\n                - reporttype: this key states that the report poins are given as dates\n                - datatype: the data types the simulator reports\n\n        filename : str, optional\n            Name of the .mako file utilized to generate the ECL input .DATA file. Must be in uppercase for the\n            ECL simulator.\n\n        options : dict, optional\n            Dictionary with options for the simulator.\n\n        Returns\n        -------\n        initial_object : object\n            Initial object from the class ecl_100.\n        \n</pre> <p>Print the Ensemble options:</p> In\u00a0[6]: Copied! <pre>print(Ensemble.__init__.__doc__)\n</pre> print(Ensemble.__init__.__doc__) <pre>\n        Parameters\n        ----------\n        keys_da : dict\n            Options for the data assimilation class\n\n            - daalg: spesification of the method, first the main type (e.g., \"enrml\"), then the solver (e.g., \"gnenrml\")\n            - analysis: update flavour (\"approx\", \"full\" or \"subspace\")\n            - energy: percent of singular values kept after SVD\n            - obsvarsave: save the observations as a file (default false)\n            - restart: restart optimization from a restart file (default false)\n            - restartsave: save a restart file after each successful iteration (defalut false)\n            - analysisdebug: specify which class variables to save to the result files\n            - truedataindex: order of the simulated data (for timeseries this is points in time)\n            - obsname: unit for truedataindex (for timeseries this is days or hours or seconds, etc.)\n            - truedata: the data, e.g., provided as a .csv file\n            - assimindex: index for the data that will be used for assimilation\n            - datatype: list with the name of the datatypes\n            - staticvar: name of the static variables\n            - datavar: data variance, e.g., provided as a .csv file\n\n        keys_en : dict\n            Options for the ensemble class\n\n            - ne: number of perturbations used to compute the gradient\n            - state: name of state variables passed to the .mako file\n            - prior_&lt;name&gt;: the prior information the state variables, including mean, variance and variable limits\n\n        sim : callable\n            The forward simulator (e.g. flow)\n        \n</pre> <p>Example using ESMDA. The input and available options are given below. During assimilation, useful information is written to the screen. The same information is also written to a log-file named pet_logger.log.</p> In\u00a0[7]: Copied! <pre>analysis = pipt_init.init_da(kd, ke, sim)\n\nprint(analysis.__init__.__doc__)\nassimilation = Assimilate(analysis)\nassimilation.run()\n</pre> analysis = pipt_init.init_da(kd, ke, sim)  print(analysis.__init__.__doc__) assimilation = Assimilate(analysis) assimilation.run() <pre>Single entry for VARIO will be copied to all 2 layers\nSingle entry for VAR will be copied to all 2 layers\nSingle entry for ANISO will be copied to all 2 layers\nSingle entry for ANGLE will be copied to all 2 layers\nSingle entry for RANGE will be copied to all 2 layers\n\n        The class is initialized by passing the keywords and simulator object upwards in the hierarchy.\n\n        Parameters\n        ----------\n        keys_da['mda']: list\n            - tot_assim_steps: total number of iterations in MDA, e.g., 3\n            - inflation_param: covariance inflation factors, e.g., [2, 4, 4]\n\n        keys_en : dict\n\n        sim : callable\n</pre> <pre>\rIterations (Obj. func. val: ):   0%|                                                                                                                                                                                             | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/50 [00:00&lt;?, ?it/s]</pre> <pre>\rIterations (Obj. func. val: ):  25%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                       | 1/4 [00:06&lt;00:19,  6.58s/it]</pre> <pre>  0%|          | 0/50 [00:00&lt;?, ?it/s]</pre> <pre>Iterations (Obj. func. val:749926.2 Reduced: 95 %):  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                | 2/4 [00:13&lt;00:13,  6.71s/it]</pre> <pre>  0%|          | 0/50 [00:00&lt;?, ?it/s]</pre> <pre>Iterations (Obj. func. val:163815.9 Reduced: 78 %):  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                        | 3/4 [00:20&lt;00:06,  6.88s/it]</pre> <pre>  0%|          | 0/50 [00:00&lt;?, ?it/s]</pre> <pre>Iterations (Obj. func. val:14396.2 Reduced: 91 %): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:27&lt;00:00,  6.76s/it]</pre> <pre>Convergence was met. Obj. function reduced from 14994770.0 to 14396.2\n</pre> <pre>\n</pre> <p>Plot the data mismatch:</p> In\u00a0[8]: Copied! <pre>combined()\n</pre> combined() <p>Plot the prior and posterior permeability in the upper layer:</p> In\u00a0[9]: Copied! <pre>export_to_grid('permx')\nplot_layer('permx', [2, 10, 10])\n</pre> export_to_grid('permx') plot_layer('permx', [2, 10, 10]) In\u00a0[10]: Copied! <pre>plot_prod()\n</pre> plot_prod() <pre>WOPR PRO1\n</pre> <pre>WOPR PRO2\n</pre> <pre>WOPR PRO3\n</pre> <pre>WWPR PRO1\n</pre> <pre>WWPR PRO2\n</pre> <pre>WWPR PRO3\n</pre> <pre>WWIR INJ1\n</pre> <pre>WWIR INJ2\n</pre> <pre>WWIR INJ3\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/pipt/tutorial_pipt/#tutorial-for-running-the-python-inverse-problem-toolbox-pipt","title":"Tutorial for running the Python Inverse Problem Toolbox (PIPT)\u00b6","text":"<p>As an illustrative example we choose a small 3D-field with three producers and three (water) injectors. The figure below shows the true (data generating) permeability field and the well positions. The grid is 10x10x2, and the porosity is 0.2. The inverse problem is to find the permeability for the reservoir by assimilation produced water and oil and injected water.</p> <p> The first step is to load neccessary external and local modules.</p>"},{"location":"tutorials/pipt/tutorial_pipt/#setting-up-the-mako-file","title":"Setting up the .mako file\u00b6","text":"<p>The data assimilation relies on a .mako file for writing the current state variables to the flow simulator input. In this case, the flow simulator is opm-flow opm-projects.org, and the input file is provided as a text file (.DATA file). The .mako file is created by replacing the keywords PERMX in the .DATA file with:</p> <pre><code>PERMX\n% for i in range(0, len(permx)):\n% if permx[i] &lt; 6:\n${\"%.3f\" %(np.exp(permx[i]))}\n% else:\n${\"%.3f\" %(np.exp(6))}\n% endif\n% endfor\n/</code></pre>"},{"location":"tutorials/pipt/tutorial_pipt/#running-locally","title":"Running locally\u00b6","text":"<p>It is recommended to run the notebook from a virtual environment. Follow these steps to run this notebook on your own computer:</p> <p>Step 1: Create virtual environment as normal</p> <pre><code>python3 -m venv pet_venv</code></pre> <p>Then activate the environment using:</p> <pre><code>source pet_venv/bin/activate</code></pre> <p>Step 2: Install Jupyter Notebook into virtual environment</p> <pre><code>python3 -m pip install ipykernel</code></pre> <p>Step 3: Install PET in the virtual environment, see PET installation</p> <p>Step 4: Install Plotting in the virtual environment, see Plotting installation</p> <p>Step 5: Allow Jupyter access to the kernel within the virtual environment</p> <pre><code>python3 -m ipykernel install --user --name=pet_venv</code></pre> <p>Start jupyter notebook, and load tutorial_popt.ipynb (this file). On the jupyter notebook toolbar, select \u2018Kernel\u2019 and \u2018Change Kernel\u2019. The new kernel is now be available in the list for selection:</p> <p></p>"},{"location":"tutorials/popt/tutorial_popt/","title":"Tutorial for running the Python Optimization Toolbox (POPT)","text":"In\u00a0[10]: Copied! <pre># Set width\nfrom IPython.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.container { width:50% !important; }&lt;/style&gt;\"))\n\n# Import global modules\nimport os\nimport glob\nimport shutil\nimport logging\nfrom glob import glob\nfrom copy import deepcopy\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt  \nfrom scipy.stats import expon\n\n# Import local modules\nfrom input_output import read_config # functions for reading input\nfrom popt.misc_tools import optim_tools as ot # help functions for optimization\nfrom popt.loop.optimize import Optimize # this class contains the iterative loop\nfrom popt.loop.ensemble import Ensemble # this class contains the control pertrubations and gradient calculations\nfrom simulator.opm import flow # the simulator we want to use\nfrom popt.update_schemes.enopt import EnOpt # the standard EnOpt method\nfrom popt.update_schemes.smcopt import SmcOpt # the sequential Monte Carlo method\nfrom popt.cost_functions.npv import npv # the cost function\n</pre> # Set width from IPython.display import display, HTML display(HTML(\"\"))  # Import global modules import os import glob import shutil import logging from glob import glob from copy import deepcopy import numpy as np from scipy.optimize import minimize import matplotlib.pyplot as plt   from scipy.stats import expon  # Import local modules from input_output import read_config # functions for reading input from popt.misc_tools import optim_tools as ot # help functions for optimization from popt.loop.optimize import Optimize # this class contains the iterative loop from popt.loop.ensemble import Ensemble # this class contains the control pertrubations and gradient calculations from simulator.opm import flow # the simulator we want to use from popt.update_schemes.enopt import EnOpt # the standard EnOpt method from popt.update_schemes.smcopt import SmcOpt # the sequential Monte Carlo method from popt.cost_functions.npv import npv # the cost function <p>Set the random seed:</p> In\u00a0[11]: Copied! <pre>np.random.seed(101122)\n</pre> np.random.seed(101122)     <p>The plottting function is used to display the objective function vs. iterations for different methods:</p> In\u00a0[12]: Copied! <pre>def plot_obj_func():\n    \n    # Collect all results\n    path_to_files = './'\n    path_to_figures = './'  # Save here\n    if not os.path.exists(path_to_figures):\n        os.mkdir(path_to_figures)\n    files = os.listdir(path_to_files)\n    results = [name for name in files if \"optimize_result\" in name]\n    num_iter = len(results)\n\n    mm = []\n    for iter in range(num_iter):\n        info = np.load(str(path_to_files) + 'optimize_result_{}.npz'.format(iter))\n        if 'best_func' in info:\n            mm.append(-info['best_func'])\n        else:\n            mm.append(-info['obj_func_values'])\n\n    f = plt.figure()\n    plt.plot(mm, 'bs-')\n    plt.xticks(range(num_iter))\n    plt.xticks(fontsize = 14)\n    plt.yticks(fontsize = 14)\n    plt.xlabel('Iteration no.', size=14)\n    plt.ylabel('NPV', size=14)\n    plt.title('Objective function', size=14)\n    f.tight_layout(pad=2.0)\n    plt.show()\n</pre> def plot_obj_func():          # Collect all results     path_to_files = './'     path_to_figures = './'  # Save here     if not os.path.exists(path_to_figures):         os.mkdir(path_to_figures)     files = os.listdir(path_to_files)     results = [name for name in files if \"optimize_result\" in name]     num_iter = len(results)      mm = []     for iter in range(num_iter):         info = np.load(str(path_to_files) + 'optimize_result_{}.npz'.format(iter))         if 'best_func' in info:             mm.append(-info['best_func'])         else:             mm.append(-info['obj_func_values'])      f = plt.figure()     plt.plot(mm, 'bs-')     plt.xticks(range(num_iter))     plt.xticks(fontsize = 14)     plt.yticks(fontsize = 14)     plt.xlabel('Iteration no.', size=14)     plt.ylabel('NPV', size=14)     plt.title('Objective function', size=14)     f.tight_layout(pad=2.0)     plt.show() <p>Remove old results and folders, if present:</p> In\u00a0[13]: Copied! <pre>for folder in glob('En_*'):\n    shutil.rmtree(folder)\nfor file in glob('optimize_result_*'):\n    os.remove(file)\n</pre> for folder in glob('En_*'):     shutil.rmtree(folder) for file in glob('optimize_result_*'):     os.remove(file) <p>Read inputfile. In this tutorial the input file is written as a .toml file, and consists of three main keys: ensemble, optim and fwdsim. The first part contains keys related to the ensemble, the second part contains the options for the optimization algorithm and the third part are options related to the forward simulation model and objective function. The description of all keys are provided in the printouts of method docstrings below.</p> In\u00a0[14]: Copied! <pre>!cat init_optim.toml\nko, kf, ke = read_config.read_toml('init_optim.toml')\n</pre> !cat init_optim.toml ko, kf, ke = read_config.read_toml('init_optim.toml') <pre>[ensemble]\r\ndisable_tqdm = true\r\nne = 10\r\nstate = [\"injbhp\",\"prodbhp\"]\r\nprior_injbhp = [\r\n    [\"mean\",\"init_injbhp.npz\"],\r\n    [\"var\",6250.0],\r\n    [\"limits\",100.0,500.0]\r\n]\r\nprior_prodbhp = [\r\n    [\"mean\",\"init_prodbhp.npz\"],\r\n    [\"var\",6250.0,],\r\n    [\"limits\",20.0,300.0]\r\n]\r\nnum_models = 1\r\ntransform = true\r\n\r\n[optim]\r\nmaxiter = 5\r\ntol = 1e-06\r\nalpha = 0.2\r\nbeta = 0.1\r\nalpha_maxiter = 3\r\nresample = 0\r\noptimizer = 'GA'\r\nnesterov = true\r\nrestartsave = true\r\nrestart = false\r\nhessian = false\r\ninflation_factor = 10\r\nsavedata = [\"alpha\",\"obj_func_values\"]\r\n\r\n[fwdsim]\r\nnpv_const = [\r\n    [\"wop\",283.05],\r\n    [\"wgp\",0.0],\r\n    [\"wwp\",37.74],\r\n    [\"wwi\",12.58],\r\n    [\"disc\",0.08],\r\n    [\"obj_scaling\",-1.0e6]\r\n]\r\nparallel = 2\r\nsimoptions = [\r\n    ['mpi', 'mpirun -np 3'],\r\n    ['sim_path', '/usr/bin/'],\r\n    ['sim_flag', '--tolerance-mb=1e-5 --parsing-strictness=low']\r\n]\r\nsim_limit = 5.0\r\nrunfile = \"3well\"\r\nreportpoint = [\r\n    1994-02-09 00:00:00,\r\n    1995-01-01 00:00:00,\r\n    1996-01-01 00:00:00,\r\n    1997-01-01 00:00:00,\r\n    1998-01-01 00:00:00,\r\n    1999-01-01 00:00:00,\r\n]\r\nreporttype = \"dates\"\r\ndatatype = [\"fopt\",\"fgpt\",\"fwpt\",\"fwit\"]\r\n</pre> <p>Set initial pressure (note that the filenames correspond to the names given in the input file above):</p> In\u00a0[15]: Copied! <pre>init_injbhp = np.array([300.0,250.0])\ninit_prodbhp = np.array([100.0])\nnp.savez('init_injbhp.npz', init_injbhp)\nnp.savez('init_prodbhp.npz', init_prodbhp)\n</pre> init_injbhp = np.array([300.0,250.0]) init_prodbhp = np.array([100.0]) np.savez('init_injbhp.npz', init_injbhp) np.savez('init_prodbhp.npz', init_prodbhp) <p>Initialize the simulator with simulator keys.</p> In\u00a0[16]: Copied! <pre>sim = flow(kf)\nprint(flow.__init__.__doc__)\n</pre> sim = flow(kf) print(flow.__init__.__doc__) <pre>\n        The inputs are all optional, but in the same fashion as the other simulators a system must be followed.\n        The input_dict can be utilized as a single input. Here all nescessary info is stored. Alternatively,\n        if input_dict is not defined, all the other input variables must be defined.\n\n        Parameters\n        ----------\n        input_dict : dict, optional\n            Dictionary containing all information required to run the simulator.\n\n                - parallel: number of forward simulations run in parallel\n                - simoptions: options for the simulations\n                    - mpi: option to use mpi (always use &gt; 2 cores)\n                    - sim_path: Path to the simulator\n                    - sim_flag: Flags sent to the simulator (see simulator documentation for all possibilities)\n                - sim_limit: maximum number of seconds a simulation can run before being killed\n                - runfile: name of the simulation input file\n                - reportpoint: these are the dates the simulator reports results\n                - reporttype: this key states that the report poins are given as dates\n                - datatype: the data types the simulator reports\n\n        filename : str, optional\n            Name of the .mako file utilized to generate the ECL input .DATA file. Must be in uppercase for the\n            ECL simulator.\n\n        options : dict, optional\n            Dictionary with options for the simulator.\n\n        Returns\n        -------\n        initial_object : object\n            Initial object from the class ecl_100.\n        \n</pre> <p>Initialize the ensemble with ensemble keys, the simulator, and the chosen objective function. Here we also extract the initial state (x0), the covariance (cov), and the bounds.</p> In\u00a0[17]: Copied! <pre>ensemble = Ensemble(ke, sim, npv)\nprint(Ensemble.__init__.__doc__)\nx0 = ensemble.get_state()\ncov = ensemble.get_cov()\nbounds = ensemble.get_bounds()\n</pre> ensemble = Ensemble(ke, sim, npv) print(Ensemble.__init__.__doc__) x0 = ensemble.get_state() cov = ensemble.get_cov() bounds = ensemble.get_bounds() <pre>\n        Parameters\n        ----------\n        keys_en : dict\n            Options for the ensemble class\n\n            - disable_tqdm: supress tqdm progress bar for clean output in the notebook\n            - ne: number of perturbations used to compute the gradient\n            - state: name of state variables passed to the .mako file\n            - prior_&lt;name&gt;: the prior information the state variables, including mean, variance and variable limits\n            - num_models: number of models (if robust optimization) (default 1)\n            - transform: transform variables to [0,1] if true (default true)\n\n        sim : callable\n            The forward simulator (e.g. flow)\n\n        obj_func : callable\n            The objective function (e.g. npv)\n        \n</pre> <p>Example using EnOpt. The input and available options are given below. During optimization, useful information is written to the screen. The same information is also written to a log-file named popt.log.</p> In\u00a0[18]: Copied! <pre>print(EnOpt.__init__.__doc__)\nEnOpt(ensemble.function, x0, args=(cov,), jac=ensemble.gradient, hess=ensemble.hessian, bounds=bounds, **ko)\n</pre> print(EnOpt.__init__.__doc__) EnOpt(ensemble.function, x0, args=(cov,), jac=ensemble.gradient, hess=ensemble.hessian, bounds=bounds, **ko) <pre>\n        Parameters\n        ----------\n            fun: callable\n                objective function\n\n            x: ndarray\n                Initial state\n\n            args: tuple\n                Initial covariance\n\n            jac: callable\n                Gradient function\n\n            hess: callable\n                Hessian function\n\n            bounds: list, optional\n                (min, max) pairs for each element in x. None is used to specify no bound.\n\n            options: dict\n                Optimization options\n\n                    - maxiter: maximum number of iterations (default 10)\n                    - restart: restart optimization from a restart file (default false)\n                    - restartsave: save a restart file after each successful iteration (defalut false)\n                    - tol: convergence tolerance for the objective function (default 1e-6)\n                    - alpha: step size for the steepest decent method (default 0.1)\n                    - beta: momentum coefficient for running accelerated optimization (default 0.0)\n                    - alpha_maxiter: maximum number of backtracing trials (default 5)\n                    - resample: number indicating how many times resampling is tried if no improvement is found\n                    - optimizer: 'GA' (gradient accent) or Adam (default 'GA')\n                    - nesterov: use Nesterov acceleration if true (default false)\n                    - hessian: use Hessian approximation (if the algorithm permits use of Hessian) (default false)\n                    - normalize: normalize the gradient if true (default true)\n                    - cov_factor: factor used to shrink the covariance for each resampling trial (defalut 0.5)\n                    - savedata: specify which class variables to save to the result files (state, objective\n                                function value, iteration number, number of function evaluations, and number\n                                of gradient evaluations, are always saved)\n        \n</pre> <pre>2023-12-14 10:16:03,832 : INFO : popt.loop.optimize :        ====== Running optimization - EnOpt ======\n2023-12-14 10:16:03,833 : INFO : popt.loop.optimize : \n{'alpha': 0.2,\n 'alpha_maxiter': 3,\n 'beta': 0.1,\n 'datatype': ['fopt', 'fgpt', 'fwpt', 'fwit'],\n 'hessian': False,\n 'inflation_factor': 10,\n 'maxiter': 5,\n 'nesterov': True,\n 'optimizer': 'GA',\n 'resample': 0,\n 'restart': False,\n 'restartsave': True,\n 'savedata': ['alpha', 'obj_func_values'],\n 'tol': 1e-06}\n2023-12-14 10:16:03,833 : INFO : popt.loop.optimize :        iter       alpha_iter obj_func        step-size       cov[0,0]        \n2023-12-14 10:16:03,834 : INFO : popt.loop.optimize :        0                     -1.9083e-01    \n2023-12-14 10:16:13,088 : INFO : popt.loop.optimize :        1          0          -3.1499e-01     2.00e-01        3.97e-02       \n2023-12-14 10:16:21,543 : INFO : popt.loop.optimize :        2          0          -3.7002e-01     2.00e-01        3.97e-02       \n2023-12-14 10:16:33,583 : INFO : popt.loop.optimize :        3          0          -3.7252e-01     2.00e-01        3.95e-02       \n2023-12-14 10:16:43,758 : INFO : popt.loop.optimize :        4          0          -3.7253e-01     2.00e-01        3.93e-02       \n2023-12-14 10:16:53,204 : INFO : popt.loop.optimize :        5          0          -3.7260e-01     2.00e-01        3.97e-02       \n2023-12-14 10:16:53,211 : INFO : popt.loop.optimize :        Optimization converged in 5 iterations \n2023-12-14 10:16:53,213 : INFO : popt.loop.optimize :        Optimization converged with final obj_func = -0.3726\n2023-12-14 10:16:53,216 : INFO : popt.loop.optimize :        Total number of function evaluations = 6\n2023-12-14 10:16:53,218 : INFO : popt.loop.optimize :        Total number of jacobi evaluations = 5\n2023-12-14 10:16:53,221 : INFO : popt.loop.optimize :        Total elapsed time = 0.84 minutes\n2023-12-14 10:16:53,227 : INFO : popt.loop.optimize :        ============================================\n</pre> Out[18]: <pre>&lt;popt.update_schemes.enopt.EnOpt at 0x7f44d18aae80&gt;</pre> <p>Finally, plot the objective function using the function defined above:</p> In\u00a0[19]: Copied! <pre>plot_obj_func()\n</pre> plot_obj_func() <p>Example using the sequential Monte Carlo method. Here we also save the best state (which is not the mean of the ensemble simulations) and the corresponding best objective function value:</p> In\u00a0[20]: Copied! <pre>ko_smc = deepcopy(ko)\nko_smc['savedata'] += [\"best_state\", \"best_func\"]\nprint(SmcOpt.__init__.__doc__)\nSmcOpt(ensemble.function, x0, args=(cov,), sens=ensemble.calc_ensemble_weights, bounds=bounds, **ko_smc)\n</pre> ko_smc = deepcopy(ko) ko_smc['savedata'] += [\"best_state\", \"best_func\"] print(SmcOpt.__init__.__doc__) SmcOpt(ensemble.function, x0, args=(cov,), sens=ensemble.calc_ensemble_weights, bounds=bounds, **ko_smc) <pre>\n        Parameters\n        ----------\n            fun: callable\n                objective function\n\n            x: ndarray\n                Initial state\n\n            sens: callable\n                Ensemble sensitivity\n\n            bounds: list, optional\n                (min, max) pairs for each element in x. None is used to specify no bound.\n\n            options: dict\n                Optimization options\n\n                - maxiter: maximum number of iterations (default 10)\n                - restart: restart optimization from a restart file (default false)\n                - restartsave: save a restart file after each successful iteration (defalut false)\n                - tol: convergence tolerance for the objective function (default 1e-6)\n                - alpha: weight between previous and new step (default 0.1)\n                - alpha_maxiter: maximum number of backtracing trials (default 5)\n                - resample: number indicating how many times resampling is tried if no improvement is found\n                - cov_factor: factor used to shrink the covariance for each resampling trial (defalut 0.5)\n                - inflation_factor: term used to weight down prior influence (defalult 1)\n                - savedata: specify which class variables to save to the result files (state, objective function\n                            value, iteration number, number of function evaluations, and number of gradient\n                            evaluations, are always saved)\n        \n</pre> <pre>2023-12-14 10:17:21,181 : INFO : popt.loop.optimize :        ====== Running optimization - SmcOpt ======\n2023-12-14 10:17:21,182 : INFO : popt.loop.optimize : \n{'alpha': 0.2,\n 'alpha_maxiter': 3,\n 'beta': 0.1,\n 'datatype': ['fopt', 'fgpt', 'fwpt', 'fwit'],\n 'hessian': False,\n 'inflation_factor': 10,\n 'maxiter': 5,\n 'nesterov': True,\n 'optimizer': 'GA',\n 'resample': 0,\n 'restart': False,\n 'restartsave': True,\n 'savedata': ['alpha', 'obj_func_values', 'best_state', 'best_func'],\n 'tol': 1e-06}\n2023-12-14 10:17:21,182 : INFO : popt.loop.optimize :        iter       alpha_iter obj_func        step-size       \n2023-12-14 10:17:21,183 : INFO : popt.loop.optimize :        0                     -1.9083e-01    \n2023-12-14 10:17:30,338 : INFO : popt.loop.optimize :        1          0          -3.6047e-01     2.00e-01       \n2023-12-14 10:17:39,446 : INFO : popt.loop.optimize :        2          0          -3.7190e-01     2.00e-01       \n2023-12-14 10:17:49,220 : INFO : popt.loop.optimize :        3          0          -3.7443e-01     2.00e-01       \n2023-12-14 10:17:58,141 : INFO : popt.loop.optimize :        4          0          -3.7443e-01     2.00e-01       \n2023-12-14 10:18:10,110 : INFO : popt.loop.optimize :        5          0          -3.7443e-01     2.00e-01       \n2023-12-14 10:18:10,112 : INFO : popt.loop.optimize :        Optimization converged in 5 iterations \n2023-12-14 10:18:10,113 : INFO : popt.loop.optimize :        Optimization converged with final obj_func = -0.3672\n2023-12-14 10:18:10,113 : INFO : popt.loop.optimize :        Total number of function evaluations = 6\n2023-12-14 10:18:10,114 : INFO : popt.loop.optimize :        Total number of jacobi evaluations = 5\n2023-12-14 10:18:10,114 : INFO : popt.loop.optimize :        Total elapsed time = 0.83 minutes\n2023-12-14 10:18:10,114 : INFO : popt.loop.optimize :        ============================================\n</pre> Out[20]: <pre>&lt;popt.update_schemes.smcopt.SmcOpt at 0x7f453c9638b0&gt;</pre> <p>Plot the objective function using the function defined above:</p> In\u00a0[21]: Copied! <pre>plot_obj_func()\n</pre> plot_obj_func() <p>Example using ensemble gradient approximation with the conjugate gradient (CG) method from scipy.minimize:</p> In\u00a0[22]: Copied! <pre>res = minimize(ensemble.function, x0, args=(cov,), method='CG', jac=ensemble.gradient, tol=ko['tol'],\n               callback=ot.save_optimize_results, bounds=bounds, options=ko)\nprint(res)\n</pre> res = minimize(ensemble.function, x0, args=(cov,), method='CG', jac=ensemble.gradient, tol=ko['tol'],                callback=ot.save_optimize_results, bounds=bounds, options=ko) print(res) <pre> message: Maximum number of iterations has been exceeded.\n success: False\n  status: 1\n     fun: -0.37443170946723164\n       x: [-1.023e-01 -1.007e-01 -2.088e-01]\n     nit: 5\n     jac: [ 0.000e+00  2.207e-06  0.000e+00]\n    nfev: 21\n    njev: 21\n</pre> <p>Example calling EnOpt through scipy.minimize (this does exactly the same as running EnOpt, but with a different random seed since we do not reset the seed):</p> In\u00a0[23]: Copied! <pre>for file in glob('optimize_result_*'):\n    os.remove(file)\nminimize(ensemble.function, x0, args=(cov,), method=EnOpt, jac=ensemble.gradient, hess=ensemble.hessian,\n         bounds=bounds, options=ko)\n</pre> for file in glob('optimize_result_*'):     os.remove(file) minimize(ensemble.function, x0, args=(cov,), method=EnOpt, jac=ensemble.gradient, hess=ensemble.hessian,          bounds=bounds, options=ko) <pre>2023-12-14 10:22:29,796 : INFO : popt.loop.optimize :        ====== Running optimization - EnOpt ======\n2023-12-14 10:22:29,797 : INFO : popt.loop.optimize : \n{'alpha': 0.2,\n 'alpha_maxiter': 3,\n 'beta': 0.1,\n 'callback': None,\n 'constraints': (),\n 'datatype': ['fopt', 'fgpt', 'fwpt', 'fwit'],\n 'hessian': False,\n 'hessp': None,\n 'inflation_factor': 10,\n 'maxiter': 5,\n 'nesterov': True,\n 'optimizer': 'GA',\n 'resample': 0,\n 'restart': False,\n 'restartsave': True,\n 'savedata': ['alpha', 'obj_func_values'],\n 'tol': 1e-06}\n2023-12-14 10:22:29,797 : INFO : popt.loop.optimize :        iter       alpha_iter obj_func        step-size       cov[0,0]        \n2023-12-14 10:22:29,798 : INFO : popt.loop.optimize :        0                     -1.9083e-01    \n2023-12-14 10:22:38,680 : INFO : popt.loop.optimize :        1          0          -3.0921e-01     2.00e-01        3.90e-02       \n2023-12-14 10:22:47,943 : INFO : popt.loop.optimize :        2          0          -3.6664e-01     2.00e-01        3.90e-02       \n2023-12-14 10:23:00,146 : INFO : popt.loop.optimize :        Optimization converged in 2 iterations \n2023-12-14 10:23:00,147 : INFO : popt.loop.optimize :        Optimization converged with final obj_func = -0.3666\n2023-12-14 10:23:00,147 : INFO : popt.loop.optimize :        Total number of function evaluations = 3\n2023-12-14 10:23:00,147 : INFO : popt.loop.optimize :        Total number of jacobi evaluations = 2\n2023-12-14 10:23:00,148 : INFO : popt.loop.optimize :        Total elapsed time = 0.52 minutes\n2023-12-14 10:23:00,148 : INFO : popt.loop.optimize :        ============================================\n</pre> Out[23]: <pre>&lt;popt.update_schemes.enopt.EnOpt at 0x7f453c96d310&gt;</pre> <p>Plot the objective function using the function defined above:</p> In\u00a0[24]: Copied! <pre>plot_obj_func()\n</pre> plot_obj_func() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/popt/tutorial_popt/#tutorial-for-running-the-python-optimization-toolbox-popt","title":"Tutorial for running the Python Optimization Toolbox (POPT)\u00b6","text":"<p>As an illustrative example we choose a 2D-field with one producer and two (water) injectors. The figure below shows the permeability field and the well positions. The grid is 100x100, and the porosity is 0.18. The optimization problem is to find the pressure control for the wells in order to maximize net present value.</p> <p></p> The first step is to load neccessary external and local modules."},{"location":"tutorials/popt/tutorial_popt/#setting-up-the-mako-file","title":"Setting up the .mako file\u00b6","text":"<p>The optimization relies on a .mako file for writing the current control variables to the flow simulator input. In this case, the flow simulator is opm-flow opm-projects.org, and the input file is provided as a text file (.DATA file). The .mako file is created by replacing the keywords WCONINJE and WCONPROD in the .DATA file with:</p> <pre><code>WCONINJE\n'INJ-1'  WATER 'OPEN' BHP 2* ${injbhp[0]} /\n'INJ-2'  WATER 'OPEN' BHP 2* ${injbhp[1]} /\n/\n\nWCONPROD\n 'PRO-1' 'OPEN' BHP 5* ${prodbhp[0]} /\n/</code></pre>"},{"location":"tutorials/popt/tutorial_popt/#running-locally","title":"Running locally\u00b6","text":"<p>It is recommended to run the notebook from a virtual environment. Follow these steps to run this notebook on your own computer:</p> <p>Step 1: Create virtual environment as normal</p> <pre><code>python3 -m venv pet_venv</code></pre> <p>Then activate the environment using:</p> <pre><code>source pet_venv/bin/activate</code></pre> <p>Step 2: Install Jupyter Notebook into virtual environment</p> <pre><code>python3 -m pip install ipykernel</code></pre> <p>Step 3: Install PET in the virtual environment, see PET installation</p> <p>Step 4: Allow Jupyter access to the kernel within the virtual environment</p> <pre><code>python3 -m ipykernel install --user --name=pet_venv</code></pre> <p>Start jupyter notebook, and load tutorial_popt.ipynb (this file). On the jupyter notebook toolbar, select \u2018Kernel\u2019 and \u2018Change Kernel\u2019. The new kernel is now be available in the list for selection:</p> <p></p>"}]}