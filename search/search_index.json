{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Home"},{"location":"#pet-python-ensemble-toolbox","title":"PET: Python Ensemble Toolbox","text":"<p>PET is a toolbox for ensemble-based Data Assimilation and Optimisation. It is developed and maintained by the eponymous group at NORCE Norwegian Research Centre AS.</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>Before installing ensure you have python3 pre-requisites. On a Debian system run:</p> <pre><code>sudo upt-get update\nsudo apt-get install python3\nsudo apt-get install python3-pip\nsudo apt-get install python3-venv\n</code></pre> <p>To install PET, first clone the repo (assuming you have added the SSH key)</p> <pre><code>git clone git@github.com:Python-Ensemble-Toolbox/PET.git PET\n</code></pre> <p>Make sure you have the latest version of <code>pip</code> and <code>setuptools</code>:</p> <pre><code>python3 -m pip install --upgrade pip setuptools\n</code></pre> <p>Optionally (but recommended): Create and activate a virtual environment:</p> <pre><code>python3 -m venv venv-PET\nsource venv-PET/bin/activate\n</code></pre> <p>Some additional features might be not part of your default installation and need to be set in the Python (virtual) environment manually:</p> <pre><code>python3 -m pip install wheel\npython3 setup.py bdist_wheel\n</code></pre> <p>If you do not install PET inside a virtual environment, you may have to include the <code>--user</code> option in the following (to install to your local Python site packages, usually located in <code>~/.local</code>).</p> <p>Inside the PET folder, run</p> <pre><code>python3 -m pip install -e .\n</code></pre> <ul> <li>The dot is needed to point to the current directory.</li> <li>The <code>-e</code> option installs PET such that changes to it take effect immediately   (without re-installation).</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>PET needs to be set up with a configuration file. See the example repository for inspiration.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li>A PIPT tutorial is found here</li> <li>A POPT tutorial is found here</li> </ul>"},{"location":"#suggested-readings","title":"Suggested readings:","text":"<p>If you use PET in a scientific publication, we would appreciate it if you cited one of the first papers where the PET was introduced. Each of them describes some of the PET's functionalities:</p>"},{"location":"#bayesian-data-assimilation-with-enrml-and-es-mda-for-history-matching-workflow-with-ai-geomodeling","title":"Bayesian data assimilation with EnRML and ES-MDA for History-Matching Workflow with AI-Geomodeling","text":""},{"location":"#cite-as","title":"Cite as","text":"<p>Fossum, Kristian, Sergey Alyaev, and Ahmed H. Elsheikh. \"Ensemble history-matching workflow using interpretable SPADE-GAN geomodel.\" First Break 42.2 (2024): 57-63. https://doi.org/10.3997/1365-2397.fb2024014</p> <pre><code>@article{fossum2024ensemble,\n  title={Ensemble history-matching workflow using interpretable SPADE-GAN geomodel},\n  author={Fossum, Kristian and Alyaev, Sergey and Elsheikh, Ahmed H},\n  journal={First Break},\n  volume={42},\n  number={2},\n  pages={57--63},\n  year={2024},\n  publisher={European Association of Geoscientists \\&amp; Engineers},\n  url = {https://doi.org/10.3997/1365-2397.fb2024014}\n}\n</code></pre>"},{"location":"#bayesian-inversion-technique-localization-and-data-compression-for-history-matching-of-the-edvard-grieg-field-using-4d-seismic-data","title":"Bayesian inversion technique, localization, and data compression for history matching of the Edvard Grieg field using 4D seismic data","text":""},{"location":"#cite-as_1","title":"Cite as","text":"<p>Lorentzen, R.J., Bhakta, T., Fossum, K. et al. Ensemble-based history matching of the Edvard Grieg field using 4D seismic data. Comput Geosci 28, 129\u2013156 (2024). https://doi.org/10.1007/s10596-024-10275-0</p> <pre><code>@article{lorentzen2024ensemble,\n  title={Ensemble-based history matching of the Edvard Grieg field using 4D seismic data},\n  author={Lorentzen, Rolf J and Bhakta, Tuhin and Fossum, Kristian and Haugen, Jon Andr{\\'e} and Lie, Espen Oen and Ndingwan, Abel Onana and Straith, Knut Richard},\n  journal={Computational Geosciences},\n  volume={28},\n  number={1},\n  pages={129--156},\n  year={2024},\n  publisher={Springer},\n  url={https://doi.org/10.1007/s10596-024-10275-0}\n}\n</code></pre>"},{"location":"#offshore-wind-farm-layout-optimization-using-ensemble-methods","title":"Offshore wind farm layout optimization using ensemble methods","text":""},{"location":"#cite-as_2","title":"Cite as","text":"<p>Eikrem, K.S., Lorentzen, R.J., Faria, R. et al. Offshore wind farm layout optimization using ensemble methods. Renewable Energy 216, 119061 (2023). https://www.sciencedirect.com/science/article/pii/S0960148123009758</p> <pre><code>@article{Eikrem2023offshore,\ntitle = {Offshore wind farm layout optimization using ensemble methods},\njournal = {Renewable Energy},\nvolume = {216},\npages = {119061},\nyear = {2023},\nissn = {0960-1481},\ndoi = {https://doi.org/10.1016/j.renene.2023.119061},\nurl = {https://www.sciencedirect.com/science/article/pii/S0960148123009758},\nauthor = {Kjersti Solberg Eikrem and Rolf Johan Lorentzen and Ricardo Faria and Andreas St{\\o}rksen Stordal and Alexandre Godard},\nkeywords = {Wind farm layout optimization, Ensemble optimization (EnOpt and EPF-EnOpt), Constrained optimization, Levelized cost of energy (LCOE), Floating offshore wind},\n}\n</code></pre>"},{"location":"dev_guide/","title":"Developer guide","text":""},{"location":"dev_guide/#writing-documentation","title":"Writing documentation","text":"<p>The documentation is built with <code>mkdocs</code>.</p> <ul> <li>It should be written in the syntax of markdown.</li> <li>The syntax is further augmented by several pymdown plugins.</li> <li>Docstrings are processed as above, but should also   declare parameters and return values in the style of numpy,   and <code>&gt;&gt;&gt;</code> markers must follow the \"Examples\" section.</li> </ul> <p>Note</p> <p>You can preview the rendered html docs by running <pre><code>mkdocs serve\n</code></pre></p> <ul> <li>Temporarily disable <code>mkdocs-jupyter</code> in <code>mkdocs.yml</code> to speed up build reloads.</li> <li>Set <code>validation: unrecognized_links: warn</code> to get warnings about linking issues.</li> </ul> <p>A summary of how to add cross-reference links is given below.</p>"},{"location":"dev_guide/#linking-to-pages","title":"Linking to pages","text":"<p>You should use relative page links, including the <code>.md</code> extension. For example, <code>[link label](sibling-page.md)</code>.</p> <p>The following works, but does not get validated! <code>[link label](../sibling-page)</code></p> <p>Why not absolute links?</p> <p>The downside of relative links is that if you move/rename source or destination, then they will need to be changed, whereas only the destination needs be watched when using absolute links.</p> <p>Previously, absolute links were not officially supported by MkDocs, meaning \"not modified at all\". Thus, if made like so <code>[label](/PET/references)</code>, i.e. without <code>.md</code> and including <code>/PET</code>, then they would work (locally with <code>mkdocs serve</code> and with GitHub hosting). Since #3485 you can instead use <code>[label](/references)</code> i.e. omitting <code>PET</code> (or whatever domain sub-dir is applied in <code>site_url</code>) by setting <code>mkdocs.yml: validation: absolute_links: relative_to_docs</code>. A different workaround is the <code>mkdocs-site-url</code> plugin.</p> <p>Either way</p> <p>It will not be link that your editor can follow to the relevant markdown file (unless you create a symlink in your file system root?) nor will GitHub's internal markdown rendering manage to make sense of it, so my advise is not to use absolute links.</p>"},{"location":"dev_guide/#linking-to-headersanchors","title":"Linking to headers/anchors","text":"<p>Thanks to the <code>autorefs</code> plugin, links to headings (including page titles) don't even require specifying the page path! Syntax: <code>[visible label][link]</code> i.e. double pairs of brackets. Shorthand: <code>[link][]</code>.</p> <p>Info</p> <ul> <li>Clearly, non-unique headings risk being confused with others in this way.</li> <li>The link (anchor) must be lowercase!</li> </ul> <p>This facilitates linking to</p> <ul> <li>API (code reference) items.   For example, <code>[`da_methods.ensemble`][]</code>,   where the backticks are optional (makes the link look like a code reference).</li> <li>References. For example <code>[`bocquet2016`][]</code>,</li> </ul>"},{"location":"dev_guide/#docstring-injection","title":"Docstring injection","text":"<p>Use the following syntax to inject the docstring of a code object.</p> <pre><code>::: da_methods.ensemble\n</code></pre> <p>But we generally don't do so manually. Instead it's taken care of by the reference generation via <code>docs/gen_ref_pages.py</code>.</p>"},{"location":"dev_guide/#including-other-files","title":"Including other files","text":"<p>The <code>pymdown</code> extension \"snippets\" enables the following syntax to include text from other files.</p> <p><code>--8&lt;-- \"/path/from/project/root/filename.ext\"</code></p>"},{"location":"dev_guide/#adding-to-the-examples","title":"Adding to the examples","text":"<p>Example scripts are very useful, and contributions are very desirable.  As well as showcasing some feature, new examples should make sure to reproduce some published literature results.  After making the example, consider converting the script to the Jupyter notebook format (or vice versa) so that the example can be run on Colab without users needing to install anything (see <code>docs/examples/README.md</code>). This should be done using the <code>jupytext</code> plug-in (with the <code>lightscript</code> format), so that the paired files can be kept in synch.</p>"},{"location":"dev_guide/#bibliography","title":"Bibliography","text":"<p>In order to add new references, insert their bibtex into <code>docs/bib/refs.bib</code>, then run <code>docs/bib/bib2md.py</code> which will format and add entries to <code>docs/references.md</code> that can be cited with regular cross-reference syntax, e.g. <code>[bocquet2010a][]</code>.</p>"},{"location":"dev_guide/#hosting","title":"Hosting","text":"<p>The above command is run by a GitHub Actions workflow whenever the <code>master</code> branch gets updated. The <code>gh-pages</code> branch is no longer being used. Instead actions/deploy-pages creates an artefact that is deployed to Github Pages.</p>"},{"location":"dev_guide/#tests","title":"Tests","text":"<p>The test suite is orchestrated using <code>pytest</code>. Both in CI and locally. I.e. you can run the tests simply by the command</p> <pre><code>pytest\n</code></pre> <p>It will discover all appropriately named tests in the source (see the <code>tests</code> dir).</p> <p>Use (for example) <code>pytest --doctest-modules some_file.py</code> to also run any example code within docstrings.</p> <p>We should also soon make use of a config file (for example <code>pyproject.toml</code>) for <code>pytest</code>.</p>"},{"location":"references/","title":"Bibliography","text":""},{"location":"references/#emerick2016a","title":"<code>emerick2016a</code>","text":"<p>Alexandre A. Emerick. <code>\"Analysis of the performance of ensemble-based assimilation of production and seismic data\"</code>. Journal of Petroleum Science and Engineering, 139:219\u2013239, 2016.</p>"},{"location":"references/#evensen2009a","title":"<code>evensen2009a</code>","text":"<p>Geir Evensen. Data Assimilation. Springer, 2 edition, 2009.</p>"},{"location":"references/#emerick2013a","title":"<code>emerick2013a</code>","text":"<p>Alexandre A. Emerick and Albert C. Reynolds. <code>\"Ensemble smoother with multiple data assimilation\"</code>. Computers &amp; Geosciences, 55:3\u201315, 2013.</p>"},{"location":"references/#rafiee2017","title":"<code>rafiee2017</code>","text":"<p>Javad Rafiee and Albert C Reynolds. <code>\"Theoretical and efficient practical procedures for the generation of inflation factors for es-mda\"</code>. Inverse Problems, 33(11):115003, 2017.</p>"},{"location":"references/#chen2013","title":"<code>chen2013</code>","text":"<p>Yan Chen and Dean S. Oliver. <code>\"Levenberg\u2013Marquardt forms of the iterative ensemble smoother for efficient history matching and uncertainty quantification\"</code>. Computational Geosciences, 17(4):689\u2013703, 2013.</p>"},{"location":"references/#raanes2019","title":"<code>raanes2019</code>","text":"<p>Patrick Nima Raanes, Andreas St\u00f8rksen Stordal, and Geir Evensen. <code>\"Revising the stochastic iterative ensemble smoother\"</code>. Nonlinear Processes in Geophysics, 26(3):325\u2013338, 2019. doi:10.5194/npg-26-325-2019.</p>"},{"location":"references/#evensen2019","title":"<code>evensen2019</code>","text":"<p>Geir Evensen, Patrick N. Raanes, Andreas S. Stordal, and Joakim Hove. <code>\"Efficient implementation of an iterative ensemble smoother for data assimilation and reservoir history matching\"</code>. Frontiers in Applied Mathematics and Statistics, 5:47, 2019. doi:10.3389/fams.2019.00047.</p>"},{"location":"references/#kingma2014","title":"<code>kingma2014</code>","text":"<p>Diederik P Kingma. <code>\"Adam: a method for stochastic optimization\"</code>. arXiv preprint arXiv:1412.6980, 2014.</p>"},{"location":"references/#hansen2006","title":"<code>hansen2006</code>","text":"<p>Nikolaus Hansen. <code>\"The CMA evolution strategy: a comparing review\"</code>. Towards a new evolutionary computation: Advances in the estimation of distribution algorithms, pages 75\u2013102, 2006.</p>"},{"location":"reference/SUMMARY/","title":"Code reference","text":"<p>Use links in sidebar to navigate the code docstrings.</p>"},{"location":"reference/ensemble/","title":"ensemble","text":"<p>Multiple realisations management.</p>"},{"location":"reference/ensemble/ensemble/","title":"ensemble","text":"<p>Package contains the basis for the PET ensemble based structure.</p>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble","title":"<code>Ensemble</code>","text":"<p>Class for organizing misc. variables and simulator for an ensemble-based inversion run. Here, the forecast step and prediction runs are performed. General methods that are useful in various ensemble loops have also been implemented here.</p>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble.__init__","title":"<code>__init__(keys_en, sim, redund_sim=None)</code>","text":"<p>Class extends the ReadInitFile class. First the PIPT init. file is passed to the parent class for reading and parsing. Rest of the initialization uses the keywords parsed in ReadInitFile (parent) class to set up observed, predicted data and data variance dictionaries. Also, the simulator to be used in forecast and/or predictions is initialized with keywords parsed in ReadInitFile (parent) class. Lastly, the initial ensemble is generated (if it has not been inputted), and some saving of variables can be done chosen in PIPT init. file.</p> Parameter <p>init_file : str             path to input file containing initiallization values</p>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble.calc_ml_prediction","title":"<code>calc_ml_prediction(enX=None)</code>","text":"<p>Function for running the simulator over several levels. We assume that it is sufficient to provide the level integer to the setup of the forward run. This will initiate the correct simulator fidelity. The function then runs the set of state through the different simulator fidelities.</p> <p>Parameters:</p> Name Type Description Default <code>enX</code> <p>If simulation is run stand-alone one can input any state.</p> <code>None</code>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble.calc_prediction","title":"<code>calc_prediction(enX=None, save_prediction=None)</code>","text":"<p>Method for making predictions using the state variable. Will output the simulator response for all report steps and all data values provided to the simulator.</p> <p>Parameters:</p> Name Type Description Default <code>input_state</code> <p>Use an input state instead of internal state (stored in self) to run predictions</p> required <code>save_prediction</code> <p>Save the predictions as a .npz file (numpy compressed file) <code>None</code> <p>Returns:</p> Name Type Description <code>prediction</code> <p>List of dictionaries with keys equal to data types (in DATATYPE), containing the responses at each time step given in PREDICTION.</p>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble.get_list_assim_steps","title":"<code>get_list_assim_steps()</code>","text":"<p>Returns list of assimilation steps. Useful in a 'loop'-script.</p> <p>Returns:</p> Name Type Description <code>list_assim</code> <code>list</code> <p>List of total assimilation steps.</p>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble.load","title":"<code>load()</code>","text":"<p>Load a pickled file and save all info. in self.</p> Changelog <ul> <li>ST 28/2-17</li> </ul>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.Ensemble.save","title":"<code>save()</code>","text":"<p>We use pickle to dump all the information we have in 'self'. Can be used, e.g., if some error has occurred.</p> Changelog <ul> <li>ST 28/2-17</li> </ul>"},{"location":"reference/input_output/","title":"input_output","text":"<p>File-based communication and storage.</p>"},{"location":"reference/input_output/get_ecl_key_val/","title":"get_ecl_key_val","text":"<p>Descriptive description.</p>"},{"location":"reference/input_output/organize/","title":"organize","text":"<p>Descriptive description.</p>"},{"location":"reference/input_output/organize/#input_output.organize.Organize_input","title":"<code>Organize_input</code>","text":""},{"location":"reference/input_output/read_config/","title":"read_config","text":"<p>Parse config files.</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.check_mand_keywords_da","title":"<code>check_mand_keywords_da(keys_da)</code>","text":"<p>Check for mandatory keywords in <code>DATAASSIM</code> part, and output error if they are not present</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.check_mand_keywords_en","title":"<code>check_mand_keywords_en(keys_en)</code>","text":"<p>Check for mandatory keywords in <code>ENSEMBLE</code> part, and output error if they are not present</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.check_mand_keywords_fwd","title":"<code>check_mand_keywords_fwd(keys_fwd)</code>","text":"<p>Check for mandatory keywords in <code>FWDSIM</code> part, and output error if they are not present</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.check_mand_keywords_opt","title":"<code>check_mand_keywords_opt(keys_opt)</code>","text":"<p>Check for mandatory keywords in <code>OPTIM</code> part, and output error if they are not present</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.parse_keywords","title":"<code>parse_keywords(lines)</code>","text":"<p>Here we parse the lines in the init. file to a Python dictionary. The keys of the dictionary is the keywords in the PIPT init. file, and the information in each keyword is stored in each key of the dictionary. To know how the keyword-information is organized in the keys of the dictionary, confront the manual located in the doc folder.</p> <p>Parameters:</p> Name Type Description Default <code>lines</code> <code>list</code> <p>List of (clean) lines from the PIPT init. file.</p> required <p>Returns:</p> Name Type Description <code>keys</code> <code>dict</code> <p>Dictionary with all info. from the init. file.</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.read","title":"<code>read(filename)</code>","text":"<p>Read configuration file. Supported formats are toml, .yaml, .pipt and .popt.</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.read_clean_file","title":"<code>read_clean_file(init_file)</code>","text":"<p>Read PIPT init. file and lines that are not comments (marked with octothorpe)</p> <p>Parameters:</p> Name Type Description Default <code>init_file</code> <code>str</code> <p>Name of file to remove all comments. WHOLE filename needed (with suffix!)</p> required <p>Returns:</p> Name Type Description <code>lines</code> <code>list</code> <p>Lines from init. file converted to list entries</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.read_toml","title":"<code>read_toml(init_file)</code>","text":"<p>Read .toml configuration file, parse and output dictionaries for PIPT/POPT</p> <p>Parameters:</p> Name Type Description Default <code>init_file</code> <code>str</code> <p>toml configuration file</p> required"},{"location":"reference/input_output/read_config/#input_output.read_config.read_txt","title":"<code>read_txt(init_file)</code>","text":"<p>Read a PIPT or POPT input file (.pipt or .popt), parse and output dictionaries for data assimilation or optimization,  and simulator classes.</p> <p>Parameters:</p> Name Type Description Default <code>init_file</code> <code>str</code> <p>PIPT init. file containing info. to run the inversion algorithm</p> required <p>Returns:</p> Name Type Description <code>keys_pr</code> <code>dict</code> <p>Parsed keywords from DATAASSIM or OPTIM</p> <code>keys_fwd</code> <code>dict</code> <p>Parsed keywords from FWDSSIM</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.read_yaml","title":"<code>read_yaml(init_file)</code>","text":"<p>Read .yaml input file, parse and return dictionaries for PIPT/POPT.</p> <p>Parameters:</p> Name Type Description Default <code>init_file</code> <code>str</code> <p>.yaml file</p> required <p>Returns:</p> Name Type Description <code>keys_da</code> <code>dict</code> <p>Parsed keywords from dataassim</p> <code>keys_fwd</code> <code>dict</code> <p>Parsed keywords from fwdsim</p>"},{"location":"reference/input_output/read_config/#input_output.read_config.remove_empty_lines","title":"<code>remove_empty_lines(lines)</code>","text":"<p>Small method for finding empty lines in a read file.</p> <p>Parameters:</p> Name Type Description Default <code>lines</code> <code>list</code> <p>List of lines from a file</p> required <p>Returns:</p> Name Type Description <code>lines_clean</code> <code>list</code> <p>List of clean lines (without empty entries)</p>"},{"location":"reference/misc/","title":"misc","text":"<p>More tools.</p>"},{"location":"reference/misc/ecl/","title":"ecl","text":"<p>Read Schlumberger Eclipse output files.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase","title":"<code>EclipseCase</code>","text":"<p>               Bases: <code>object</code></p> <p>Read data for an Eclipse simulation case.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.__init__","title":"<code>__init__(casename)</code>","text":"<p>Initialize the Eclipse case.</p> <p>Parameters:</p> Name Type Description Default <code>casename</code> <code>str</code> <p>Path to the case, with or without extension.</p> required <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.at","title":"<code>at(when)</code>","text":"<p>Recurrent data for a certain timestep.</p> <p>The result of this method is usually passed to functions that need to calculate something using several properties.</p> <p>Parameters:</p> Name Type Description Default <code>when</code> <code>datetime or int</code> <p>Date of the property.</p> required <p>Returns:</p> Type Description <code>EclipseRestart</code> <p>Object containing properties for this timestep.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.atsm","title":"<code>atsm(when)</code>","text":"<p>Recurrent data from summary file for a certain timestep.</p> <p>The result of this method is usually passed to functions that need to calculate something using several properties.</p> <p>Parameters:</p> Name Type Description Default <code>when</code> <code>datetime or int</code> <p>Date of the property.</p> required <p>Returns:</p> Type Description <code>EclipseSummary</code> <p>Object containing summary properties for this timestep.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.cell_data","title":"<code>cell_data(prop, when=None)</code>","text":"<p>Read cell-wise data from case. This can be either static information or recurrent information for a certain date.</p> <p>Parameters:</p> Name Type Description Default <code>prop</code> <code>str</code> <p>Name of the property, e.g., 'SWAT'.</p> required <code>when</code> <code>datetime or int</code> <p>Date of the property, or None if static.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Loaded array for the property.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; case = EclipseCase(cmd_args.filename)\n&gt;&gt;&gt; zmf2 = case.cell_data('ZMF2', datetime.datetime(2054, 7, 1))\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.components","title":"<code>components()</code>","text":"<p>Components that exist in the restart file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; case = EclipseCase(filename)\n&gt;&gt;&gt; comps = case.components()\n&gt;&gt;&gt; print(\"Number of components is %d\" % (len(comps)))\n&gt;&gt;&gt; for num, name in enumerate(comps):\n&gt;&gt;&gt;     print(\"%d : %s\" % (num, name))\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.field_data","title":"<code>field_data(prop, when=None)</code>","text":"<p>Read field-wise data from case. This can be either static information or recurrent information for a certain date.</p> <p>Parameters:</p> Name Type Description Default <code>prop</code> <code>str</code> <p>Name of the property, e.g., 'ZPHASE'.</p> required <code>when</code> <code>datetime or int</code> <p>Date of the property, or None if static.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Loaded array for the property.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; case = EclipseCase(cmd_args.filename)\n&gt;&gt;&gt; zphase = case.cell_data('ZPHASE', datetime.datetime(2054, 7, 1))\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.grid","title":"<code>grid()</code>","text":"<p>Grid structure for simulation case.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.phases","title":"<code>phases()</code>","text":"<p>Phases that exist in the restart file.</p> <p>case = EclipseCase (filename) phs = case.phases () print (\"Number of phases is %d\" % (len (phs))) print (\"Keyword for first phase is %s\" % (\"S\" + phs [0])) if Phase.oil in phs:     print (\"Oil is present\") else:     print (\"Oil is not present\")</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.report_dates","title":"<code>report_dates()</code>","text":"<p>List of all the report dates that are available in this case.</p> <p>Items from this list can be used as a parameter to <code>at</code> to get the case for that particular report step.</p> <p>Returns:</p> Type Description <code>list of datetime.datetime</code> <p>List of available report dates in sequence.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; for rstp in case.report_dates():\n&gt;&gt;&gt;     print(case.at(rstp).this_date)\n</code></pre> See Also <p>ecl.EclipseCase.at</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.shape","title":"<code>shape()</code>","text":"<p>Get shape of returned field data.</p> <p>Returns:</p> Type Description <code>tuple of int</code> <p>Shape of the field data as (num_k, num_j, num_i).</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.start_date","title":"<code>start_date()</code>","text":"<p>Starting date of the simulation</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseCase.summary_data","title":"<code>summary_data(prop, when)</code>","text":"<p>Read summary data from case. This is typically well data, but can also be, for example, newton iterations.</p> <p>Parameters:</p> Name Type Description Default <code>prop</code> <code>str</code> <p>Name of the property, e.g., 'WWPR PRO1'.</p> required <code>when</code> <code>datetime or int</code> <p>Date of the property.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Loaded array for the property.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; case = EclipseCase(cmd_args.filename)\n&gt;&gt;&gt; data = case.cell_data('WWIR INJ-5', datetime.datetime(2054, 7, 1))\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseData","title":"<code>EclipseData</code>","text":"<p>               Bases: <code>object</code></p> <p>Base class for both static and recurrent data.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseData.cell_data","title":"<code>cell_data(selector)</code>","text":"<p>Get a field property for every cell at this restart step.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>tuple</code> <p>Specification of the property to be loaded. This is a tuple starting with a Prop,  and then some context-dependent items.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of the data with inactive cells masked off.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pres = case.cell_data((Prop.pres,))\n&gt;&gt;&gt; swat = case.cell_data((Prop.sat, Phase.wat))\n&gt;&gt;&gt; xco2 = case.cell_data((Prop.mole, 'CO2', Phase.gas))\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseData.components","title":"<code>components()</code>","text":"<p>Components that exist in the restart file. Components used in the simulation are stored in all the restart files instead of once in the init file, since it is the restart file that hold component fields.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseData.field_data","title":"<code>field_data(propname)</code>","text":"<p>Get a property for the entire field at this restart step.</p> <p>Parameters:</p> Name Type Description Default <code>propname</code> <code>str</code> <p>Name of the property to be loaded.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of the data.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseData.summary_data","title":"<code>summary_data(propname)</code>","text":"<p>Get a property from the summary file at this restart step.</p> <p>Parameters:</p> Name Type Description Default <code>propname</code> <code>str</code> <p>Name of the property to be loaded. This is in the form 'mnemonic well',  e.g., 'WWIR I05'. Alternatively, propname can be either only well or  only mnemonic. Then the value for all mnemonics or all wells are given,  e.g., propname='WWIR' returns WWIR for all wells.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of the data.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseFile","title":"<code>EclipseFile</code>","text":"<p>               Bases: <code>object</code></p> <p>Low-level class to read records from binary files.</p> <p>Access to this object must be within a monitor (<code>with</code>-statement).</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseFile.__exit__","title":"<code>__exit__(typ, val, traceback)</code>","text":"<p>Close the underlaying file object when we go out of scope.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseFile.__init__","title":"<code>__init__(root, ext)</code>","text":"<p>Initialize file object from a path in the filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Stem of the file name (including directory).</p> required <code>ext</code> <code>str</code> <p>Extension of the file to read from.</p> required <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseFile.dump","title":"<code>dump(positional=False, fileobj=sys.stdout)</code>","text":"<p>Dump catalog contents of records in the datafile.</p> <p>Parameters:</p> Name Type Description Default <code>positional</code> <code>bool</code> <p>If True, the keywords should be sorted on position in the file.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; f = EclipseFile(\"foo\", 'INIT')\n&gt;&gt;&gt; f.dump()\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseFile.get","title":"<code>get(kwd, seq=0)</code>","text":"<p>Read a (potentially indexed) keyword from file.</p> <p>Parameters:</p> Name Type Description Default <code>kwd</code> <code>str</code> <p>Keyword to read.</p> required <code>seq</code> <code>int</code> <p>Sequence number, starting from zero.</p> <code>0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with EclipseFile('foo', 'EGRID') as grid:\n&gt;&gt;&gt;     zcorn = grid.get('ZCORN')\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseGrid","title":"<code>EclipseGrid</code>","text":"<p>               Bases: <code>object</code></p> <p>Corner-point geometry data from an Eclipse Extensive Grid file.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseGrid.grid","title":"<code>grid()</code>","text":"<p>Create a grid structure from the information read from file.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Grid structure.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # convert from .EGRID to .grdecl:\n&gt;&gt;&gt; import pyresito.io.ecl as ecl\n&gt;&gt;&gt; import pyresito.io.grdecl as grdecl\n&gt;&gt;&gt; grdecl.write('foo.grdecl', ecl.EclipseGrid('FOO').grid())\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseInit","title":"<code>EclipseInit</code>","text":"<p>               Bases: <code>EclipseData</code></p> <p>Read information from static data (init) file.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseRFT","title":"<code>EclipseRFT</code>","text":"<p>               Bases: <code>object</code></p> <p>Read data from an Eclipse RFT file.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseRFT.__init__","title":"<code>__init__(casename)</code>","text":"<p>Initialize the Eclipse case.</p> <p>Parameters:</p> Name Type Description Default <code>casename</code> <code>str</code> <p>Path to the case, with or without extension.</p> required <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseRFT.rft_data","title":"<code>rft_data(well, prop)</code>","text":"<p>Read the RFT data for the requested well.</p> <p>Parameters:</p> Name Type Description Default <code>well</code> <code>str</code> <p>Name of the well, e.g., 'PRO-1'.</p> required <code>prop</code> <code>str</code> <p>Type of property (depth, pressure, swat, or sgas).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Loaded array for the property.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; case = EclipseRFT(cmd_args.filename)\n&gt;&gt;&gt; data = case.rft_data(well='INJ-5', prop='PRESSURE')\n</code></pre>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseRestart","title":"<code>EclipseRestart</code>","text":"<p>               Bases: <code>EclipseData</code></p> <p>Read information from a recurrent data (restart) file.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseRestart.__init__","title":"<code>__init__(grid, seq)</code>","text":"<p>Initialize the restart file reader.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>EclipseInit or EclipseGrid</code> <p>Initialization file which contains grid dimensions.</p> required <code>seq</code> <code>int</code> <p>Run number.</p> required <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseRestart.date","title":"<code>date()</code>","text":"<p>Simulation date the restart file is created for.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseSummary","title":"<code>EclipseSummary</code>","text":"<p>               Bases: <code>EclipseData</code></p> <p>Read information from a recurrent data (summary) file.</p>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseSummary.__init__","title":"<code>__init__(grid, seq)</code>","text":"<p>Initialize the restart file reader.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>EclipseInit or EclipseGrid</code> <p>Initialization file which contains grid dimensions.</p> required <code>seq</code> <code>int</code> <p>Run number.</p> required <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/misc/ecl/#misc.ecl.EclipseSummary.date","title":"<code>date()</code>","text":"<p>Simulation date the restart file is created for.</p>"},{"location":"reference/misc/ecl/#misc.ecl.main","title":"<code>main(*args)</code>","text":"<p>Read a data file to see if it parses OK.</p>"},{"location":"reference/misc/ecl_common/","title":"ecl_common","text":"<p>Common definitions for all import filters</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from .ecl_common import Phase, Prop\n</code></pre>"},{"location":"reference/misc/grdecl/","title":"grdecl","text":"<p>Read Schlumberger Eclipse grid input files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pyresito.io.grdecl as grdecl\n&gt;&gt;&gt; grid = grdecl.read (\"FOO\")\n</code></pre>"},{"location":"reference/misc/grdecl/#misc.grdecl.GrdEclError","title":"<code>GrdEclError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception thrown if invalid syntax is encountered.</p>"},{"location":"reference/misc/grdecl/#misc.grdecl.main","title":"<code>main(*args)</code>","text":"<p>Read a data file to see if it parses OK.</p>"},{"location":"reference/misc/grdecl/#misc.grdecl.read","title":"<code>read(filename)</code>","text":"<p>Read an Eclipse input grid into a dictionary.</p>"},{"location":"reference/misc/grdecl/#misc.grdecl.read_prop","title":"<code>read_prop(fname, dims, typ=numpy.float64, prop=None, *, mask=None)</code>","text":"<p>Read a property from a text file into a dictionary. This is akin to Petrel's Import onto selection popup menu choice.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>File name to load property from. Currently, this must be a file in the filesystem and cannot be a file-like object.</p> required <code>dims</code> <code>tuple of int</code> <p>Dimensions of the data cube, in a format compatible with the return of the shape function, i.e. (nk, nj, ni).</p> required <code>typ</code> <code>dtype</code> <p>Data type of the data to be loaded.</p> <code>float64</code> <code>prop</code> <code>dict</code> <p>Dictionary where the keyword will be added. If this is None, then a new dictionary will be created.</p> <code>None</code> <code>mask</code> <code>ndarray</code> <p>Existing mask for inactive elements; this will be combined with the mask inherent in the data, if specified. If you have loaded the grid, this should be the ACTNUM field.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary where the property being read is added as a key.</p>"},{"location":"reference/misc/grdecl/#misc.grdecl.shape","title":"<code>shape(grdecl)</code>","text":"<p>Get shape of field data cube.</p> <p>Parameters:</p> Name Type Description Default <code>grdecl</code> <code>dict</code> <p>Corner-point grid structure.</p> required <p>Returns:</p> Type Description <code>tuple of int</code> <p>Shape of the field data cube as (num_k, num_j, num_i).</p>"},{"location":"reference/misc/grdecl/#misc.grdecl.write","title":"<code>write(filename, grid, dialect='ecl', multi_file=True)</code>","text":"<p>Write a grid to corner-point text format, formatted in such a way that it can easily be read again.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of the filename containing a wrapper for all the properties. This will also serve as the stem for all individual property files.</p> required <code>grid</code> <code>dict</code> <p>Grid object containing values for all properties.</p> required <code>dialect</code> <code>str</code> <p>Name of the simulator that we are writing files for. Currently, this must be 'ecl'.</p> <code>'ecl'</code> <code>multi_file</code> <code>bool</code> <p>Write properties in separate files, instead of putting everything in one large file.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/misc/grdecl/#misc.grdecl.write_compressed","title":"<code>write_compressed(fname, keyw, cube, *, fmt='12.6e')</code>","text":"<p>Write a data field as a keyword that can be included into a grid file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>Path to the output file to write to.</p> required <code>keyw</code> <code>str</code> <p>Name of the keyword to put in the header; this should not be greater than eight characters.</p> required <code>cube</code> <code>ndarray</code> <p>Data cube to be written.</p> required <code>fmt</code> <code>str</code> <p>Format of a single item, in the form used to specify formats in the built-in routines, but without percent or braces.</p> <code>'12.6e'</code> <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/misc/read_input_csv/","title":"read_input_csv","text":"<p>CSV and Pickle Data Reader Utilities</p> <p>This module provides utility functions for reading and processing data from CSV and pickle files. It supports various data formats including NumPy arrays, pandas DataFrames, and handles data type conversions for ensemble modeling and data assimilation workflows.</p> <p>Main Functions:     - read_data_df: Reads data from CSV/pickle files, returns as NumPy arrays or dictionaries     - read_var_df: Reads variance data from CSV/pickle files     - read_data_csv: Legacy CSV reading function with data flattening     - read_var_csv: Legacy variance CSV reading function     - convert_to_array: Converts string representations to NumPy arrays     - to_array_if_sequence: Converts various data types to NumPy array format</p> <p>Typical use cases:     - Loading observational data for data assimilation     - Reading ensemble data with various data types     - Processing CSV files with mixed data types and array-like strings     - Handling variance/uncertainty data alongside measurements</p> <p>Last Modified: February 2026</p>"},{"location":"reference/misc/read_input_csv/#misc.read_input_csv.convert_to_array","title":"<code>convert_to_array(array_str)</code>","text":"<p>Convert space-separated string representations of numbers to NumPy arrays.</p> <p>This function handles strings with space-separated numeric values and converts them back to NumPy arrays. It removes brackets and whitespace before parsing.</p> <p>Parameters:</p> Name Type Description Default <code>array_str</code> <code>str</code> <p>String containing space-separated numbers, optionally with brackets. Example: \"[1.0 2.0 3.0]\" or \"1.0 2.0 3.0\"</p> required <p>Returns:</p> Type Description <code>ndarray or str</code> <p>NumPy array of floats if conversion is successful, otherwise returns the original string unchanged.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; convert_to_array(\"1.0 2.0 3.0\")\narray([1., 2., 3.])\n&gt;&gt;&gt; convert_to_array(\"[1.0 2.0 3.0]\")\narray([1., 2., 3.])\n</code></pre>"},{"location":"reference/misc/read_input_csv/#misc.read_input_csv.read_data_csv","title":"<code>read_data_csv(filename, datatype, truedataindex)</code>","text":"<p>Read observational data from CSV files (legacy function).</p> <p>This is a legacy function for reading CSV files with flexible header configurations. Supports files with column headers, row headers, both, or neither. Handles missing values by replacing them with 'n/a'.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the CSV file.</p> required <code>datatype</code> <code>list of str</code> <p>Column names (or positional column identifiers) for data types to extract.</p> required <code>truedataindex</code> <code>list</code> <p>Row identifiers where observational data was recorded (e.g., time stamps, observation indices). Used to select specific rows from the CSV.</p> required <p>Returns:</p> Name Type Description <code>imported_data</code> <code>list of list</code> <p>2D list where each sublist represents a row of extracted data. Each element is either a float (numeric data) or string (text/missing data). Missing numeric values are replaced with 'n/a'.</p> Notes <ul> <li>If the first column is 'header_both', the CSV is assumed to have both   row and column headers.</li> <li>If row count matches len(truedataindex), assumes column headers exist.</li> <li>If row count is len(truedataindex)+1, assumes first row was misinterpreted   as header and re-reads it as data.</li> <li>NaN values in numeric columns are replaced with 'n/a' strings.</li> </ul> See Also <p>read_data_df : Modern version using pandas DataFrames with more flexible output.</p>"},{"location":"reference/misc/read_input_csv/#misc.read_input_csv.read_data_df","title":"<code>read_data_df(filename, datatype=None, truedataindex=None, outtype='np.array', return_data_info=True)</code>","text":"<p>Read observational data from CSV or pickle files with flexible output formats.</p> <p>This function reads data files (CSV or pickle) containing observational data, processes array-like string representations, and returns the data in the requested format. Supports filtering by data types and row indices.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the data file. Must end with '.csv' or '.pkl'.</p> required <code>datatype</code> <code>list of str</code> <p>Column names to extract. If None, all columns are used. Default is None.</p> <code>None</code> <code>truedataindex</code> <code>list of int</code> <p>Row indices to extract (0-based). If None, all rows are used. Default is None.</p> <code>None</code> <code>outtype</code> <code>(array, list)</code> <p>Output format: - 'np.array': Returns flattened NumPy array - 'list': Returns list of dictionaries Default is 'np.array'.</p> <code>'np.array'</code> <code>return_data_info</code> <code>bool</code> <p>If True, also returns metadata (column names and row indices). Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>flat_array</code> <code>ndarray</code> <p>Flattened 1D array of all data (if outtype='np.array').</p> <code>data</code> <code>list of dict</code> <p>List where each element is a dictionary with column names as keys (if outtype='list').</p> <code>datatype</code> <code>list of str</code> <p>Column names used (only if return_data_info=True).</p> <code>indices</code> <code>list</code> <p>Row indices/labels used (only if return_data_info=True).</p> Notes <ul> <li>String representations of arrays (e.g., \"[1.0 2.0 3.0]\") are automatically   converted to NumPy arrays.</li> <li>When outtype='np.array', arrays from multiple columns and rows are concatenated   into a single flat array.</li> <li>The first column in CSV files is used as the index.</li> </ul>"},{"location":"reference/misc/read_input_csv/#misc.read_input_csv.read_var_csv","title":"<code>read_var_csv(filename, datatype, truedataindex)</code>","text":"<p>Read variance/uncertainty data from CSV files (legacy function).</p> <p>This is a legacy function for reading CSV files containing variance or standard deviation data. Assumes that variance data is stored in alternating columns: data type identifier (string) followed by variance value (numeric).</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the CSV file containing variance data.</p> required <code>datatype</code> <code>list of str</code> <p>Column names (or positional identifiers) for data types. The function expects variance values in adjacent columns (datatype_col + 1).</p> required <code>truedataindex</code> <code>list</code> <p>Row identifiers where variance data was recorded. Used to select specific rows from the CSV.</p> required <p>Returns:</p> Name Type Description <code>imported_var</code> <code>list of list</code> <p>2D list where each sublist contains alternating data type identifiers (strings, converted to lowercase) and variance values (floats). Format: [type1, var1, type2, var2, ...] for each row.</p> Notes <ul> <li>The function expects variance data in alternating columns with the structure:   [type_name, variance_value, type_name, variance_value, ...]</li> <li>Data type names are automatically converted to lowercase.</li> <li>Supports the same header configurations as read_data_csv:   both headers, column headers only, row headers only, or no headers.</li> <li>If first column is 'header_both', assumes both row and column headers exist.</li> </ul> See Also <p>read_var_df : Modern version using pandas DataFrames. read_data_csv : Companion function for reading observational data.</p>"},{"location":"reference/misc/read_input_csv/#misc.read_input_csv.read_var_df","title":"<code>read_var_df(filename, datatype=None, truedataindex=None, outtype='list')</code>","text":"<p>Read variance/uncertainty data from CSV or pickle files.</p> <p>This function is designed to read variance or standard deviation data that corresponds to observational data. It returns the data as a list of dictionaries, with special handling for datatype columns that may contain tuple representations.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the variance file. Must end with '.csv' or '.pkl'.</p> required <code>datatype</code> <code>list of str</code> <p>Column names to extract. Supports tuple-like string representations (e.g., \"('OPR', 'WWCT')\") which are parsed using ast.literal_eval. If None, all columns are used. Default is None.</p> <code>None</code> <code>truedataindex</code> <code>list of str or int</code> <p>Row indices/labels to extract. If None, all rows are used. Default is None.</p> <code>None</code> <code>outtype</code> <code>list</code> <p>Output format. Currently only 'list' is supported. Default is 'list'.</p> <code>'list'</code> <p>Returns:</p> Name Type Description <code>var</code> <code>list of dict</code> <p>List where each element is a dictionary with column names as keys and variance/uncertainty values as values. Each dictionary corresponds to one row.</p> Notes <ul> <li>CSV file indices are converted to strings for consistent lookup.</li> <li>The datatype parameter attempts to evaluate string representations of tuples,   which is useful when column names are composite keys.</li> <li>This function is typically used alongside read_data_df to load both   observations and their uncertainties.</li> </ul>"},{"location":"reference/misc/read_input_csv/#misc.read_input_csv.to_array_if_sequence","title":"<code>to_array_if_sequence(val)</code>","text":"<p>Convert various data types to NumPy array or sequence format.</p> <p>Handles conversion of different input types (scalars, lists, strings, arrays) into a consistent array-like format for data processing.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>various</code> <p>Input value to convert. Can be np.ndarray, int, float, list, str, or other.</p> required <p>Returns:</p> Type Description <code>ndarray or list</code> <ul> <li>NumPy array if input is ndarray, numeric scalar, list, or parseable string</li> <li>List containing the value if input is of another type</li> </ul> Notes <p>String inputs are only parsed if they are enclosed in brackets (e.g., \"[1 2 3]\"). All numeric scalars are wrapped into 1D arrays.</p>"},{"location":"reference/misc/grid/","title":"grid","text":"<p>Generic read module which determines format from extension.</p>"},{"location":"reference/misc/grid/#misc.grid.read_grid","title":"<code>read_grid(filename, cache_dir=None)</code>","text":"<p>Read a grid file and optionally cache it for faster future reads.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of the grid file to read, including path.</p> required <code>cache_dir</code> <code>str</code> <p>Path to a directory where a cache of the grid may be stored to ensure faster read next time.</p> <code>None</code>"},{"location":"reference/misc/grid/cornerpoint/","title":"cornerpoint","text":"<p>Common functionality for visualization of corner-point grids.</p> <p>import pyresito.grid.cornerpoint as cp</p>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.bounding_box","title":"<code>bounding_box(corn, filtr)</code>","text":"<p>Calculate the bounding box of the grid.</p> <p>This function assumes that the grid is aligned with the geographical axes.</p> <p>Parameters:</p> Name Type Description Default <code>corn</code> <code>ndarray</code> <p>Coordinate values for each corner. This matrix can be constructed with the <code>corner_coordinates</code> function. Shape: (3, nk*2*nj*2*ni*2)</p> required <code>filtr</code> <code>ndarray</code> <p>Active corners; use scatter of ACTNUM if no filtering. Shape: (nk, 2, nj, 2, ni, 2), dtype: numpy.bool</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Bottom front right corner and top back left corner. Since the matrix is returned with C ordering, it is specified the opposite way of what is natural for mathematical matrices. Shape: (2, 3), dtype: numpy.float64</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; corn = corner_coordinates(grid['COORD'], grid['ZCORN'])\n&gt;&gt;&gt; bbox = bounding_box(corn, scatter(grid['ACTNUM']))\n&gt;&gt;&gt; p, q = bbox[0, :], bbox[1, :]\n&gt;&gt;&gt; diag = np.sqrt(np.dot(q - p, q - p))\n</code></pre>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.cell_filter","title":"<code>cell_filter(grid, func)</code>","text":"<p>Create a filter for a specific grid.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Grid structure that should be filtered.</p> required <code>func</code> <code>Callable[(int, int, int), bool]</code> <p>Lambda function that takes i, j, k indices (one-based) and returns boolean for whether the cells should be included or not. The function should be vectorized.</p> required <p>Returns:</p> Name Type Description <code>layr</code> <code>ndarray</code> <p>Filtered grid layer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; layr = cell_filter(grid, lambda i, j, k: np.greater_equal(k, 56))\n</code></pre>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.corner_coordinates","title":"<code>corner_coordinates(coord, zcorn)</code>","text":"<p>Generate (x, y, z) coordinates for each corner-point.</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>ndarray</code> <p>Pillar geometrical information. Shape: (nj+1, ni+1, 2, 3)</p> required <code>zcorn</code> <code>ndarray</code> <p>Depth values along each pillar. Shape: (nk, 2, nj, 2, ni, 2)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Coordinate values for each corner. Shape: (3, nk*2*nj*2*ni*2)</p>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.cp_cells","title":"<code>cp_cells(grid, face)</code>","text":"<p>Make a cell array from a cornerpoint grid. The cells will be in the same order as the Cartesian enumeration of the grid.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Pillar coordinates and corner depths. Must contain 'coord' and 'zcorn' properties.</p> required <code>face</code> <code>Face enum</code> <p>Which face that should be extracted, e.g. Face.UP.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Set of geometrical objects that can be sent to rendering. Contains: - 'points': ndarray, shape (nverts, 3) - 'cells': ndarray, shape (nelems, ncorns), where ncorns is either 8    (hexahedron volume) or 4 (quadrilateral face), depending on the face parameter.</p>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.elem_vtcs_ndcs","title":"<code>elem_vtcs_ndcs(nk, nj, ni)</code>","text":"<p>List zcorn indices used by every element in an nk*nj*ni grid.</p> <p>Parameters:</p> Name Type Description Default <code>nk</code> <code>int</code> <p>Number of layers in Z direction.</p> required <code>nj</code> <code>int</code> <p>Number of elements in the Y direction.</p> required <code>ni</code> <code>int</code> <p>Number of elements in the X direction.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Zero-based indices for the hexahedral element corners,  with shape (nk*nj*ni, 8) and dtype int.</p>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.face_coords","title":"<code>face_coords(grid)</code>","text":"<p>Get (x, y, z) coordinates for each corner-point.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Cornerpoint-grid.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Coordinate values for each corner. Use the Face enum to index the first dimension, k, j, i coordinates to index the next three, and Dim enum to index the last dimension. Note that the first point in a face is not necessarily the point that is closest to the origin of the grid. Shape = (8, nk, nj, ni, 3).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pyresito.io.ecl as ecl\n&gt;&gt;&gt; import pyresito.grid.cornerpoint as cp\n&gt;&gt;&gt; case = ecl.EclipseCase(\"FOO\")\n&gt;&gt;&gt; coord_fijkd = cp.face_coords(case.grid())\n&gt;&gt;&gt; # get the midpoint of the upper face in each cell\n&gt;&gt;&gt; up_mid = np.average(coord_fijkd[cp.Face.UP, :, :, :, :], axis=0)\n</code></pre>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.horizon","title":"<code>horizon(grid, layer=0, top=True)</code>","text":"<p>Extract the points that are in a certain horizon and average them so that the result is per cell and not per pillar.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Grid structure.</p> required <code>layer</code> <code>int</code> <p>The K index of the horizon. Default is the layer at the top.</p> <code>0</code> <code>top</code> <code>bool</code> <p>Whether the top face should be exported. If this is False, then the bottom face is exported instead.</p> <code>True</code> <p>Returns:</p> Type Description <code>array</code> <p>Depth at the specified horizon for each cell center, with shape (nk, nj, ni) and dtype numpy.float64.</p>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.horizon_pillars","title":"<code>horizon_pillars(grid, layer=0, top=True)</code>","text":"<p>Extract heights where a horizon crosses the pillars.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Grid structure, containing both COORD and ZCORN properties.</p> required <code>layer</code> <code>int</code> <p>The K index of the horizon. Default is the layer at the top.</p> <code>0</code> <code>top</code> <code>bool</code> <p>Whether the top face should be exported. If False, then the bottom face is exported instead.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Heights of the horizon at each pillar, in the same format as the COORD matrix. Shape: (nj+1, ni+1, 2, 3)</p>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.inner_dup","title":"<code>inner_dup(pillar_field)</code>","text":"<p>Duplicate all inner items in both dimensions.</p> <p>Use this method to duplicate values associated with each pillar to the corners on each side(s) of the pillar; four corners for all interior pillars, two corners for all pillars on the rim and only one (element) corner for those pillars that are on the grid corners.</p> <p>Parameters:</p> Name Type Description Default <code>pillar_field</code> <code>ndarray</code> <p>Property per pillar in the grid, shape = (m+1, n+1)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Property per corner in a grid plane, shape = (2*m, 2*n)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; inner_dup(np.array([[1, 2, 3],\n                        [4, 5, 6],\n                        [7, 8, 9]]))\narray([[1, 2, 2, 3],\n       [4, 5, 5, 6],\n       [4, 5, 5, 6],\n       [7, 8, 8, 9]])\n</code></pre>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.mass_center","title":"<code>mass_center(corn, filtr)</code>","text":"<p>Mass center of the grid.</p> <p>This function will always assume that the density is equal throughout the field.</p> <p>Parameters:</p> Name Type Description Default <code>corn</code> <code>ndarray</code> <p>Coordinate values for each corner. This matrix can be constructed with the  <code>corner_coordinates</code> function. Shape = (3, nk*2*nj*2*ni*2).</p> required <code>filtr</code> <code>ndarray</code> <p>Active corners; use scatter of ACTNUM if no filtering. Shape = (nk, 2, nj, 2, ni, 2),  dtype = numpy.bool.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Center of mass. This should be the focal point of the grid. Shape = (3,), dtype = np.float64.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; corn = cp.corner_coordinates(grid['COORD'], grid['ZCORN'])\n&gt;&gt;&gt; focal_point = cp.mass_center(corn, cp.scatter(grid['ACTNUM']))\n</code></pre>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.scatter","title":"<code>scatter(cell_field)</code>","text":"<p>Duplicate all items in every dimension.</p> <p>Use this method to duplicate values associated with each cell to each of the corners in the cell.</p> <p>Parameters:</p> Name Type Description Default <code>cell_field</code> <code>ndarray</code> <p>Property per cell in the grid, shape = (nk, nj, ni)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Property per corner in the cube, shape = (nk, 2, nj, 2, ni, 2)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; scatter(np.array([[[1, 2],\n                       [3, 4]],\n                      [[5, 6],\n                       [7, 8]]]))\narray([[[[[[1, 1], [2, 2]],\n          [[1, 1], [2, 2]]],\n         [[[3, 3], [4, 4]],\n          [[3, 3], [4, 4]]]],\n</code></pre> <pre><code>    [[[[1, 1], [2, 2]],\n      [[1, 1], [2, 2]]],\n     [[[3, 3], [4, 4]],\n      [[3, 3], [4, 4]]]]],\n\n   [[[[[5, 5], [6, 6]],\n      [[5, 5], [6, 6]]],\n     [[[7, 7], [8, 8]],\n      [[7, 7], [8, 8]]]],\n\n    [[[[5, 5], [6, 6]],\n      [[5, 5], [6, 6]]],\n     [[[7, 7], [8, 8]],\n      [[7, 7], [8, 8]]]]]])\n</code></pre>"},{"location":"reference/misc/grid/cornerpoint/#misc.grid.cornerpoint.snugfit","title":"<code>snugfit(grid)</code>","text":"<p>Create coordinate pillars that match exactly the top and bottom horizon of the grid cells.</p> <p>This version assumes that the pillars in the grid are all strictly vertical, i.e., the x- and y-coordinates will not be changed.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Grid structure, containing both COORD and ZCORN properties.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Pillar coordinates, in the same format as the COORD matrix. Note that a new matrix is returned, the original grid is not altered/updated. Shape: (nj+1, ni+1, 2, 3)</p>"},{"location":"reference/misc/grid/sector/","title":"sector","text":"<p>Extract a sector from an existing cornerpoint grid.</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.extract_cell_prop","title":"<code>extract_cell_prop(prop, least, most)</code>","text":"<p>Extract the property values for a submodel.</p> <p>Parameters:</p> Name Type Description Default <code>prop</code> <code>ndarray</code> <p>Property values for each cell in the entire grid with shape (nk, nj, ni).</p> required <code>least</code> <code>tuple of int</code> <p>Lower, left-most, back corner of submodel, (k1, j1, i1).</p> required <code>most</code> <code>tuple of int</code> <p>Upper, right-most, front corner of submodel, (k2, j2, i2).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Property values for each cell in the submodel with shape (k2-k1+1, j2-j1+1, i2-i1+1).</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.extract_coord","title":"<code>extract_coord(coord, least, most)</code>","text":"<p>Extract the coordinate pillars for a submodel.</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>ndarray</code> <p>Coordinate pillars for the entire grid with shape (nj+1, ni+1, 2, 3).</p> required <code>least</code> <code>tuple of int</code> <p>Lower, left-most, back corner of submodel, (k1, j1, i1).</p> required <code>most</code> <code>tuple of int</code> <p>Upper, right-most, front corner of submodel, (k2, j2, i2).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Coordinate pillars for the submodel with shape (j2-j1+2, i2-i1+2, 2, 3).</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.extract_dimens","title":"<code>extract_dimens(least, most)</code>","text":"<p>Build a new dimension tuple for a submodel.</p> <p>Parameters:</p> Name Type Description Default <code>least</code> <code>tuple of int</code> <p>Lower, left-most, back corner of submodel, (k1, j1, i1).</p> required <code>most</code> <code>tuple of int</code> <p>Upper, right-most, front corner of submodel, (k2, j2, i2).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Dimensions of the submodel.</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.extract_grid","title":"<code>extract_grid(grid, least, most)</code>","text":"<p>Extract a submodel from a full grid.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Attributes of the full grid, such as COORD, ZCORN, ACTNUM.</p> required <code>least</code> <code>tuple of int</code> <p>Lower, left-most, back corner of the submodel, (k1, j1, i1).</p> required <code>most</code> <code>tuple of int</code> <p>Upper, right-most, front corner of the submodel, (k2, j2, i2).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Attributes of the sector model.</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.extract_zcorn","title":"<code>extract_zcorn(zcorn, least, most)</code>","text":"<p>Extract hinge depth values for a submodel from the entire grid.</p> <p>Parameters:</p> Name Type Description Default <code>zcorn</code> <code>ndarray</code> <p>Hinge depth values for the entire grid with shape (nk, 2, nj, 2, ni, 2).</p> required <code>least</code> <code>tuple of int</code> <p>Lower, left-most, back corner of submodel, (k1, j1, i1).</p> required <code>most</code> <code>tuple of int</code> <p>Upper, right-most, front corner of submodel, (k2, j2, i2).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Hinge depth values for the submodel with shape (k2-k1+1, 2, j2-j1+1, 2, i2-i1+1).</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.main","title":"<code>main(*args)</code>","text":"<p>Read a data file to see if it parses OK.</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.parse_tuple","title":"<code>parse_tuple(corner)</code>","text":"<p>Parse a coordinate specification string into a tuple of zero-based coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>corner</code> <code>str</code> <p>Coordinate specification in the format \"(i1,j1,k1)\".</p> required <p>Returns:</p> Type Description <code>tuple of int</code> <p>The parsed tuple, converted into zero-based coordinates and in Python-matrix order: (k, j, i).</p>"},{"location":"reference/misc/grid/sector/#misc.grid.sector.sort_tuples","title":"<code>sort_tuples(corner, opposite)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>corner</code> <code>tuple of int</code> <p>Coordinates of one corner.</p> required <code>opposite</code> <code>tuple of int</code> <p>Coordinates of the opposite corner.</p> required <p>Returns:</p> Type Description <code>tuple of tuple of int</code> <p>The two tuples, but with coordinates interchanged so that one corner is always in the lower, left, back and the other is in the upper, right, front.</p>"},{"location":"reference/misc/grid/unstruct/","title":"unstruct","text":"<p>Convert cornerpoint grids to unstructured grids.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pyresito.grid.unstruct as us\n&gt;&gt;&gt; import pyresito.io.grdecl as grdecl\n&gt;&gt;&gt; g = grdecl.read('~/proj/cmgtools/bld/overlap.grdecl')\n</code></pre>"},{"location":"reference/misc/grid/unstruct/#misc.grid.unstruct.Face","title":"<code>Face</code>","text":"<p>               Bases: <code>object</code></p> <p>A (vertical) face consists of two ridges, because all of the faces in a hexahedron can be seen as (possibly degenerate) quadrilaterals.</p>"},{"location":"reference/misc/grid/unstruct/#misc.grid.unstruct.Face.is_above","title":"<code>is_above(other)</code>","text":"<p>Weak ordering of faces based on vertical placement.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Face</code> <p>Face to be compared to this object.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if all points in face self are above all points in face other, False otherwise.</p>"},{"location":"reference/misc/grid/unstruct/#misc.grid.unstruct.Ridge","title":"<code>Ridge</code>","text":"<p>               Bases: <code>object</code></p> <p>A ridge consists of two points, anchored in each their pillar. We only need to store the z-values, because the x- and y- values are determined by the pillar themselves.</p>"},{"location":"reference/misc/grid/unstruct/#misc.grid.unstruct.Ridge.is_not_below","title":"<code>is_not_below(other)</code>","text":"<p>Weak ordering of ridges based on vertical placement.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Ridge</code> <p>Ridge to be compared to this object.</p> required <p>Returns:</p> Type Description <code>bool or None</code> <p>True if no point on self is below any on the other, None if the ridges cross, and False if there is a point on the other ridge that is above any on self.</p>"},{"location":"reference/misc/grid/unstruct/#misc.grid.unstruct.conv","title":"<code>conv(grid)</code>","text":"<p>Convert a cornerpoint grid to an unstructured grid.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>dict</code> <p>Cornerpoint grid to be converted. Should contain 'COORD', 'ZCORN', 'ACTNUM'.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Unstructured grid.</p>"},{"location":"reference/misc/system_tools/","title":"system_tools","text":"<p>Multiprocessing and environment management.</p>"},{"location":"reference/misc/system_tools/environ_var/","title":"environ_var","text":"<p>Descriptive description.</p>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.CmgRunEnvironment","title":"<code>CmgRunEnvironment</code>","text":"<p>A context manager class to run CMG simulators with correct environmental variables.</p>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.CmgRunEnvironment.__enter__","title":"<code>__enter__()</code>","text":"<p>Method that is run when class is initiated by a 'with'-statement</p> Changelog <ul> <li>ST 25/10-18</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.CmgRunEnvironment.__exit__","title":"<code>__exit__(exc_typ, exc_val, exc_trb)</code>","text":"<p>Method that is run when 'with'-statement closes. Here, we reset environment variables and Process context to what is was set to before the 'with'-statement. Input here in this method are required to work in 'with'-statement.</p> Changelog <ul> <li>ST 25/10-18</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.CmgRunEnvironment.__init__","title":"<code>__init__(root, simulator, version, license)</code>","text":"<p>We initialize the context manager by setting up correct paths and environment variable names that we set in enter.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root folder where CMG simulator(s) are installed.</p> required <code>simulator</code> <code>str</code> <p>Simulator name.</p> required <code>version</code> <code>str</code> <p>Version of the simulator.</p> required <code>license</code> <code>str</code> <p>License server name.</p> required Changelog <ul> <li>ST 25/10-18</li> </ul> Notes <p>'version' is the release version of CMG, e.g., 2017.101.G.</p>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.EclipseRunEnvironment","title":"<code>EclipseRunEnvironment</code>","text":"<p>A context manager class to run eclipse simulators with correct environmental variables.</p>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.EclipseRunEnvironment.__enter__","title":"<code>__enter__()</code>","text":"<p>Method that is run when class is initiated by a 'with'-statement</p> Changelog <ul> <li>KF 30/10-19</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.EclipseRunEnvironment.__exit__","title":"<code>__exit__(exc_typ, exc_val, exc_trb)</code>","text":"<p>Method that is run when 'with'-statement closes. Here, we reset environment variables and Process context to what is was set to before the 'with'-statement. Input here in this method are required to work in 'with'-statement.</p> Changelog <ul> <li>ST 25/10-18</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.EclipseRunEnvironment.__init__","title":"<code>__init__(filename)</code>","text":"<p>input filename: eclipse run file, needed to check for errors (string)</p> Changelog <ul> <li>KF 30/10-19</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.FlowRockRunEnvironment","title":"<code>FlowRockRunEnvironment</code>","text":"<p>A context manager class to run flowRock simulators with correct environmental variables.</p>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.FlowRockRunEnvironment.__enter__","title":"<code>__enter__()</code>","text":"<p>Method that is run when class is initiated by a 'with'-statement</p> Changelog <ul> <li>KF 30/10-19</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.FlowRockRunEnvironment.__exit__","title":"<code>__exit__(exc_typ, exc_val, exc_trb)</code>","text":"<p>Method that is run when 'with'-statement closes. Here, we reset environment variables and Process context to what is was set to before the 'with'-statement. Input here in this method are required to work in 'with'-statement.</p> Changelog <ul> <li>ST 25/10-18</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.FlowRockRunEnvironment.__init__","title":"<code>__init__(filename)</code>","text":"<ul> <li>filename: dummy run file</li> </ul> Changelog <ul> <li>KF 30/10-19</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OPMRunEnvironment","title":"<code>OPMRunEnvironment</code>","text":"<p>A context manager class to run OPM simulators with correct environmental variables.</p>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OPMRunEnvironment.__enter__","title":"<code>__enter__()</code>","text":"<p>Method that is run when class is initiated by a 'with'-statement</p> Changelog <ul> <li>KF 30/10-19</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OPMRunEnvironment.__exit__","title":"<code>__exit__(exc_typ, exc_val, exc_trb)</code>","text":"<p>Method that is run when 'with'-statement closes. Here, we reset environment variables and Process context to what it was set to before the 'with'-statement. Input here in this method are required to work in 'with'-statement.</p> Changelog <ul> <li>ST 25/10-18</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OPMRunEnvironment.__init__","title":"<code>__init__(filename, suffix, matchstring)</code>","text":"<ul> <li>filename: OPM run file, needed to check for errors (string)</li> <li>suffix: What file to search for complete sign</li> <li>matchstring: what is the complete sign</li> </ul> Changelog <ul> <li>KF 30/10-19</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OpenBlasSingleThread","title":"<code>OpenBlasSingleThread</code>","text":"<p>A context manager class to set OpenBLAS multi threading environment variable to 1 (i.e., single threaded). The class is used in a 'with'-statement to ensure that everything inside the statement is run single threaded, and outside the statement is run using whatever the environment variable was set before the 'with'-statement. The environment variable setting threading in OpenBLAS is OMP_NUM_THREADS.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from system_tools.environ_var import OpenBlasSingleThread\n... import ctypes\n... import multiprocessing as mp\n...\n... def justdoit():\n...     # Load OpenBLAS library and print number of threads\n...     openblas_lib = ctypes.cdll.LoadLibrary('/scratch/openblas/lib/libopenblas.so')\n...     print(openblas_lib.openblas_get_num_threads())\n...\n... if __name__ == \"__main__\":\n...     # Load OpenBLAS library and print number of threads before the with-statement\n...     openblas_lib = ctypes.cdll.LoadLibrary('/scratch/openblas/lib/libopenblas.so')\n...     print(openblas_lib.openblas_get_num_threads())\n...\n...     # Run a Process inside the with-statement with the OpenBlasSingleThread class.\n...     with OpenBlasSingleThread ():\n...         p = mp.Process (target=justdoit)\n...         p.start ()\n...         p.join ()\n...\n... # Load OpenBLAS library and print number of threads before the with-statement\n... openblas_lib = ctypes.cdll.LoadLibrary('/scratch/openblas/lib/libopenblas.so')\n... print(openblas_lib.openblas_get_num_threads())\n</code></pre>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OpenBlasSingleThread.__enter__","title":"<code>__enter__()</code>","text":"<p>Method that is run when class is initiated by a 'with'-statement</p> Changelog <ul> <li>ST 31/10-17</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OpenBlasSingleThread.__exit__","title":"<code>__exit__(exc_typ, exc_val, exc_trb)</code>","text":"<p>Method that is run when 'with'-statement closes. Here, we reset OMP_NUM_THREADS and Process context to what is was set to before the 'with'-statement. Input here in this method are required to work in 'with'-statement.</p> Changelog <ul> <li>ST 31/10-17</li> </ul>"},{"location":"reference/misc/system_tools/environ_var/#misc.system_tools.environ_var.OpenBlasSingleThread.__init__","title":"<code>__init__()</code>","text":"<p>Init. the class with no inputs. Use this to initialize internal variables for storing number of threads an the Process context manager before the change to single thread.</p> <p>Attributes:</p> Name Type Description <code>num_threads</code> <p>String with number of OpenBLAS threads before change to single threaded (it is the content of OMP_NUM_THREADS)</p> <code>ctx</code> <p>The context variable from Process (default is 'fork' context, but we want to use 'spawn')</p> Changelog <ul> <li>ST 31/10-17</li> </ul>"},{"location":"reference/pipt/","title":"pipt","text":"<p>Inversion (estimation, data assimilation)</p>"},{"location":"reference/pipt/#pipt--python-inverse-problem-toolbox-pipt","title":"Python Inverse Problem Toolbox (PIPT)","text":"<p>PIPT is one part of the PET application. Here we solve Data-Assimilation and inverse problems.</p>"},{"location":"reference/pipt/pipt_init/","title":"pipt_init","text":"<p>Descriptive description.</p>"},{"location":"reference/pipt/pipt_init/#pipt.pipt_init.init_da","title":"<code>init_da(da_input, en_input, sim)</code>","text":"<p>initialize the ensemble object based on the DA inputs</p>"},{"location":"reference/pipt/loop/","title":"loop","text":"<p>Main loop for running data assimilation.</p>"},{"location":"reference/pipt/loop/assimilation/","title":"assimilation","text":"<p>Descriptive description.</p>"},{"location":"reference/pipt/loop/assimilation/#pipt.loop.assimilation.Assimilate","title":"<code>Assimilate</code>","text":"<p>Class for iterative ensemble-based methods. This loop is similar/equal to a deterministic/optimization loop, but since we use ensemble-based method, we need to invoke <code>pipt.fwd_sim.ensemble.Ensemble</code> to get correct hierarchy of classes. The iterative loop will go until the max. iterations OR convergence has been met. Parameters for both these stopping criteria have to be given by the user through methods in their <code>pipt.update_schemes</code> class. Note that only iterative ensemble smoothers can be implemented with this loop (at the moment). Methods needed to be provided by user in their update_schemes class:  </p> <p><code>calc_analysis</code> <code>check_convergence</code> </p> <p>% Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/loop/assimilation/#pipt.loop.assimilation.Assimilate.__init__","title":"<code>__init__(ensemble)</code>","text":"<p>Initialize by passing the PIPT init. file up the hierarchy.</p>"},{"location":"reference/pipt/loop/assimilation/#pipt.loop.assimilation.Assimilate.calc_forecast","title":"<code>calc_forecast()</code>","text":"<p>Calculate the forecast step.</p> <p>Run the forward simulator, generating predicted data for the analysis step. First input to the simulator instances is the ensemble of (joint) state to be run and how many to run in parallel. The forward runs are done in a while-loop consisting of the following steps:</p> <pre><code>    1. Run the simulator for each ensemble member in the background.\n    2. Check for errors during run (if error, correct and run again or abort).\n    3. Check if simulation has ended; if yes, run simulation for the next ensemble members.\n    4. Get results from successfully ended simulations.\n</code></pre> <p>The procedure here is general, hence a simulator used here must contain the initial step of setting up the parameters and steps i-iv, if not an error will be outputted. Initialization of the simulator is done when initializing the Ensemble class (see init). The names of the mandatory methods in a simulator are:</p> <pre><code>    &gt; setup_fwd_sim\n    &gt; run_fwd_sim\n    &gt; check_sim_end\n    &gt; get_sim_results\n</code></pre> Notes <p>Parallel run in \"ampersand\" mode means that it will be started in the background and run independently of the Python script. Hence, check for simulation finished or error must be conducted!</p> <p>Info</p> <p>It is only necessary to get the results from the forward simulations that corresponds to the observed data at the particular assimilation step. That is, results from all data types are not necessary to extract at step iv; if they are not present in the obs_data (indicated by a None type) then this result does not need to be extracted.</p> <p>Info</p> <p>It is assumed that no underscore is inputted in DATATYPE. If there are underscores in DATATYPE entries, well, then we may have a problem when finding out which response to extract in get_sim_results below.</p>"},{"location":"reference/pipt/loop/assimilation/#pipt.loop.assimilation.Assimilate.post_process_forecast","title":"<code>post_process_forecast()</code>","text":"<p>Post processing of predicted data after a forecast run</p>"},{"location":"reference/pipt/loop/assimilation/#pipt.loop.assimilation.Assimilate.run","title":"<code>run()</code>","text":"<p>The general loop implemented here is:</p> <ol> <li>Forecast/forward simulation</li> <li>Check for convergence</li> <li>If convergence have not been achieved, do analysis/update</li> </ol> <p>% Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/loop/ensemble/","title":"ensemble","text":"<p>Descriptive description.</p>"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble","title":"<code>Ensemble</code>","text":"<p>               Bases: <code>Ensemble</code></p> <p>Class for organizing/initializing misc. variables and simulator for an ensemble-based inversion run. Inherits the PET ensemble structure</p>"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.__init__","title":"<code>__init__(keys_da, keys_en, sim)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>keys_da</code> <code>dict</code> <p>Options for the data assimilation class</p> <ul> <li>daalg: spesification of the method, first the main type (e.g., \"enrml\"), then the solver (e.g., \"gnenrml\")</li> <li>analysis: update flavour (\"approx\", \"full\" or \"subspace\")</li> <li>energy: percent of singular values kept after SVD</li> <li>obsvarsave: save the observations as a file (default false)</li> <li>restart: restart optimization from a restart file (default false)</li> <li>restartsave: save a restart file after each successful iteration (defalut false)</li> <li>analysisdebug: specify which class variables to save to the result files</li> <li>truedataindex: order of the simulated data (for timeseries this is points in time)</li> <li>obsname: unit for truedataindex (for timeseries this is days or hours or seconds, etc.)</li> <li>truedata: the data, e.g., provided as a .csv file</li> <li>assimindex: index for the data that will be used for assimilation</li> <li>datatype: list with the name of the datatypes</li> <li>staticvar: name of the static variables</li> <li>dynamicvar: name of the dynamic variables</li> <li>datavar: data variance, e.g., provided as a .csv file</li> </ul> required <code>keys_en</code> <code>dict</code> <p>Options for the ensemble class</p> <ul> <li>ne: number of perturbations used to compute the gradient</li> <li>state: name of state variables passed to the .mako file</li> <li>prior_: the prior information the state variables, including mean, variance and variable limits <p>NB: If keys_en is empty dict, it is assumed that the prior info is contained in keys_da. The merged dict keys_da|keys_en is what is sent to the parent class.</p> required <code>sim</code> <code>callable</code> <p>The forward simulator (e.g. flow)</p> required"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.check_assimindex_sequential","title":"<code>check_assimindex_sequential()</code>","text":"<p>Check if assim. indices is given as a 2D list as is needed in sequential updating. If not, make it a 2D list</p>"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.check_assimindex_simultaneous","title":"<code>check_assimindex_simultaneous()</code>","text":"<p>Check if assim. indices is given as a 1D list as is needed in simultaneous updating. If not, make it a 2D list with one row.</p>"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.compress_manager","title":"<code>compress_manager(data=None, vintage=0, aug_coeff=None)</code>","text":"<p>Compress the input data using wavelets.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>data to be compressed If data is <code>None</code>, all data (true and simulated) is re-compressed (used if leading indices are updated)</p> <code>None</code> <code>vintage</code> <code>int</code> <p>the time index for the data</p> <code>0</code> <code>aug_coeff</code> <code>bool</code> <ul> <li>False: in this case the leading indices for wavelet coefficients are computed</li> <li>True: in this case the leading indices are augmented using information from the ensemble</li> <li>None: in this case simulated data is compressed</li> </ul> <code>None</code>"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.local_analysis_update","title":"<code>local_analysis_update()</code>","text":"<p>Function for updates that can be used by all algorithms. Do this once to avoid duplicate code for local analysis.</p>"},{"location":"reference/pipt/loop/ensemble/#pipt.loop.ensemble.Ensemble.set_observations","title":"<code>set_observations()</code>","text":"<p>Generate the perturbed observed data ensemble</p>"},{"location":"reference/pipt/misc_tools/","title":"misc_tools","text":"<p>More tools.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/","title":"analysis_tools","text":"<p>Collection of tools that can be used in update/analysis schemes.</p> <p>Only put tools here that are so general that they can be used by several update/analysis schemes. If some method is only applicable to the update scheme you are implementing, leave it in that class.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.aug_obs_pred_data","title":"<code>aug_obs_pred_data(obs_data, pred_data, assim_index, list_data)</code>","text":"<p>Augment the observed and predicted data to an array at an assimilation step. The observed data will be an augemented vector and the predicted data will be an ensemble matrix.</p> <p>Parameters:</p> Name Type Description Default <code>obs_data</code> <code>list</code> <p>List of dictionaries containing observed data</p> required <code>pred_data</code> <code>list</code> <p>List of dictionaries where each entry of the list is the forward simulation results at an assimilation step. The dictionary has keys equal to the data type (given in <code>OBSNAME</code>).</p> required <p>Returns:</p> Name Type Description <code>obs</code> <code>ndarray</code> <p>Augmented vector of observed data</p> <code>pred</code> <code>ndarray</code> <p>Ensemble matrix of predicted data</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.aug_state","title":"<code>aug_state(state, list_state, cell_index=None)</code>","text":"<p>Augment the state variables to an array.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary of initial ensemble of (joint) state variables (static parameters and dynamic variables) to be assimilated.</p> required <code>list_state</code> <code>list</code> <p>Fixed list of keys in state dict.</p> required <code>cell_index</code> <code>list of vector indexes to be extracted</code> <code>None</code> <p>Returns:</p> Name Type Description <code>aug</code> <code>ndarray</code> <p>Ensemble matrix of augmented state variables</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.block_diag_cov","title":"<code>block_diag_cov(cov, list_state)</code>","text":"<p>Block diagonalize a covariance matrix dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>cov</code> <code>dict</code> <p>Dict. with cov. matrices</p> required <code>list_state</code> <code>list</code> <p>Fixed list of keys in state dict.</p> required <p>Returns:</p> Name Type Description <code>cov_out</code> <code>ndarray</code> <p>Block diag. matrix with prior covariance matrices for each state.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.calc_autocov","title":"<code>calc_autocov(pert)</code>","text":"<p>Calculate sample auto-covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>pert</code> <code>ndarray</code> <p>Perturbation matrix (matrix of variables perturbed with their mean)</p> required <p>Returns:</p> Name Type Description <code>cov_auto</code> <code>ndarray</code> <p>Sample auto-covariance matrix</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.calc_crosscov","title":"<code>calc_crosscov(pert1, pert2)</code>","text":"<p>Calculate sample cross-covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>pert1</code> <p>Perturbation matrices (matrix of variables perturbed with their mean).</p> required <code>pert2</code> <p>Perturbation matrices (matrix of variables perturbed with their mean).</p> required <p>Returns:</p> Name Type Description <code>cov_cross</code> <code>ndarray</code> <p>Sample cross-covariance matrix</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.calc_kalman_filter_eq","title":"<code>calc_kalman_filter_eq(aug_state, kalman_gain, obs_data, pred_data)</code>","text":"<p>Calculate the updated augment state using the Kalman filter equations</p> <p>Parameters:</p> Name Type Description Default <code>aug_state</code> <code>ndarray</code> <p>Augmented state variable (all the parameters defined in <code>STATICVAR</code> augmented in one array)</p> required <code>kalman_gain</code> <code>ndarray</code> <p>Kalman gain</p> required <code>obs_data</code> <code>ndarray</code> <p>Augmented observed data vector (all <code>OBSNAME</code> augmented in one array)</p> required <code>pred_data</code> <code>ndarray</code> <p>Augmented predicted data vector (all <code>OBSNAME</code> augmented in one array)</p> required <p>Returns:</p> Name Type Description <code>aug_state_upd</code> <code>ndarray</code> <p>Updated augmented state variable using the Kalman filter equations</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.calc_kalmangain","title":"<code>calc_kalmangain(cov_cross, cov_auto, cov_data, opt=None)</code>","text":"<p>Calculate the Kalman gain</p> <p>Parameters:</p> Name Type Description Default <code>cov_cross</code> <code>ndarray</code> <p>Cross-covariance matrix between state and predicted data</p> required <code>cov_auto</code> <code>ndarray</code> <p>Auto-covariance matrix of predicted data</p> required <code>cov_data</code> <code>ndarray</code> <p>Variance on observed data (diagonal matrix)</p> required <code>opt</code> <code>str</code> <p>Which method should we use to calculate Kalman gain</p> <ul> <li>'lu': LU decomposition (default)</li> <li>'chol': Cholesky decomposition</li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>kalman_gain</code> <code>ndarray</code> <p>Kalman gain</p> Notes <p>In the following Kalman gain is \\(K\\), cross-covariance is \\(C_{mg}\\), predicted data auto-covariance is \\(C_{g}\\), and data covariance is \\(C_{d}\\).</p> <p>With <code>'lu'</code> option, we solve the transposed linear system: $$     K^T = (C_{g} + C_{d})<sup>{-T}C_{mg}</sup>T $$</p> <p>With <code>'chol'</code> option we use Cholesky on auto-covariance matrix, $$    L L^T = (C_{g} + C_{d})^T $$ and solve linear system with the square-root matrix from Cholesky: $$     L^T Y = C_{mg}^T\\     LK = Y $$</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.calc_objectivefun","title":"<code>calc_objectivefun(pert_obs, pred_data, Cd)</code>","text":"<p>Calculate the objective function.</p> <p>Parameters:</p> Name Type Description Default <code>pert_obs</code> <code>array - like</code> <p>NdxNe array containing perturbed observations.</p> required <code>pred_data</code> <code>array - like</code> <p>NdxNe array containing ensemble of predictions.</p> required <code>Cd</code> <code>array - like</code> <p>NdxNd array containing data covariance, or Ndx1 array containing data variance.</p> required <p>Returns:</p> Name Type Description <code>data_misfit</code> <code>array - like</code> <p>Nex1 array containing objective function values.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.calc_scaling","title":"<code>calc_scaling(enX, idX, prior_info)</code>","text":"<p>Form the scaling to be used in svd related algoritms. Scaling consist of standard deviation for each <code>STATICVAR</code> It is important that this is formed in the same manner as the augmentet state vector is formed. Hence, with the same list of states.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary containing the state</p> required <code>list_state</code> <code>list</code> <p>List of states for augmenting</p> required <code>prior_info</code> <code>dict</code> <p>Nested dictionary containing prior information</p> required <p>Returns:</p> Name Type Description <code>scaling</code> <code>numpy array</code> <p>scaling</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.calc_subspace_kalmangain","title":"<code>calc_subspace_kalmangain(cov_cross, data_pert, cov_data, energy)</code>","text":"<p>Compute the Kalman gain in a efficient subspace determined by how much energy (i.e. percentage of singluar values) to retain. For more info regarding the implementation, see Chapter 14 in <code>evensen2009a</code>.</p> <p>Parameters cov_cross : ndarray     Cross-covariance matrix between state and predicted data data_pert : ndarray         Predicted data - mean of predicted data cov_data : ndarray     Variance on observed data (diagonal matrix)</p> <p>Returns:</p> Name Type Description <code>k_g</code> <code>ndarray</code> <p>Subspace Kalman gain</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.compute_x","title":"<code>compute_x(pert_preddata, cov_data, keys_da, alfa=None)</code>","text":"<p>INSERT DESCRIPTION</p> <p>Parameters:</p> Name Type Description Default <code>pert_preddata</code> <code>ndarray</code> <p>Perturbed predicted data</p> required <code>cov_data</code> <code>ndarray</code> <p>Data covariance matrix</p> required <code>keys_da</code> <code>dict</code> <p>Dictionary with every input in <code>DATAASSIM</code></p> required <code>alfa</code> <code>None</code> <p>INSERT DESCRIPTION</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X</code> <code>ndarray</code> <p>INSERT DESCRIPTION</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.extract_tot_empirical_cov","title":"<code>extract_tot_empirical_cov(data_var, assim_index, list_data, ne)</code>","text":"<p>Extract realizations of noise from data_var (if imported), or generate realizations if only variance is specified (assume uncorrelated)</p> <p>Parameters:</p> Name Type Description Default <code>data_var</code> <code>list</code> <p>List of dictionaries containing the varianse as read from the input</p> required <code>assim_index</code> <code>int</code> <p>Index of the assimilation</p> required <code>list_data</code> <code>list</code> <p>List of data types</p> required <code>ne</code> <code>int</code> <p>Ensemble size</p> required <p>Returns:</p> Name Type Description <code>E</code> <code>ndarray</code> <p>Sorted (according to assim_index and list_data) matrix of data realization noise.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.gen_covdata","title":"<code>gen_covdata(datavar, assim_index, list_data)</code>","text":"<p>Generate the data covariance matrix at current assimilation step. Note here that the data covariance may be a diagonal matrix with only variance entries, or an empirical covariance matrix, or both if in combination. For diagonal data covariance we only store vector of variance values.</p> <p>Parameters:</p> Name Type Description Default <code>datavar</code> <code>list</code> <p>List of dictionaries containing variance for the observed data. The structure of this list is the same as for <code>obs_data</code></p> required <code>assim_index</code> <code>int</code> <p>Current assimilation index</p> required <code>list_data</code> <code>list</code> <p>List of the data types</p> required <p>Returns:</p> Name Type Description <code>cd</code> <code>ndarray</code> <p>Data auto-covariance matrix</p> Notes <p>For empirical covariance generation, the datavar entry must be a 2D array, arranged as a standard ensemble matrix (N x Ns, where Ns is the number of samples).</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.get_list_data_types","title":"<code>get_list_data_types(obs_data, assim_index)</code>","text":"<p>Extract the list of all and active data types </p> <p>Parameters:</p> Name Type Description Default <code>obs_data</code> <code>dict</code> <p>Observed data</p> required <code>assim_index</code> <code>int</code> <p>Current assimilation index</p> required <p>Returns:</p> Name Type Description <code>l_all</code> <code>list</code> <p>List of all data types</p> <code>l_act</code> <code>list</code> <p>List of the data types that are active (that are not <code>None</code>)</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.get_obs_size","title":"<code>get_obs_size(obs_data, time_index, datatypes)</code>","text":"<p>Return a 2D list of sizes for each observation array.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.limits","title":"<code>limits(state, prior_info)</code>","text":"<p>Check if any state variables overshoots the limits given by the prior info. If so, modify these values</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary containing the states</p> required <code>prior_info</code> <code>dict</code> <p>Dictionary containing prior information for all the states.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>dict</code> <p>Valid state</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.parallel_upd","title":"<code>parallel_upd(list_state, prior_info, states_dict, X, local_mask_info, obs_data, pred_data, parallel, actnum=None, field_dim=None, act_data_list=None, scale_data=None, num_states=1, emp_d_cov=False)</code>","text":"<p>Script to initialize and control a parallel update of the ensemble state following <code>emerick2016a</code>.</p> <p>Parameters:</p> Name Type Description Default <code>list_state</code> <code>list</code> <p>List of state names</p> required <code>prior_info</code> <code>dict</code> <p>INSERT DESCRIPTION</p> required <code>states_dict</code> <code>dict</code> <p>Dict. of state arrays</p> required <code>X</code> <code>ndarray</code> <p>INSERT DESCRIPTION</p> required <code>local_mask_info</code> <code>dict</code> <p>INSERT DESCRIPTION</p> required <code>obs_data</code> <code>ndarray</code> <p>Observed data</p> required <code>pred_data</code> <code>ndarray</code> <p>Predicted data</p> required <code>parallel</code> <code>int</code> <p>Number of parallel runs</p> required <code>actnum</code> <code>ndarray</code> <p>Active cells</p> <code>None</code> <code>field_dim</code> <code>list</code> <p>Number of grid cells in each direction</p> <code>None</code> <code>act_data_list</code> <code>list</code> <p>List of active data names</p> <code>None</code> <code>scale_data</code> <code>ndarray</code> <p>Scaling array for data</p> <code>None</code> <code>num_states</code> <code>int</code> <p>Number of states</p> <code>1</code> <code>emp_d_cov</code> <code>bool</code> <p>INSERT DESCRIPTION</p> <code>False</code> Notes <p>Since the localization matrix is to large for evaluation, we instead calculate it row for row.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.resample_state","title":"<code>resample_state(aug_state, state, list_state, new_en_size)</code>","text":"<p>Extract the seperate state variables from an augmented state matrix. Calculate the mean and covariance, and resample this.</p> <p>Parameters:</p> Name Type Description Default <code>aug_state</code> <code>ndarray</code> <p>Augmented matrix of state variables</p> required <code>state</code> <code>dict</code> <p>Dict. af state variables</p> required <code>list_state</code> <code>list</code> <p>List of state variable</p> required <code>new_en_size</code> <code>int</code> <p>Size of the new ensemble</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>dict</code> <p>Dict. of resampled members</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.save_analysisdebug","title":"<code>save_analysisdebug(ind_save, **kwargs)</code>","text":"<p>Save variables in analysis step for debugging purpose</p> <p>Parameters:</p> Name Type Description Default <code>ind_save</code> <code>int</code> <p>Index of analysis step</p> required <code>**kwargs</code> <code>dict</code> <p>Variables that will be saved to npz file</p> <code>{}</code> Notes <p>Use kwargs here because the input will be a dictionary with names equal the variable names to store, and when this is passed to np.savez (kwargs) the variable will be stored with their original name.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.screen_data","title":"<code>screen_data(cov_data, pred_data, obs_data_vector, keys_da, iteration)</code>","text":"<p>INSERT DESCRIPTION</p> <p>Parameters:</p> Name Type Description Default <code>cov_data</code> <code>ndarray</code> <p>Data covariance matrix</p> required <code>pred_data</code> <code>ndarray</code> <p>Predicted data</p> required <code>obs_data_vector</code> <p>Observed data (1D array)</p> required <code>keys_da</code> <code>dict</code> <p>Dictionary with every input in <code>DATAASSIM</code></p> required <code>iteration</code> <code>int</code> <p>Current iteration</p> required <p>Returns:</p> Name Type Description <code>cov_data</code> <code>ndarray</code> <p>Updated data covariance matrix</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.store_ensemble_sim_information","title":"<code>store_ensemble_sim_information(saveinfo, member)</code>","text":"<p>Here, we can either run a unique python script or do some other post-processing routines. The function should not return anything, but provide a method for storing revevant information. Input the current member for easy storage</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.subsample_state","title":"<code>subsample_state(index, aug_state, pert_state)</code>","text":"<p>Draw a subsample from the original state, given by the index</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>ndarray</code> <p>Index of parameters to draw.</p> required <code>aug_state</code> <code>ndarray</code> <p>Original augmented state.</p> required <code>pert_state</code> <code>ndarray</code> <p>Perturbed augmented state, for error covariance.</p> required <p>Returns:</p> Name Type Description <code>new_state</code> <code>dict</code> <p>Subsample of state.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.truncSVD","title":"<code>truncSVD(matrix, r=None, energy=None, full_matrices=False)</code>","text":"<p>Perform truncated SVD on input matrix.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>(ndarray, shape(m, n))</code> <p>Input matrix to perform SVD on.</p> required <code>r</code> <code>int</code> <p>Rank to truncate the SVD to. If None, energy must be specified.</p> <code>None</code> <code>energy</code> <code>float</code> <p>Percentage of energy to retain in the truncated SVD. If None, r must be specified.</p> <code>None</code> <code>full_matrices</code> <code>bool</code> <p>Whether to compute full or reduced SVD. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>U</code> <code>(ndarray, shape(m, r))</code> <p>Left singular vectors.</p> <code>S</code> <code>(ndarray, shape(r))</code> <p>Singular values.</p> <code>VT</code> <code>(ndarray, shape(r, n))</code> <p>Right singular vectors transposed.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.update_datavar","title":"<code>update_datavar(cov_data, datavar, assim_index, list_data)</code>","text":"<p>Extract the separate variance from an augmented vector. It is assumed that the augmented variance is made gen_covdata, hence this is the reverse method of gen_covdata.</p> <p>Parameters:</p> Name Type Description Default <code>cov_data</code> <code>array - like</code> <p>Augmented vector of variance.</p> required <code>datavar</code> <code>dict</code> <p>Dictionary of separate variances.</p> required <code>assim_index</code> <code>list</code> <p>Assimilation order as a list.</p> required <code>list_data</code> <code>list</code> <p>List of data keys.</p> required <p>Returns:</p> Name Type Description <code>datavar</code> <code>dict</code> <p>Updated dictionary of separate variances.</p>"},{"location":"reference/pipt/misc_tools/analysis_tools/#pipt.misc_tools.analysis_tools.update_state","title":"<code>update_state(aug_state, state, list_state, cell_index=None)</code>","text":"<p>Extract the separate state variables from an augmented state array. It is assumed that the augmented state array is made in <code>aug_state</code>, hence this is the reverse method of <code>aug_state</code>.</p> <p>Parameters:</p> Name Type Description Default <code>aug_state</code> <code>ndarray</code> <p>Augmented array of UPDATED state variables</p> required <code>state</code> <code>dict</code> <p>Dict. of state variables NOT updated.</p> required <code>list_state</code> <code>list</code> <p>List of state keys that have been updated</p> required <code>cell_index</code> <code>list</code> <p>List of indexes that gives the where the aug state should be placed</p> <code>None</code> <p>Returns:</p> Name Type Description <code>state</code> <code>dict</code> <p>Dict. of UPDATED state variables</p>"},{"location":"reference/pipt/misc_tools/cov_regularization/","title":"cov_regularization","text":"<p>Scripts used for localization in the fwd_sim step of Bayes.</p> Changelog <ul> <li>28/6-16: Initialise major reconstruction of the covariance regularization script.</li> </ul> <p>Main outline is:</p> <ul> <li>make this a collection of support functions, not a class</li> <li> <p>initialization will be performed at the initialization of the ensemble class, not at the analysis step. This will   return a dictionary of dictionaries, with a triple as key (data_type, assim_time, parameter). From this key the info   for a unique localization function can be found as a new dictionary with keys:   <code>taper_func</code>, <code>position</code>, <code>anisotropi</code>, <code>range</code>.   This is, potentially, a substantial amount of data which should be imported   as a npz file. For small cases, it can be defined in the init file in csv   form:</p> <p>LOCALIZATION   FIELD    10 10   fb 2 2 1 5 1 0 WBHP PRO-1 10 PERMX,fb 7 7 1 5 1 0 WBHP PRO-2 10 PERMX,fb 5 5 1 5 1 0 WBHP INJ-1 10 PERMX   (taper_func pos(x) pos(y) pos(z) range range(z) anisotropi(ratio) anisotropi(angel) data well assim_time parameter)</p> </li> <li> <p>Generate functions that return the correct localization function.</p> </li> </ul>"},{"location":"reference/pipt/misc_tools/cov_regularization/#pipt.misc_tools.cov_regularization.localization","title":"<code>localization</code>","text":""},{"location":"reference/pipt/misc_tools/cov_regularization/#pipt.misc_tools.cov_regularization.localization.__init__","title":"<code>__init__(parsed_info, assimIndex, data_typ, free_parameter, ne)</code>","text":"<p>Format the parsed info from the input file, and generate the unique localization masks</p>"},{"location":"reference/pipt/misc_tools/qaqc_tools/","title":"qaqc_tools","text":"<p>Quality Assurance of the forecast (QA) and analysis (QC) step.</p>"},{"location":"reference/pipt/misc_tools/qaqc_tools/#pipt.misc_tools.qaqc_tools.QAQC","title":"<code>QAQC</code>","text":"<p>Perform Quality Assurance of the forecast (QA) and analysis (QC) step. Available functions:    1) calc_coverage: check forecast data coverage    2) calc_mahalanobis: evaluate \"higher-order\" data coverage    3) calc_kg: check/write individual gain for parameters;                flag data which have conflicting updates    4) calc_da_stat: compute statistics for updated parameters</p> <p>Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/misc_tools/qaqc_tools/#pipt.misc_tools.qaqc_tools.QAQC.calc_coverage","title":"<code>calc_coverage(line=None, field_dim=None, uxl=None, uil=None, contours=None, uxl_c=None, uil_c=None)</code>","text":"<p>Calculate the Data coverage for production and seismic data. For seismic data the plotting is based on the importance-scaled coverage developed by Espen O. Lie from GeoCore.</p> <p>Input:     line: if not None, plot 1d coverage     field_dim: if None, must import utm coordinates. Else give the grid</p> <p>Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/misc_tools/qaqc_tools/#pipt.misc_tools.qaqc_tools.QAQC.calc_da_stat","title":"<code>calc_da_stat(options=None)</code>","text":"<p>Calculate statistics for the updated parameters. The persentage of parameters that have updates larger than one, two and three standard deviations (calculated from the initial ensemble) are flagged.</p> <p>Input: options: Settings for statistics     - write_to_file: write results to .grdecl file (default False)</p> <p>Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/misc_tools/qaqc_tools/#pipt.misc_tools.qaqc_tools.QAQC.calc_kg","title":"<code>calc_kg(options=None)</code>","text":"<p>Check/write individual gain for parameters. Note form ES gain with an identity Cd... This can be improved</p> <p>Visualization of the many of these parameters is problem-specific. In reservoir simulation cases, it is necessary to write this to the simulation grid. While for other applications, one might want other visualization. Hence, the method also depends on a simulator specific writer.</p> <p>Input: options: Settings for the kalman gain computations     - num_store: number of elements to store (default 10)     - unique_time: calculate for each time instance (default False)     - plot_all_kg: plot all the kalman gains for the field parameters, if not plot the num_store (default False)     - only_log: only write to logger; no plotting (default True)     - auto_ada_loc: use localization in computations (default True)     - write_to_resinsight: pipe results to ResInsight (default False)       (Note: this requires that ResInsight is open on the computer)</p> <p>Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/misc_tools/qaqc_tools/#pipt.misc_tools.qaqc_tools.QAQC.calc_mahalanobis","title":"<code>calc_mahalanobis(combi_list=(1, None))</code>","text":"<p>Calculate the mahalanobis distance as described in \"Oliver, D. S. (2020). Diagnosing reservoir model deficiency for model improvement. Journal of Petroleum Science and Engineering, 193(February). https://doi.org/10.1016/j.petrol.2020.107367\"</p> <p>Input: combi_list: list of levels and possible combination of datatypes. The list must be given as a tuple with pairs:     level int: defines which level. default = 1     combi_typ: defines how data are combined: Default is no combine.</p> <p>Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/misc_tools/wavelet_tools/","title":"wavelet_tools","text":"<p>Sparse representation of seismic data using wavelet compression.</p> <p>Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/update_schemes/","title":"update_schemes","text":"<p>Ensemble analysis/conditioning/inversion schemes.</p>"},{"location":"reference/pipt/update_schemes/enkf/","title":"enkf","text":"<p>EnKF type schemes</p>"},{"location":"reference/pipt/update_schemes/enkf/#pipt.update_schemes.enkf.enkfMixIn","title":"<code>enkfMixIn</code>","text":"<p>               Bases: <code>Ensemble</code></p> <p>Straightforward EnKF analysis scheme implementation. The sequential updating can be done with general grouping and ordering of data. If only one-step EnKF is to be done, use <code>es</code> instead.</p>"},{"location":"reference/pipt/update_schemes/enkf/#pipt.update_schemes.enkf.enkfMixIn.__init__","title":"<code>__init__(keys_da, keys_en, sim)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p>"},{"location":"reference/pipt/update_schemes/enkf/#pipt.update_schemes.enkf.enkfMixIn.calc_analysis","title":"<code>calc_analysis()</code>","text":"<p>Calculate the analysis step of the EnKF procedure. The updating is done using the Kalman filter equations, using svd for numerical stability. Localization is available.</p>"},{"location":"reference/pipt/update_schemes/enkf/#pipt.update_schemes.enkf.enkfMixIn.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Calculate the \"convergence\" of the method. Important to</p>"},{"location":"reference/pipt/update_schemes/enkf/#pipt.update_schemes.enkf.enkf_approx","title":"<code>enkf_approx</code>","text":"<p>               Bases: <code>enkfMixIn</code>, <code>approx_update</code></p> <p>MixIn the main EnKF update class with the standard analysis scheme.</p>"},{"location":"reference/pipt/update_schemes/enkf/#pipt.update_schemes.enkf.enkf_full","title":"<code>enkf_full</code>","text":"<p>               Bases: <code>enkfMixIn</code>, <code>approx_update</code></p> <p>MixIn the main EnKF update class with the standard analysis scheme. Note that this class is only included for completness. The EnKF does not iterate, and the standard scheme is therefor always applied.</p>"},{"location":"reference/pipt/update_schemes/enkf/#pipt.update_schemes.enkf.enkf_subspace","title":"<code>enkf_subspace</code>","text":"<p>               Bases: <code>enkfMixIn</code>, <code>subspace_update</code></p> <p>MixIn the main EnKF update class with the subspace analysis scheme.</p>"},{"location":"reference/pipt/update_schemes/enrml/","title":"enrml","text":"<p>EnRML type schemes</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.co_lm_enrml","title":"<code>co_lm_enrml</code>","text":"<p>               Bases: <code>lmenrmlMixIn</code>, <code>approx_update</code></p> <p>This is the implementation of the approximative LM-EnRML algorithm as described in <code>chen2013</code>.</p> <p>This algorithm is quite similar to the lm_enrml as provided above, and will therefore inherit most of its methods. We only change the calc_analysis part...</p> <p>% Copyright \u00a9 2019-2022 NORCE, All Rights Reserved. 4DSEIS</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.co_lm_enrml.__init__","title":"<code>__init__(keys_da)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.co_lm_enrml.calc_analysis","title":"<code>calc_analysis()</code>","text":"<p>Calculate the update step in approximate LM-EnRML code.</p> <p>Attributes:</p> Name Type Description <code>iteration</code> <code>int</code> <p>Iteration number</p> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>True if data mismatch is decreasing, False if increasing</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gn_enrml","title":"<code>gn_enrml</code>","text":"<p>               Bases: <code>lmenrmlMixIn</code></p> <p>This is the implementation of the stochastig IES as  described in <code>raanes2019</code>.</p> <p>More information about the method is found in <code>evensen2019</code>. This implementation is the Gauss-Newton version.</p> <p>This algorithm is quite similar to the <code>lm_enrml</code> as provided above, and will therefore inherit most of its methods. We only change the calc_analysis part...</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gn_enrml.__init__","title":"<code>__init__(keys_da)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gn_enrml.calc_analysis","title":"<code>calc_analysis()</code>","text":"Changelog <ul> <li>KF 25/2-20</li> </ul>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gn_enrml.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Check if GN-EnRML have converged based on evaluation of change sizes of objective function, state and damping parameter. Very similar to original function, but exit if there is no reduction in obj. function.</p> <p>Returns:</p> Name Type Description <code>conv</code> <code>bool</code> <p>Logic variable indicating if the algorithm has converged.</p> <code>status</code> <code>bool</code> <p>Indicates whether the objective function has reduced.</p> <code>why_stop</code> <code>dict</code> <p>Dictionary with keys corresponding to convergence criteria, with logical variables indicating which of them has been met.</p> Changelog <ul> <li>ST 3/6-16</li> <li>ST 6/6-16: Added LM damping param. check</li> <li>KF 16/11-20: Modified for GN-EnRML</li> <li>KF 10/3-21: Output whether the method reduced the objective function</li> </ul>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gnenrmlMixIn","title":"<code>gnenrmlMixIn</code>","text":"<p>               Bases: <code>Ensemble</code></p> <p>This is an implementation of EnRML using the Gauss-Newton approach. The update scheme is selected by a MixIn with multiple update_methods_ns. This class must therefore facititate many different update schemes.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gnenrmlMixIn.__init__","title":"<code>__init__(keys_da, keys_en, sim)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gnenrmlMixIn.calc_analysis","title":"<code>calc_analysis()</code>","text":"<p>Calculate the update step in LM-EnRML, which is just the Levenberg-Marquardt update algorithm with the sensitivity matrix approximated by the ensemble.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gnenrmlMixIn.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Check if LM-EnRML have converged based on evaluation of change sizes of objective function, state and damping parameter.</p> <p>Returns:</p> Name Type Description <code>conv</code> <code>bool</code> <p>Logic variable telling if algorithm has converged</p> <code>why_stop</code> <code>dict</code> <p>Dict. with keys corresponding to conv. criteria, with logical variable telling which of them that has been met</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.gnenrml_margis","title":"<code>gnenrml_margis</code>","text":"<p>               Bases: <code>gnenrmlMixIn</code>, <code>margIS_update</code></p> <p>The marg-IS scheme is currently not available in this version of PIPT. To utilize the scheme you have to import the margIS_update class from a standalone repository.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.lmenrmlMixIn","title":"<code>lmenrmlMixIn</code>","text":"<p>               Bases: <code>Ensemble</code></p> <p>This is an implementation of EnRML using Levenberg-Marquardt. The update scheme is selected by a MixIn with multiple update_methods_ns. This class must therefore facititate many different update schemes.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.lmenrmlMixIn.__init__","title":"<code>__init__(keys_da, keys_en, sim)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.lmenrmlMixIn.calc_analysis","title":"<code>calc_analysis()</code>","text":"<p>Calculate the update step in LM-EnRML, which is just the Levenberg-Marquardt update algorithm with the sensitivity matrix approximated by the ensemble.</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.lmenrmlMixIn.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Check if LM-EnRML have converged based on evaluation of change sizes of objective function, state and damping parameter. </p> <p>Returns:</p> Name Type Description <code>conv</code> <code>bool</code> <p>Logic variable telling if algorithm has converged</p> <code>why_stop</code> <code>dict</code> <p>Dict. with keys corresponding to conv. criteria, with logical variable telling which of them that has been met</p>"},{"location":"reference/pipt/update_schemes/enrml/#pipt.update_schemes.enrml.lmenrmlMixIn.log_update","title":"<code>log_update(success, prior_run=False)</code>","text":"<p>Log the update results in a formatted table.</p>"},{"location":"reference/pipt/update_schemes/es/","title":"es","text":"<p>ES type schemes</p>"},{"location":"reference/pipt/update_schemes/es/#pipt.update_schemes.es.esMixIn","title":"<code>esMixIn</code>","text":"<p>This is the straightforward ES analysis scheme. We treat this as a all-data-at-once EnKF step, hence the calc_analysis method here is identical to that in the <code>enkf</code> class. Since, for the moment, ASSIMINDEX is parsed in a specific manner (or more precise, single rows and columns in the PIPT init. file is parsed to a 1D list), a <code>Simultaneous</code> 'loop' had to be implemented, and <code>es</code> will use this to do the inversion. Maybe in the future, we can make the <code>enkf</code> class do simultaneous updating also. The consequence of all this is that we inherit BOTH <code>enkf</code> and <code>Simultaneous</code> classes, which is convenient. The <code>Simultaneous</code> class is inherited to set up the correct inversion structure and <code>enkf</code> is inherited to get <code>calc_analysis</code>, so we do not have to implement it again.</p>"},{"location":"reference/pipt/update_schemes/es/#pipt.update_schemes.es.esMixIn.__init__","title":"<code>__init__(keys_da, keys_en, sim)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p>"},{"location":"reference/pipt/update_schemes/es/#pipt.update_schemes.es.esMixIn.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Calculate the \"convergence\" of the method. Important to</p>"},{"location":"reference/pipt/update_schemes/es/#pipt.update_schemes.es.es_approx","title":"<code>es_approx</code>","text":"<p>               Bases: <code>esMixIn</code>, <code>enkf_approx</code></p> <p>Mixin of ES class and approximate update</p>"},{"location":"reference/pipt/update_schemes/es/#pipt.update_schemes.es.es_full","title":"<code>es_full</code>","text":"<p>               Bases: <code>esMixIn</code>, <code>enkf_full</code></p> <p>mixin of ES class and full update. Note that since we do not iterate there is no difference between is full and approx.</p>"},{"location":"reference/pipt/update_schemes/es/#pipt.update_schemes.es.es_subspace","title":"<code>es_subspace</code>","text":"<p>               Bases: <code>esMixIn</code>, <code>enkf_subspace</code></p> <p>mixin of ES class and subspace update.</p>"},{"location":"reference/pipt/update_schemes/esmda/","title":"esmda","text":"<p>ES-MDA type schemes</p>"},{"location":"reference/pipt/update_schemes/esmda/#pipt.update_schemes.esmda.esmdaMixIn","title":"<code>esmdaMixIn</code>","text":"<p>               Bases: <code>Ensemble</code></p> <p>This is the implementation of the ES-MDA algorithm given in <code>emerick2013a</code>. This algorithm have been implemented mostly to illustrate how a algorithm using the Mda loop can be implemented.</p>"},{"location":"reference/pipt/update_schemes/esmda/#pipt.update_schemes.esmda.esmdaMixIn.__init__","title":"<code>__init__(keys_da, keys_en, sim)</code>","text":"<p>The class is initialized by passing the keywords and simulator object upwards in the hierarchy.</p> <p>Parameters:</p> Name Type Description Default <code>keys_da</code> <ul> <li>tot_assim_steps: total number of iterations in MDA, e.g., 3</li> <li>inflation_param: covariance inflation factors, e.g., [2, 4, 4]</li> </ul> required <code>keys_en</code> <code>dict</code> required <code>sim</code> <code>callable</code> required"},{"location":"reference/pipt/update_schemes/esmda/#pipt.update_schemes.esmda.esmdaMixIn.calc_analysis","title":"<code>calc_analysis()</code>","text":"<p>Analysis step of ES-MDA. The analysis algorithm is similar to EnKF analysis, only difference is that the data covariance matrix is inflated with an inflation parameter alpha. The update is done as an iterative smoother where all data is assimilated at once.</p> Notes <p>ES-MDA is an iterative ensemble smoother with a predefined number of iterations, where the updates is done with the EnKF update equations but where the data covariance matrix have been inflated:</p> \\[ \\begin{align} d_{obs} &amp;= d_{true} + \\sqrt{\\alpha}C_d^{1/2}Z \\\\ m &amp;= m_{prior} + C_{md}(C_g + \\alpha C_d)^{-1}(g(m) - d_{obs}) \\end{align} \\] <p>where \\(d_{true}\\) is the true observed data, \\(\\alpha\\) is the inflation factor, \\(C_d\\) is the data covariance matrix, \\(Z\\) is a standard normal random variable, \\(C_{md}\\) and \\(C_{g}\\) are sample covariance matrices, \\(m\\) is the model parameter, and \\(g(\\)\\) is the predicted data. Note that \\(\\alpha\\) can have a different value in each assimilation step and must fulfill:</p> \\[ \\sum_{i=1}^{N_a} \\frac{1}{\\alpha} = 1 \\] <p>where \\(N_a\\) being the total number of assimilation steps.</p>"},{"location":"reference/pipt/update_schemes/esmda/#pipt.update_schemes.esmda.esmdaMixIn.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Check if LM-EnRML have converged based on evaluation of change sizes of objective function, state and damping parameter.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Logic variable telling if algorithm has converged</p> <code>dict</code> <p>Dict. with keys corresponding to conv. criteria, with logical variable telling which of them that has been met</p>"},{"location":"reference/pipt/update_schemes/esmda/#pipt.update_schemes.esmda.esmdaMixIn.log_update","title":"<code>log_update(success=None, prior_run=False)</code>","text":"<p>Log the update results in a formatted table.</p>"},{"location":"reference/pipt/update_schemes/esmda/#pipt.update_schemes.esmda.esmda_geo","title":"<code>esmda_geo</code>","text":"<p>               Bases: <code>esmda_approx</code></p> <p>This is the implementation of the ES-MDA-GEO algorithm from [1]. The main analysis step in this algorithm is the same as the standard ES-MDA algorithm (implemented in the <code>es_mda</code> class). The difference between this and the standard algorithm is the calculation of the inflation factor. Also see <code>rafiee2017</code>.</p>"},{"location":"reference/pipt/update_schemes/esmda/#pipt.update_schemes.esmda.esmda_geo.__init__","title":"<code>__init__(keys_da)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p>"},{"location":"reference/pipt/update_schemes/multilevel/","title":"multilevel","text":"<p>Here we place the classes that are required to run the multilevel schemes developed in the 4DSeis project. All methods inherit the ensemble class, hence the main loop is inherited. These classes will consider the analysis step.</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.esmda_hybrid","title":"<code>esmda_hybrid</code>","text":"<p>               Bases: <code>multilevel</code>, <code>hybrid_update</code>, <code>esmdaMixIn</code></p> <p>A multilevel implementation of the ES-MDA algorithm with the hybrid gain</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.esmda_hybrid.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Check ESMDA objective function for logging purposes.</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.multilevel","title":"<code>multilevel</code>","text":"<p>               Bases: <code>Ensemble</code></p> <p>Inititallize the multilevel class. Similar for all ML schemes, hence make one class for all.</p>"},{"location":"reference/pipt/update_schemes/multilevel/#pipt.update_schemes.multilevel.multilevel.reorganize_ml_prior","title":"<code>reorganize_ml_prior(enX)</code>","text":"<p>Reorganize prior ensemble to multilevel structure (list of matrices).</p>"},{"location":"reference/pipt/update_schemes/gies/","title":"gies","text":"<p>Descriptive description.</p>"},{"location":"reference/pipt/update_schemes/gies/gies_base/","title":"gies_base","text":"<p>EnRML type schemes</p>"},{"location":"reference/pipt/update_schemes/gies/gies_base/#pipt.update_schemes.gies.gies_base.GIESMixIn","title":"<code>GIESMixIn</code>","text":"<p>               Bases: <code>Ensemble</code></p> <p>This is a base template for implementating the generalized iterative ensemble smoother (GIES) in the following papers: Luo, Xiaodong. \"Novel iterative ensemble smoothers derived from a class of generalized cost functions.\"                 Computational Geosciences 25.3 (2021): 1159-1189. Luo, Xiaodong, and William C. Cruz. \"Data assimilation with soft constraints (DASC) through a generalized iterative                 ensemble smoother.\" Computational Geosciences 26.3 (2022): 571-594.</p>"},{"location":"reference/pipt/update_schemes/gies/gies_base/#pipt.update_schemes.gies.gies_base.GIESMixIn.__init__","title":"<code>__init__(keys_da, keys_fwd, sim)</code>","text":"<p>The class is initialized by passing the PIPT init. file upwards in the hierarchy to be read and parsed in <code>pipt.input_output.pipt_init.ReadInitFile</code>.</p> <p>Parameters:</p> Name Type Description Default <code>init_file</code> <p>PIPT init. file containing info. to run the inversion algorithm</p> required"},{"location":"reference/pipt/update_schemes/gies/gies_base/#pipt.update_schemes.gies.gies_base.GIESMixIn.calc_analysis","title":"<code>calc_analysis()</code>","text":"<p>Calculate the update step in LM-EnRML, which is just the Levenberg-Marquardt update algorithm with the sensitivity matrix approximated by the ensemble.</p>"},{"location":"reference/pipt/update_schemes/gies/gies_base/#pipt.update_schemes.gies.gies_base.GIESMixIn.check_convergence","title":"<code>check_convergence()</code>","text":"<p>Check if LM-EnRML have converged based on evaluation of change sizes of objective function, state and damping parameter. </p> <p>Returns:</p> Name Type Description <code>conv</code> <code>bool</code> <p>Logic variable telling if algorithm has converged</p> <code>why_stop</code> <code>dict</code> <p>Dict. with keys corresponding to conv. criteria, with logical variable telling which of them that has been met</p>"},{"location":"reference/pipt/update_schemes/gies/rlmmac_update/","title":"rlmmac_update","text":"<p>EnRML (IES) without the prior increment term.</p>"},{"location":"reference/pipt/update_schemes/gies/rlmmac_update/#pipt.update_schemes.gies.rlmmac_update.rlmmac_update","title":"<code>rlmmac_update</code>","text":"<p>Regularized Levenburg-Marquardt algorithm for Minimum Average Cost (RLM-MAC) problem, following the paper: Luo, Xiaodong, et al. \"Iterative ensemble smoother as an approximate solution to a regularized minimum-average-cost problem: theory and applications.\" SPE Journal 2015: 962-982.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/","title":"update_methods_ns","text":"<p>Descriptive description.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/approx_update/","title":"approx_update","text":"<p>EnRML (IES) without the prior increment term.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/approx_update/#pipt.update_schemes.update_methods_ns.approx_update.approx_update","title":"<code>approx_update</code>","text":"<p>Approximate LM Update scheme as defined in \"Chen, Y., &amp; Oliver, D. S. (2013). Levenberg\u2013Marquardt forms of the iterative ensemble smoother for efficient history matching and uncertainty quantification. Computational Geosciences, 17(4), 689\u2013703. https://doi.org/10.1007/s10596-013-9351-5\". Note that for a EnKF or ES update, or for update within GN scheme, lambda = 0.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/approx_update/#pipt.update_schemes.update_methods_ns.approx_update.approx_update.scale","title":"<code>scale(data, scaling)</code>","text":"<p>Scale the data perturbations by the data error standard deviation.</p> <p>Args:     data (np.ndarray): data perturbations     scaling (np.ndarray): data error standard deviation</p> <p>Returns:     np.ndarray: scaled data perturbations</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/approx_update/#pipt.update_schemes.update_methods_ns.approx_update.approx_update.update","title":"<code>update(enX, enY, enE, **kwargs)</code>","text":"<p>Perform the approximate LM update.</p> Parameters: <pre><code>enX : np.ndarray \n    State ensemble matrix (nx, ne)\n\nenY : np.ndarray\n    Predicted data ensemble matrix (nd, ne)\n\nenE : np.ndarray\n    Ensemble of perturbed observations (nd, ne)\n</code></pre>"},{"location":"reference/pipt/update_schemes/update_methods_ns/full_update/","title":"full_update","text":"<p>EnRML (IES) as in 2013.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/full_update/#pipt.update_schemes.update_methods_ns.full_update.full_update","title":"<code>full_update</code>","text":"<p>Full LM Update scheme as defined in \"Chen, Y., &amp; Oliver, D. S. (2013). Levenberg\u2013Marquardt forms of the iterative ensemble smoother for efficient history matching and uncertainty quantification. Computational Geosciences, 17(4), 689\u2013703. https://doi.org/10.1007/s10596-013-9351-5\". Note that for a EnKF or ES update, or for update within GN scheme, lambda = 0.</p> <p>Note</p> <p>no localization is implemented for this method yet.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/full_update/#pipt.update_schemes.update_methods_ns.full_update.full_update.ext_Am","title":"<code>ext_Am(*args, **kwargs)</code>","text":"<p>The class is initialized by calculating the required Am matrix.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/full_update/#pipt.update_schemes.update_methods_ns.full_update.full_update.scale","title":"<code>scale(data, scaling)</code>","text":"<p>Scale the data perturbations by the data error standard deviation.</p> <p>Args:     data (np.ndarray): data perturbations     scaling (np.ndarray): data error standard deviation</p> <p>Returns:     np.ndarray: scaled data perturbations</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/hybrid_update/","title":"hybrid_update","text":"<p>ES, and Iterative ES updates with hybrid update matrix calculated from multi-fidelity runs.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/hybrid_update/#pipt.update_schemes.update_methods_ns.hybrid_update.hybrid_update","title":"<code>hybrid_update</code>","text":"<p>Class for hybrid update schemes as described in: Fossum, K., Mannseth, T., &amp; Stordal, A. S. (2020). Assessment of multilevel ensemble-based data assimilation for reservoir history matching. Computational Geosciences, 24(1), 217\u2013239. https://doi.org/10.1007/s10596-019-09911-x</p> <p>Note that the scheme is slightly modified to be inline with the standard (I)ES approximate update scheme. This enables the scheme to efficiently be coupled with multiple updating strategies via class MixIn</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/hybrid_update/#pipt.update_schemes.update_methods_ns.hybrid_update.hybrid_update.scale","title":"<code>scale(data, scaling)</code>","text":"<p>Scale the data perturbations by the data error standard deviation.</p> <p>Args:     data (np.ndarray): data perturbations     scaling (np.ndarray): data error standard deviation</p> <p>Returns:     np.ndarray: scaled data perturbations</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/hybrid_update/#pipt.update_schemes.update_methods_ns.hybrid_update.hybrid_update.update","title":"<code>update(enX, enY, enE, **kwargs)</code>","text":"<p>Perform the hybrid update.</p> Parameters: <pre><code>enX : list of np.ndarray \n    List of state ensemble matrices for each level (nx, ne)\n\nenY : list of np.ndarray\n    List of predicted data ensemble matrices for each level (nd, ne)\n\nenE : list of np.ndarray\n    List of ensemble of perturbed observations for each level (nd, ne)\n</code></pre>"},{"location":"reference/pipt/update_schemes/update_methods_ns/subspace_update/","title":"subspace_update","text":"<p>Stochastic iterative ensemble smoother (IES, i.e. EnRML) with subspace implementation.</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/subspace_update/#pipt.update_schemes.update_methods_ns.subspace_update.subspace_update","title":"<code>subspace_update</code>","text":"<p>Ensemble subspace update, as described in  Raanes, P. N., Stordal, A. S., &amp; Evensen, G. (2019). Revising the stochastic iterative ensemble smoother. Nonlinear Processes in Geophysics, 26(3), 325\u2013338. https://doi.org/10.5194/npg-26-325-2019 More information about the method is found in Evensen, G., Raanes, P. N., Stordal, A. S., &amp; Hove, J. (2019). Efficient Implementation of an Iterative Ensemble Smoother for Data Assimilation and Reservoir History Matching. Frontiers in Applied Mathematics and Statistics, 5(October), 114. https://doi.org/10.3389/fams.2019.00047</p>"},{"location":"reference/pipt/update_schemes/update_methods_ns/subspace_update/#pipt.update_schemes.update_methods_ns.subspace_update.subspace_update.scale","title":"<code>scale(data, scaling)</code>","text":"<p>Scale the data perturbations by the data error standard deviation.</p> <p>Args:     data (np.ndarray): data perturbations     scaling (np.ndarray): data error standard deviation</p> <p>Returns:     np.ndarray: scaled data perturbations</p>"},{"location":"reference/popt/","title":"popt","text":"<p>Optimisation methods.</p>"},{"location":"reference/popt/#popt--python-optimization-problem-toolbox-popt","title":"Python Optimization Problem Toolbox (POPT)","text":"<p>POPT is one part of the PET application. Here we solve the optimization problem.  Currently, the following methods are implemented: </p> <ul> <li>EnOpt: The standard ensemble optimization method</li> <li>GenOpt: Generalized ensemble optimization (using non-Gaussian distributions)</li> <li>SmcOpt: Gradient-free optimization based on sequential Monte Carlo</li> <li>LineSearch: Gradient based method satisfying the strong Wolfie conditions</li> </ul> <p>The gradient and Hessian methods are compatible with SciPy, and can be used as input to scipy.optimize.minimize.  A POPT tutorial is found here.</p>"},{"location":"reference/popt/cost_functions/","title":"cost_functions","text":""},{"location":"reference/popt/cost_functions/ecalc_npv/","title":"ecalc_npv","text":"<p>Net present value.</p>"},{"location":"reference/popt/cost_functions/ecalc_npv/#popt.cost_functions.ecalc_npv.ecalc_npv","title":"<code>ecalc_npv(pred_data, **kwargs)</code>","text":"<p>Net present value cost function using eCalc to calculate emmisions</p> <p>Parameters:</p> Name Type Description Default <code>pred_data</code> <code>array_like</code> <p>Ensemble of predicted data.</p> required <code>**kwargs</code> <code>dict</code> <p>Other arguments sent to the npv function</p> <p>keys_opt : list     Keys with economic data.</p> <p>report : list     Report dates.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>objective_values</code> <code>array_like</code> <p>Objective function values (NPV) for all ensemble members.</p>"},{"location":"reference/popt/cost_functions/ecalc_npv/#popt.cost_functions.ecalc_npv.results_as_df","title":"<code>results_as_df(yaml_model, results, getter)</code>","text":"<p>Extract relevant values, as well as some meta (<code>attrs</code>).</p>"},{"location":"reference/popt/cost_functions/ecalc_pareto_npv/","title":"ecalc_pareto_npv","text":"<p>Net present value.</p>"},{"location":"reference/popt/cost_functions/ecalc_pareto_npv/#popt.cost_functions.ecalc_pareto_npv.ecalc_pareto_npv","title":"<code>ecalc_pareto_npv(pred_data, kwargs)</code>","text":"<p>Net present value cost function using eCalc to calculate emmisions</p> <p>Parameters:</p> Name Type Description Default <code>pred_data</code> <code>array_like</code> <p>Ensemble of predicted data.</p> required <code>**kwargs</code> <code>dict</code> <p>Other arguments sent to the npv function</p> <p>keys_opt : list     Keys with economic data.</p> <p>report : list     Report dates.</p> required <p>Returns:</p> Name Type Description <code>objective_values</code> <code>array_like</code> <p>Objective function values (NPV) for all ensemble members.</p>"},{"location":"reference/popt/cost_functions/ecalc_pareto_npv/#popt.cost_functions.ecalc_pareto_npv.results_as_df","title":"<code>results_as_df(yaml_model, results, getter)</code>","text":"<p>Extract relevant values, as well as some meta (<code>attrs</code>).</p>"},{"location":"reference/popt/cost_functions/npv/","title":"npv","text":"<p>Net present value.</p>"},{"location":"reference/popt/cost_functions/npv/#popt.cost_functions.npv.npv","title":"<code>npv(pred_data, **kwargs)</code>","text":"<p>Net present value cost function</p> <p>Parameters:</p> Name Type Description Default <code>pred_data</code> <code>array_like</code> <p>Ensemble of predicted data.</p> required <code>**kwargs</code> <code>dict</code> <p>Other arguments sent to the npv function</p> <p>keys_opt : list     Keys with economic data.</p> <pre><code>    - wop: oil price\n    - wgp: gas price\n    - wwp: water production cost\n    - wwi: water injection cost\n    - disc: discount factor\n    - obj_scaling: used to scale the objective function (negative since all methods are minimizers)\n</code></pre> <p>report : list     Report dates.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>objective_values</code> <code>ndarray</code> <p>Objective function values (NPV) for all ensemble members.</p>"},{"location":"reference/popt/cost_functions/quadratic/","title":"quadratic","text":"<p>Descriptive description.</p>"},{"location":"reference/popt/cost_functions/quadratic/#popt.cost_functions.quadratic.quadratic","title":"<code>quadratic(x, *args, **kwargs)</code>","text":"<p>Quadratic objective function</p> \\[ f(x) = ||x - b||^2_A \\]"},{"location":"reference/popt/cost_functions/ren_npv/","title":"ren_npv","text":"<p>Net present value cost function with injection from RENewable energy</p>"},{"location":"reference/popt/cost_functions/ren_npv/#popt.cost_functions.ren_npv.ren_npv","title":"<code>ren_npv(pred_data, kwargs)</code>","text":"<p>Net present value cost function with injection from RENewable energy</p> <p>Parameters:</p> Name Type Description Default <code>pred_data</code> <code>ndarray</code> <p>Ensemble of predicted data.</p> required <code>**kwargs</code> <code>dict</code> <p>Other arguments sent to the npv function</p> <ul> <li> <p>keys_opt (list)     Keys with economic data.</p> </li> <li> <p>report (list)     Report dates.</p> </li> </ul> required <p>Returns:</p> Name Type Description <code>objective_values</code> <code>ndarray</code> <p>Objective function values (NPV) for all ensemble members.</p>"},{"location":"reference/popt/cost_functions/ren_npv_co2/","title":"ren_npv_co2","text":"<p>Net Present Value with Renewable Power and co2 emissions</p>"},{"location":"reference/popt/cost_functions/ren_npv_co2/#popt.cost_functions.ren_npv_co2.ren_npv_co2","title":"<code>ren_npv_co2(pred_data, keys_opt, report, save_emissions=False)</code>","text":"<p>Net Present Value with Renewable Power and co2 emissions (with eCalc)</p> <p>Parameters:</p> Name Type Description Default <code>pred_data</code> <code>array_like</code> <p>Ensemble of predicted data.</p> required <code>keys_opt</code> <code>list</code> <p>Keys with economic data.</p> required <code>report</code> <code>list</code> <p>Report dates.</p> required <p>Returns:</p> Name Type Description <code>objective_values</code> <code>array_like</code> <p>Objective function values (NPV) for all ensemble members.</p>"},{"location":"reference/popt/cost_functions/rosenbrock/","title":"rosenbrock","text":"<p>Rosenbrock objective function.</p>"},{"location":"reference/popt/cost_functions/rosenbrock/#popt.cost_functions.rosenbrock.rosenbrock","title":"<code>rosenbrock(state, *args, **kwargs)</code>","text":"<p>Rosenbrock: http://en.wikipedia.org/wiki/Rosenbrock_function</p>"},{"location":"reference/popt/loop/","title":"loop","text":"<p>Main loop for running optimization.</p>"},{"location":"reference/popt/misc_tools/","title":"misc_tools","text":"<p>Optimisation helpers.</p>"},{"location":"reference/popt/misc_tools/basic_tools/","title":"basic_tools","text":"<p>Collection of simple, yet useful Python tools</p>"},{"location":"reference/popt/misc_tools/basic_tools/#popt.misc_tools.basic_tools.index2d","title":"<code>index2d(list2d, value)</code>","text":"<p>Search in a 2D list for pattern or value and return is (i, j) index. If the pattern/value is not found, (None, None) is returned</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; l = [['string1', 1], ['string2', 2]]\n&gt;&gt;&gt; print index2d(l, 'string1')\n(0, 0)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>list2d</code> <code>list of lists</code> <p>2D list.</p> required <code>value</code> <code>object</code> <p>Pattern or value to search for.</p> required <p>Returns:</p> Name Type Description <code>ind</code> <code>tuple</code> <p>Indices (i, j) of the value.</p>"},{"location":"reference/popt/misc_tools/basic_tools/#popt.misc_tools.basic_tools.read_file","title":"<code>read_file(val_type, filename)</code>","text":"<p>Read an eclipse file with specified keyword.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; read_file('PERMX','filename.permx')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>val_type</code> <p>keyword or property</p> required <code>filename</code> <p>the file that is read</p> required <p>Returns:</p> Name Type Description <code>values</code> <p>a vector with values for each cell</p>"},{"location":"reference/popt/misc_tools/basic_tools/#popt.misc_tools.basic_tools.write_file","title":"<code>write_file(filename, val_type, data)</code>","text":"<p>Write an eclipse file with specified keyword.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; write_file('filename.permx','PERMX',data_vec)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>the file that is read</p> required <code>val_type</code> <p>keyword or property</p> required <code>data</code> <p>data written to file</p> required"},{"location":"reference/popt/misc_tools/optim_tools/","title":"optim_tools","text":"<p>Collection of tools that can be used in optimization schemes. Only put tools here that are so general that they can be used by several optimization schemes. If some method is only applicable to the update scheme you are implementing, leave it in that class.</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.aug_optim_state","title":"<code>aug_optim_state(state, list_state)</code>","text":"<p>Augment the state variables to get one augmented array.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary of state variables for optimization. OBS: 1D arrays!</p> required <code>list_state</code> <code>list</code> <p>Fixed list of keys in the state dictionary.</p> required <p>Returns:</p> Name Type Description <code>aug_state</code> <code>ndarray</code> <p>Augmented 1D array of state variables.</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.clip_state","title":"<code>clip_state(x, bounds)</code>","text":"<p>Clip a state vector according to the bounds</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>The input state</p> required <code>bounds</code> <code>array_like</code> <p>(min, max) pairs for each element in x. None is used to specify no bound.</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>The state after truncation</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.corr2BlockDiagonal","title":"<code>corr2BlockDiagonal(state, corr)</code>","text":"<p>Makes the correlation matrix block diagonal. The blocks are the state varible types.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <p>Current control state, including state names</p> required <code>corr</code> <code>array_like</code> <p>Correlation matrix, of shape (d, d)</p> required <p>Returns:</p> Name Type Description <code>corr_blocks</code> <code>list</code> <p>block matrices, one for each variable type</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.corr2cov","title":"<code>corr2cov(corr, std)</code>","text":"<p>Transfroms a correlation matrix to a covaraince matrix</p> <p>Parameters:</p> Name Type Description Default <code>corr</code> <code>array_like</code> <p>The correlation matrix, of shape (d,d).</p> required <code>std</code> <code>array_like</code> <p>Array of the standard deviations, of shape (d, ).</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>The covaraince matrix, of shape (d,d)</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.cov2corr","title":"<code>cov2corr(cov)</code>","text":"<p>Transfroms a covaraince matrix to a correlation matrix</p> <p>Parameters:</p> Name Type Description Default <code>cov</code> <code>array_like</code> <p>The covaraince matrix, of shape (d,d).</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>The correlation matrix, of shape (d,d)</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.get_list_element","title":"<code>get_list_element(list, element)</code>","text":"<p>Retrieve the value associated with a given element in a list of tuples.</p> <p>Parameters:</p> Name Type Description Default <code>list</code> <code>list</code> <p>A list of tuples, where each tuple contains two elements.</p> required <code>element</code> <code>any</code> <p>The element to search for in the first position of the tuples.</p> required <p>Returns:</p> Type Description <code>any</code> <p>The value associated with the given element in the list of tuples, or None if the element is not found.</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.get_optimize_result","title":"<code>get_optimize_result(obj)</code>","text":"<p>Collect optimize results based on requested</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Optimize</code> <p>An instance of an optimization class</p> required <p>Returns:</p> Name Type Description <code>save_dict</code> <code>OptimizeResult</code> <p>The requested optimization results</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.get_sym_pos_semidef","title":"<code>get_sym_pos_semidef(a)</code>","text":"<p>Force matrix to positive semidefinite</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>array_like</code> <p>The input matrix, of shape (d,d)</p> required <p>Returns:</p> Name Type Description <code>a</code> <code>ndarray</code> <p>The positive semidefinite matrix, of shape (d,d)</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.save_optimize_results","title":"<code>save_optimize_results(intermediate_result)</code>","text":"<p>Save optimize results</p> <p>Parameters:</p> Name Type Description Default <code>intermediate_result</code> <code>OptimizeResult</code> <p>An instance of an OptimizeResult class</p> required"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.time_correlation","title":"<code>time_correlation(a, state, n_timesteps, dt=1.0)</code>","text":"<p>Constructs correlation matrix with time correlation using an autoregressive model.</p> \\[ Corr(t_1, t_2) = a^{|t_1 - t_2|} \\] <p>Assumes that each varaible in state is time-order such that <code>x = [x1, x2,..., xi,..., xn]</code>, where <code>i</code> is the time index,  and <code>xi</code> is d-dimensional.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Correlation coef, in range (0, 1).</p> required <code>state</code> <code>dict</code> <p>Control state (represented in a dict).</p> required <code>n_timesteps</code> <code>int</code> <p>Number of time-steps to correlate for each component.</p> required <code>dt</code> <code>float or int</code> <p>Duration between each time-step. Default is 1.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>Correlation matrix with time correlation</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.toggle_ml_state","title":"<code>toggle_ml_state(state, ml_ne)</code>","text":"<p>Toggle the state from a dictionary to a list of levels, or from a list of levels to a dictionary. This is necessary when we are using multi-level ensembles.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict or list</code> <p>The current state, either as a dictionary or a list of levels.</p> required <code>ml_ne</code> <code>list</code> <p>List of ensemble sizes for each level.</p> required <p>Returns:</p> Name Type Description <code>new_state</code> <code>dict or list</code> <p>The toggled state, either as a list of levels or a dictionary.</p>"},{"location":"reference/popt/misc_tools/optim_tools/#popt.misc_tools.optim_tools.update_optim_state","title":"<code>update_optim_state(aug_state, state, list_state)</code>","text":"<p>Extract the separate state variables from an augmented state array.</p> <p>It is assumed that the augmented state array is made in the aug_optim_state method, hence this is the reverse method.</p> <p>Parameters:</p> Name Type Description Default <code>aug_state</code> <code>ndarray</code> <p>Augmented state array.</p> required <code>state</code> <code>dict</code> <p>Dictionary of state variables for optimization.</p> required <code>list_state</code> <code>list</code> <p>Fixed list of keys in the state dictionary.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>dict</code> <p>State dictionary updated with aug_state.</p>"},{"location":"reference/popt/update_schemes/","title":"update_schemes","text":"<p>Iterative steppers.</p>"},{"location":"reference/popt/update_schemes/enopt/","title":"enopt","text":"<p>Ensemble optimisation algorithm.</p>"},{"location":"reference/popt/update_schemes/enopt/#popt.update_schemes.enopt.EnOpt","title":"<code>EnOpt</code>","text":"<p>               Bases: <code>Optimize</code></p> <p>This is an implementation of the ensemble steepest descent ensemble optimization algorithm - EnOpt. The update of the control variable is done with the simple steepest (or gradient) descent algorithm:</p> \\[ x_l = x_{l-1} - \\alpha \\times C \\times G \\] <p>where \\(x\\) is the control variable, \\(l\\) is the iteration index, \\(\\alpha\\) is the step size, \\(C\\) is a smoothing matrix (e.g., covariance matrix for \\(x\\)), and \\(G\\) is the ensemble gradient.</p> <p>Methods:</p> Name Description <code>calc_update</code> <p>Update using steepest descent method with ensemble gradient</p> References <p>Chen et al., 2009, 'Efficient Ensemble-Based Closed-Loop Production Optimization', SPE Journal, 14 (4): 634-645.</p> <p>TODO: Implement getter for optimize_result</p>"},{"location":"reference/popt/update_schemes/enopt/#popt.update_schemes.enopt.EnOpt.__init__","title":"<code>__init__(fun, x, args, jac, hess, bounds=None, **options)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fun</code> <code>callable</code> <p>objective function</p> required <code>x</code> <code>ndarray</code> <p>Initial state</p> required <code>args</code> <code>tuple</code> <p>Initial covariance</p> required <code>jac</code> <code>callable</code> <p>Gradient function</p> required <code>hess</code> <code>callable</code> <p>Hessian function</p> required <code>bounds</code> <code>list</code> <p>(min, max) pairs for each element in x. None is used to specify no bound.</p> <code>None</code> <code>options</code> <code>dict</code> <p>Optimization options</p> <ul> <li>maxiter: maximum number of iterations (default 10)</li> <li>restart: restart optimization from a restart file (default false)</li> <li>restartsave: save a restart file after each successful iteration (defalut false)</li> <li>tol: convergence tolerance for the objective function (default 1e-6)</li> <li>alpha: step size for the steepest descent method (default 0.1)</li> <li>beta: momentum coefficient for running accelerated optimization (default 0.0)</li> <li>alpha_maxiter: maximum number of backtracing trials (default 5)</li> <li>resample: number indicating how many times resampling is tried if no improvement is found</li> <li>optimizer: 'GD' (gradient descent) or Adam (default 'GD')</li> <li>nesterov: use Nesterov acceleration if true (default false)</li> <li>hessian: use Hessian approximation (if the algorithm permits use of Hessian) (default false)</li> <li>normalize: normalize the gradient if true (default true)</li> <li>cov_factor: factor used to shrink the covariance for each resampling trial (defalut 0.5)</li> <li>savedata: specify which class variables to save to the result files (state, objective             function value, iteration number, number of function evaluations, and number             of gradient evaluations, are always saved)</li> </ul> <code>{}</code>"},{"location":"reference/popt/update_schemes/enopt/#popt.update_schemes.enopt.EnOpt.calc_update","title":"<code>calc_update()</code>","text":"<p>Update using steepest descent method with ensemble gradients</p>"},{"location":"reference/popt/update_schemes/genopt/","title":"genopt","text":"<p>Non-Gaussian generalisation of EnOpt.</p>"},{"location":"reference/popt/update_schemes/genopt/#popt.update_schemes.genopt.GenOpt","title":"<code>GenOpt</code>","text":"<p>               Bases: <code>Optimize</code></p>"},{"location":"reference/popt/update_schemes/genopt/#popt.update_schemes.genopt.GenOpt.__init__","title":"<code>__init__(fun, x, args, jac, jac_mut, corr_adapt=None, bounds=None, **options)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fun</code> <code>callable</code> <p>objective function</p> required <code>x</code> <code>ndarray</code> <p>Initial state</p> required <code>args</code> <code>tuple</code> <p>Initial covariance</p> required <code>jac</code> <code>callable</code> <p>Gradient function</p> required <code>jac_mut</code> <code>callable</code> <p>Mutation gradient function</p> required <code>corr_adapt</code> <code>callable</code> <p>Function for correalation matrix adaption</p> <code>None</code> <code>bounds</code> <code>list</code> <p>(min, max) pairs for each element in x. None is used to specify no bound.</p> <code>None</code> <code>options</code> <code>dict</code> <p>Optimization options</p> <code>{}</code>"},{"location":"reference/popt/update_schemes/genopt/#popt.update_schemes.genopt.GenOpt.calc_update","title":"<code>calc_update()</code>","text":"<p>Update using steepest descent method with ensemble gradients</p>"},{"location":"reference/popt/update_schemes/smcopt/","title":"smcopt","text":"<p>Stochastic Monte-Carlo optimisation.</p>"},{"location":"reference/popt/update_schemes/smcopt/#popt.update_schemes.smcopt.SmcOpt","title":"<code>SmcOpt</code>","text":"<p>               Bases: <code>Optimize</code></p> <p>TODO: Write docstring ala EnOpt</p>"},{"location":"reference/popt/update_schemes/smcopt/#popt.update_schemes.smcopt.SmcOpt.__init__","title":"<code>__init__(fun, x, args, sens, bounds=None, **options)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fun</code> <code>callable</code> <p>objective function</p> required <code>x</code> <code>ndarray</code> <p>Initial state</p> required <code>sens</code> <code>callable</code> <p>Ensemble sensitivity</p> required <code>bounds</code> <code>list</code> <p>(min, max) pairs for each element in x. None is used to specify no bound.</p> <code>None</code> <code>options</code> <code>dict</code> <p>Optimization options</p> <ul> <li>maxiter: maximum number of iterations (default 10)</li> <li>restart: restart optimization from a restart file (default false)</li> <li>restartsave: save a restart file after each successful iteration (defalut false)</li> <li>tol: convergence tolerance for the objective function (default 1e-6)</li> <li>alpha: weight between previous and new step (default 0.1)</li> <li>alpha_maxiter: maximum number of backtracing trials (default 5)</li> <li>resample: number indicating how many times resampling is tried if no improvement is found</li> <li>cov_factor: factor used to shrink the covariance for each resampling trial (defalut 0.5)</li> <li>inflation_factor: term used to weight down prior influence (defalult 1)</li> <li>survival_factor: fraction of surviving samples</li> <li>savedata: specify which class variables to save to the result files (state, objective function             value, iteration number, number of function evaluations, and number of gradient             evaluations, are always saved)</li> </ul> <code>{}</code>"},{"location":"reference/popt/update_schemes/smcopt/#popt.update_schemes.smcopt.SmcOpt.calc_update","title":"<code>calc_update()</code>","text":"<p>Update using sequential monte carlo method</p>"},{"location":"reference/popt/update_schemes/subroutines/cma/","title":"cma","text":"<p>Covariance matrix adaptation (CMA).</p>"},{"location":"reference/popt/update_schemes/subroutines/cma/#popt.update_schemes.subroutines.cma.CMA","title":"<code>CMA</code>","text":""},{"location":"reference/popt/update_schemes/subroutines/cma/#popt.update_schemes.subroutines.cma.CMA.__call__","title":"<code>__call__(cov, step, X, J)</code>","text":"<p>Performs the CMA update.</p> <p>Parameters:</p> Name Type Description Default <code>cov</code> <code>array_like, of shape (d, d)</code> <p>Current covariance or correlation matrix.</p> required <code>step</code> <code>array_like, of shape (d,)</code> <p>New step of control vector. Used to update the evolution path.</p> required <code>X</code> <code>array_like, of shape (n, d)</code> <p>Control ensemble of size n.</p> required <code>J</code> <code>array_like, of shape (n,)</code> <p>Objective ensemble of size n.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>array_like, of shape (d, d)</code> <p>CMA updated covariance (correlation) matrix.</p>"},{"location":"reference/popt/update_schemes/subroutines/cma/#popt.update_schemes.subroutines.cma.CMA.__init__","title":"<code>__init__(ne, dim, alpha_mu=None, n_mu=None, alpha_1=None, alpha_c=None, corr_update=False, equal_weights=True)</code>","text":"<p>This is a rather simple simple CMA class <code>hansen2006</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ne</code> <code>int</code> <p>Ensemble size</p> required <code>dim</code> <code>int</code> <p>Dimensions of control vector</p> required <code>alpha_mu</code> <code>float</code> <p>Learning rate for rank-mu update. If None, value proposed in [1] is used.</p> <code>None</code> <code>n_mu</code> <code>int, `n_mu &lt; ne`</code> <p>Number of best samples of ne, to be used for rank-mu update. Default is int(ne/2).</p> <code>None</code> <code>alpha_1</code> <code>float</code> <p>Learning rate fro rank-one update. If None, value proposed in [1] is used.</p> <code>None</code> <code>alpha_c</code> <code>float</code> <p>Parameter (inverse if backwards time horizen)for evolution path update  in the rank-one update. See [1] for more info. If None, value proposed in [1] is used.</p> <code>None</code> <code>corr_update</code> <code>bool</code> <p>If True, CMA is used to update a correlation matrix. Default is False.</p> <code>False</code> <code>equal_weights</code> <code>bool</code> <p>If True, all n_mu members are assign equal weighting, <code>w_i = 1/n_mu</code>. If False, the weighting scheme proposed in [1], where <code>w_i = log(n_mu + 1)-log(i)</code>, and normalized such that they sum to one. Defualt is True.</p> <code>True</code>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/","title":"optimizers","text":"<p>Gradient acceleration.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.AdaMax","title":"<code>AdaMax</code>","text":"<p>               Bases: <code>Adam</code></p> <p>AdaMax optimizer <code>kingma2014</code></p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Adam","title":"<code>Adam</code>","text":"<p>A class implementing the Adam optimizer for gradient-based optimization <code>kingma2014</code>.</p> <p>The Adam update equation for the control x using gradient g, iteration t, and small constants \u03b5 is given by:</p> <pre><code>m_t = \u03b21 * m_{t-1} + (1 - \u03b21) * g\n\nv_t = \u03b22 * v_{t-1} + (1 - \u03b22) * g^2\n\nm_t_hat = m_t / (1 - \u03b21^t)\n\nv_t_hat = v_t / (1 - \u03b22^t)\n\nx_{t+1} = x_t - \u03b1 * m_t_hat / (sqrt(v_t_hat) + \u03b5)\n</code></pre> <p>Attributes:</p> Name Type Description <code>step_size</code> <code>float</code> <p>The initial step size provided during initialization.</p> <code>beta1</code> <code>float</code> <p>The exponential decay rate for the first moment estimates.</p> <code>beta2</code> <code>float</code> <p>The exponential decay rate for the second moment estimates.</p> <code>vel1</code> <code>1-D array_like</code> <p>First moment estimate.</p> <code>vel2</code> <code>1-D array_like</code> <p>Second moment estimate.</p> <code>eps</code> <code>float</code> <p>Small constant to prevent division by zero.</p> <code>_step_size</code> <code>float</code> <p>Private attribute for temporarily modifying step size.</p> <code>temp_vel1</code> <code>1-D array_like</code> <p>Temporary first moment estimate.</p> <code>temp_vel2</code> <code>1-D array_like</code> <p>Temporary Second moment estimate.</p> <p>Methods:</p> Name Description <code>apply_update</code> <p>Apply an Adam update to the control parameter.</p> <code>apply_backtracking</code> <p>Apply backtracking by reducing step size temporarily.</p> <code>restore_parameters</code> <p>Restore the original step size.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Adam.__init__","title":"<code>__init__(step_size, beta1=0.9, beta2=0.999)</code>","text":"<p>A class implementing the Adam optimizer for gradient-based optimization. The Adam update equation for the control x using gradient g,  iteration t, and small constants \u03b5 is given by:</p> <pre><code>m_t = \u03b21 * m_{t-1} + (1 - \u03b21) * g\n\nv_t = \u03b22 * v_{t-1} + (1 - \u03b22) * g^2\n\nm_t_hat = m_t / (1 - \u03b21^t)\n\nv_t_hat = v_t / (1 - \u03b22^t)\n\nx_{t+1} = x_t - \u03b1 * m_t_hat / (sqrt(v_t_hat) + \u03b5)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>step_size</code> <code>float</code> <p>The step size (learning rate) for the optimization.</p> required <code>beta1</code> <code>float</code> <p>The exponential decay rate for the first moment estimates (default is 0.9).</p> <code>0.9</code> <code>beta2</code> <code>float</code> <p>The exponential decay rate for the second moment estimates (default is 0.999).</p> <code>0.999</code>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Adam.apply_backtracking","title":"<code>apply_backtracking()</code>","text":"<p>Apply backtracking by reducing step size temporarily.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Adam.apply_update","title":"<code>apply_update(control, gradient, **kwargs)</code>","text":"<p>Apply a gradient update to the control parameter.</p> <p>Note</p> <p>This is the steepest descent update: x_new = x_old - x_step.</p> <p>Parameters:</p> Name Type Description Default <code>control</code> <code>array_like</code> <p>The current value of the parameter being optimized.</p> required <code>gradient</code> <code>array_like</code> <p>The gradient of the objective function with respect to the control parameter.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments, including 'iter' for the current iteration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>new_control, temp_velocity: tuple</code> <p>The new value of the control parameter after the update, and the current state step.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Adam.restore_parameters","title":"<code>restore_parameters()</code>","text":"<p>Restore the original step size.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.GradientDescent","title":"<code>GradientDescent</code>","text":"<p>A class for performing gradient descent optimization with momentum and backtracking. The gradient descent update equation with momentum is given by:</p> \\[ \\begin{align}     v_t &amp;= \\beta * v_{t-1} + \\alpha * gradient \\\\     x_t &amp;= x_{t-1} - v_t \\end{align} \\] <p>Attributes:</p> Name Type Description <code>step_size</code> <code>float</code> <p>The initial step size provided during initialization.</p> <code>momentum</code> <code>float</code> <p>The initial momentum factor provided during initialization.</p> <code>velocity</code> <code>array_like</code> <p>Current velocity of the optimization process.</p> <code>temp_velocity</code> <code>array_like</code> <p>Temporary velocity</p> <code>_step_size</code> <code>float</code> <p>Private attribute for temporarily modifying step size.</p> <code>_momentum</code> <code>float</code> <p>Private attribute for temporarily modifying momentum.</p> <p>Methods:</p> Name Description <code>apply_update</code> <p>Apply a gradient update to the control parameter.</p> <code>apply_backtracking</code> <p>Apply backtracking by reducing step size and momentum temporarily.</p> <code>restore_parameters</code> <p>Restore the original step size and momentum values.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.GradientDescent.__init__","title":"<code>__init__(step_size, momentum)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>step_size</code> <code>float</code> <p>The step size (learning rate) for the gradient descent.</p> required <code>momentum</code> <code>float</code> <p>The momentum factor to apply during updates.</p> required"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.GradientDescent.apply_backtracking","title":"<code>apply_backtracking(shrink=0.5)</code>","text":"<p>Apply backtracking by reducing step size and momentum temporarily.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.GradientDescent.apply_smc_update","title":"<code>apply_smc_update(control, gradient, **kwargs)</code>","text":"<p>Apply a gradient update to the control parameter.</p> <p>Parameters:</p> Name Type Description Default <code>control</code> <code>array_like</code> <p>The current value of the parameter being optimized.</p> required <code>gradient</code> <code>array_like</code> <p>The gradient of the objective function with respect to the control parameter.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>new_control</code> <code>ndarray</code> <p>The new value of the control parameter after the update.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.GradientDescent.apply_update","title":"<code>apply_update(control, gradient, **kwargs)</code>","text":"<p>Apply a gradient update to the control parameter.</p> <p>Note</p> <p>This is the steepest descent update: x_new = x_old - x_step.</p> <p>Parameters:</p> Name Type Description Default <code>control</code> <code>array_like</code> <p>The current value of the parameter being optimized.</p> required <code>gradient</code> <code>array_like</code> <p>The gradient of the objective function with respect to the control parameter.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>new_control, temp_velocity: tuple</code> <p>The new value of the control parameter after the update, and the current state step.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.GradientDescent.restore_parameters","title":"<code>restore_parameters()</code>","text":"<p>Restore the original step size and momentum value.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Steihaug","title":"<code>Steihaug</code>","text":"<p>A class implementing the Steihaug conjugate-gradient trust region optimizer. This code is based on the minfx optimisation library, https://gna.org/projects/minfx</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Steihaug.__init__","title":"<code>__init__(maxiter=1000000.0, epsilon=1e-08, delta_max=100000.0, delta0=1.0)</code>","text":"<p>Page 75 from 'Numerical Optimization' by Jorge Nocedal and Stephen J. Wright, 1999, 2<sup>nd</sup> ed.  The CG-Steihaug algorithm is:</p> <ul> <li>epsilon &gt; 0</li> <li>p0 = 0, r0 = g, d0 = -r0</li> <li>if ||r0|| &lt; epsilon:<ul> <li>return p = p0</li> </ul> </li> <li>while 1:<ul> <li>if djT.B.dj &lt;= 0:<ul> <li>Find tau such that p = pj + tau.dj minimises m(p) in (4.9) and satisfies ||p|| = delta</li> <li>return p</li> </ul> </li> <li>aj = rjT.rj / djT.B.dj</li> <li>pj+1 = pj + aj.dj</li> <li>if ||pj+1|| &gt;= delta:<ul> <li>Find tau such that p = pj + tau.dj satisfies ||p|| = delta</li> <li>return p</li> </ul> </li> <li>rj+1 = rj + aj.B.dj</li> <li>if ||rj+1|| &lt; epsilon.||r0||:<ul> <li>return p = pj+1</li> </ul> </li> <li>bj+1 = rj+1T.rj+1 / rjT.rj</li> <li>dj+1 = rj+1 + bj+1.dj</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>maxiter</code> <code>float</code> <p>Maximum number of iterations.</p> <code>1000000.0</code> <code>epsilon</code> <code>float</code> <p>Tolerance for iterations.</p> <code>1e-08</code> <code>delta_max</code> <code>float</code> <p>Maximum thrust region size.</p> <code>100000.0</code> <code>delta0</code> <code>float</code> <p>Initial thrust region size.</p> <code>1.0</code>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Steihaug.apply_backtracking","title":"<code>apply_backtracking()</code>","text":"<p>Apply backtracking by reducing step size temporarily.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Steihaug.apply_update","title":"<code>apply_update(xk, dfk, **kwargs)</code>","text":"<p>Apply a Steihaug update to the control vector.</p> <p>Parameters:</p> Name Type Description Default <code>xk</code> <code>array_like</code> <p>The current value of the parameter being optimized.</p> required <code>dfk</code> <code>array_like</code> <p>The gradient of the objective function with respect to the control parameter.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments, including the hessian of the objective function with respect to the control parameter.</p> <code>{}</code> <p>Returns:</p> Type Description <code>new_control, step: tuple</code> <p>The new value of the control parameter after the update, and the current state step.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Steihaug.get_tau","title":"<code>get_tau(pj, dj)</code>","text":"<p>Function to find tau such that p = pj + tau.dj, and ||p|| = delta.</p>"},{"location":"reference/popt/update_schemes/subroutines/optimizers/#popt.update_schemes.subroutines.optimizers.Steihaug.restore_parameters","title":"<code>restore_parameters()</code>","text":"<p>Restore the original step size.</p>"},{"location":"reference/simulator/","title":"simulator","text":"<p>Model wrappers.</p>"},{"location":"reference/simulator/#simulator--simulator-package","title":"Simulator package","text":"<p>This folder contains simple models intended for testing the PET code. For wrappers for Eclipse, OPM, seismic, and rock physics, and information about how to add wrappers for new simulators, see the repo: </p> <p>https://github.com/Python-Ensemble-Toolbox/SimulatorWrap</p>"},{"location":"reference/simulator/simple_models/","title":"simple_models","text":"<p>A collection of trivial toy models.</p>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.lin_1d","title":"<code>lin_1d</code>","text":"<p>linear 1x150 model (or whatever), just make observations of the state at given positions.</p>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.lin_1d.__init__","title":"<code>__init__(input_dict=None, m=None)</code>","text":"<p>Two inputs here. A dictionary of keys, or parameter directly.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>dict</code> <p>Dictionary containing all information required to run the simulator. It may come from, for example, an init file.</p> <code>None</code> <code>m</code> <code>int</code> <p>Parameter to make predicted data.</p> <code>None</code> Changelog <ul> <li>ST 7/9-15</li> </ul>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.nonlin_onedimmodel","title":"<code>nonlin_onedimmodel</code>","text":"<p>Class of simple 1D forward model for testing purposes.</p>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.nonlin_onedimmodel.__init__","title":"<code>__init__(input_dict=None)</code>","text":"<p>Two inputs here. A dictionary of keys, or parameter directly.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <p>contains all information the run the simulator (may come from, e.g., an init file)</p> <code>None</code>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.sevenmountains","title":"<code>sevenmountains</code>","text":"<p>The objective function is the elevations of the seven mountains around bergen, to test optimization algorithm</p>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.sevenmountains.__init__","title":"<code>__init__(input_dict=None, state=None)</code>","text":"<p>Two inputs here. A dictionary of keys, or parameter directly.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>dict</code> <p>contains all information the run the simulator (may come from, e.g., an init file)</p> <code>None</code> <code>state</code> <code>any</code> <p>Parameter to make predicted data</p> <code>None</code> Changelog <ul> <li>ST 9/5-18</li> </ul>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.sevenmountains.call_sim","title":"<code>call_sim(path=None)</code>","text":"<p>Run the simple 1D forward model</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Alternative folder where the MARE2DEM input files are located.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>d</code> <code>object</code> <p>Predicted data.</p> Changelog <ul> <li>ST 3/6-16</li> </ul>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.sevenmountains.check_sim_end","title":"<code>check_sim_end(current_run)</code>","text":"<p>Check if a simulation that has run in the background is finished. For ensemble-based methods, there can possibly be several folders with simulations, and each folder must be checked for finished runs. To check if simulation is done we search for .resp file which is the output in a successful  run.</p> <p>Parameters:</p> Name Type Description Default <code>current_run</code> <code>list</code> <p>List of ensemble members currently running simulation.</p> required <p>Returns:</p> Name Type Description <code>member</code> <code>int</code> <p>Ensemble member that is finished.</p> Changelog <ul> <li>ST 9/5-18</li> </ul>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.sevenmountains.get_sim_results","title":"<code>get_sim_results(which_resp, ext_data_info=None, member=None)</code>","text":"<p>Get forward simulation results. Simply load the numpy array...</p> <p>Parameters:</p> Name Type Description Default <code>which_resp</code> <code>str</code> <p>Specifies which of the responses is to be outputted (just one data type in this case).</p> required <code>member</code> <code>int</code> <p>Ensemble member that is finished.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>Array containing the predicted data (response).</p> Changelog <ul> <li>ST 3/6-16</li> </ul>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.sevenmountains.run_fwd_sim","title":"<code>run_fwd_sim(en_member=None, folder=os.getcwd(), wait_for_proc=False)</code>","text":"<p>Set up and run a forward simulation in an fwd_sim. The parameters for the forward simulation is set in setup_fwd_run. All the steps to set up and run a forward simulation is done in this object.</p> <p>Parameters:</p> Name Type Description Default <code>en_member</code> <code>int</code> <p>Index of the ensemble member to be run.</p> <code>None</code> <code>folder</code> <code>str</code> <p>Folder where the forward simulation is run.</p> <code>getcwd()</code> Changelog <ul> <li>ST 3/6-16</li> </ul>"},{"location":"reference/simulator/simple_models/#simulator.simple_models.sevenmountains.setup_fwd_run","title":"<code>setup_fwd_run(state, assim_ind=None, true_ind=None)</code>","text":"<p>Set input parameters from an fwd_sim in the simulation to get predicted data. Parameters can be an ensemble or a single array.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary of input parameter. It can be either a single 'state' or an ensemble of 'state'.</p> required <p>Other Parameters:</p> Name Type Description <code>true_ind</code> <code>list</code> <p>The list of observed data assimilation indices.</p> <code>assim_ind</code> <code>list</code> <p>List with information on assimilation order for ensemble-based methods.</p> Changelog <ul> <li>ST 3/6-16</li> </ul>"},{"location":"tutorials/","title":"Examples","text":"<p>Here are some tutorials.</p> <ul> <li><code>tutorial_pipt.ipynb</code>: Tutorial for running PIPT</li> <li><code>tutorial_pipt.ipynb</code>: Tutorial for running POPT</li> </ul>"},{"location":"tutorials/pipt/tutorial_pipt/","title":"Tutorial for running the Python Inverse Problem Toolbox (PIPT)","text":"In\u00a0[1]: Copied! <pre># Set width\nfrom IPython.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.container { width:50% !important; }&lt;/style&gt;\"))\n\n# Import global modules\nimport numpy as np\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt  \n\n# Import local modules\nfrom pipt.loop.ensemble import Ensemble # this class contains the data\nfrom pipt.loop.assimilation import Assimilate # this class contains the iterative assimilation loop\nfrom subsurface.multphaseflow.opm import flow # the simulator we want to use\nfrom input_output import read_config # functions for reading input\nfrom pipt import pipt_init # script for initializing the module with the data assimilation method \nfrom plot_objective_function import combined # plot the data mismatch\nfrom plot_parameters import plot_layer, export_to_grid # plot the parameters\nfrom plot_data import plot_prod # plot the production data\n</pre> # Set width from IPython.display import display, HTML display(HTML(\"\"))  # Import global modules import numpy as np from glob import glob import os import matplotlib.pyplot as plt    # Import local modules from pipt.loop.ensemble import Ensemble # this class contains the data from pipt.loop.assimilation import Assimilate # this class contains the iterative assimilation loop from subsurface.multphaseflow.opm import flow # the simulator we want to use from input_output import read_config # functions for reading input from pipt import pipt_init # script for initializing the module with the data assimilation method  from plot_objective_function import combined # plot the data mismatch from plot_parameters import plot_layer, export_to_grid # plot the parameters from plot_data import plot_prod # plot the production data <p>Set the random seed:</p> In\u00a0[2]: Copied! <pre>np.random.seed(10)\n</pre> np.random.seed(10)     <p>Remove old results and folders, if present:</p> In\u00a0[3]: Copied! <pre>for folder in glob('En_*'):\n    shutil.rmtree(folder)\nfor file in glob('debug_analysis_step_*'):\n    os.remove(file)\n</pre> for folder in glob('En_*'):     shutil.rmtree(folder) for file in glob('debug_analysis_step_*'):     os.remove(file) <p>Read inputfile. In this tutorial the input file is written as a .toml file, and consists of two main keys: dataassim and fwdsim. The first part contains the options for the data assimilation algorithm and the second part are options related to the forward simulation model. The description of all keys are provided in the printouts of method docstrings below.</p> In\u00a0[4]: Copied! <pre>!cat 3D_ESMDA.toml\nkd, kf, ke = read_config.read_toml('3D_ESMDA.toml')\n</pre> !cat 3D_ESMDA.toml kd, kf, ke = read_config.read_toml('3D_ESMDA.toml') <pre>[ensemble]\r\nne = 50.0\r\nstate = \"permx\"\r\nprior_permx = [[\"vario\", \"sph\"], [\"mean\", \"priormean.npz\"], [\"var\", 1.0], [\"range\", 10.0], [\"aniso\", 1.0],\r\n               [\"angle\", 0.0], [\"grid\", [10.0, 10.0, 2.0]]]\r\n               \r\n[dataassim]\r\ndaalg = [\"esmda\", \"esmda\"]\r\nanalysis = \"approx\"\r\nenergy = 98.0\r\nobsvarsave = \"yes\"\r\nrestartsave = \"no\"\r\nanalysisdebug = [\"pred_data\", \"state\", \"data_misfit\", \"prev_data_misfit\"]\r\nrestart = \"no\"\r\nobsname = \"days\"\r\ntruedataindex = [400, 800, 1200, 1600, 2000, 2400, 2800, 3200, 3600, 4000]\r\ntruedata = \"true_data.csv\"\r\nassimindex = [0,1,2,3,4,5,6,7,8,9]\r\ndatatype = [\"WOPR PRO1\", \"WOPR PRO2\", \"WOPR PRO3\", \"WWPR PRO1\", \"WWPR PRO2\",\r\n            \"WWPR PRO3\", \"WWIR INJ1\", \"WWIR INJ2\", \"WWIR INJ3\"]\r\nstaticvar = \"permx\"\r\ndatavar = \"var.csv\"\r\nmda = [ [\"tot_assim_steps\", 3], ['inflation_param', [2, 4, 4]] ]\r\n\r\n[fwdsim]\r\nreporttype = \"days\"\r\nreportpoint = [400, 800, 1200, 1600, 2000, 2400, 2800, 3200, 3600, 4000]\r\nreplace = \"yes\"\r\nsaveforecast = \"yes\"\r\nsim_limit = 300.0\r\nrerun = 1\r\nrunfile = \"runfile\"\r\ndatatype = [\"WOPR PRO1\", \"WOPR PRO2\", \"WOPR PRO3\", \"WWPR PRO1\", \"WWPR PRO2\",\r\n            \"WWPR PRO3\", \"WWIR INJ1\", \"WWIR INJ2\", \"WWIR INJ3\"]\r\nparallel = 4\r\nstartdate = \"1/1/2022\"\r\n</pre> <p>Initialize the simulator with simulator keys.</p> In\u00a0[5]: Copied! <pre>sim = flow(kf)\nprint(flow.__init__.__doc__)\n</pre> sim = flow(kf) print(flow.__init__.__doc__) <pre>\n        The inputs are all optional, but in the same fashion as the other simulators a system must be followed.\n        The input_dict can be utilized as a single input. Here all nescessary info is stored. Alternatively,\n        if input_dict is not defined, all the other input variables must be defined.\n\n        Parameters\n        ----------\n        input_dict : dict, optional\n            Dictionary containing all information required to run the simulator.\n\n                - parallel: number of forward simulations run in parallel\n                - simoptions: options for the simulations\n                    - mpi: option to use mpi (always use &gt; 2 cores)\n                    - sim_path: Path to the simulator\n                    - sim_flag: Flags sent to the simulator (see simulator documentation for all possibilities)\n                - sim_limit: maximum number of seconds a simulation can run before being killed\n                - runfile: name of the simulation input file\n                - reportpoint: these are the dates the simulator reports results\n                - reporttype: this key states that the report poins are given as dates\n                - datatype: the data types the simulator reports\n\n        filename : str, optional\n            Name of the .mako file utilized to generate the ECL input .DATA file. Must be in uppercase for the\n            ECL simulator.\n\n        options : dict, optional\n            Dictionary with options for the simulator.\n\n        Returns\n        -------\n        initial_object : object\n            Initial object from the class ecl_100.\n        \n</pre> <p>Print the Ensemble options:</p> In\u00a0[6]: Copied! <pre>print(Ensemble.__init__.__doc__)\n</pre> print(Ensemble.__init__.__doc__) <pre>\n        Parameters\n        ----------\n        keys_da : dict\n            Options for the data assimilation class\n\n            - daalg: spesification of the method, first the main type (e.g., \"enrml\"), then the solver (e.g., \"gnenrml\")\n            - analysis: update flavour (\"approx\", \"full\" or \"subspace\")\n            - energy: percent of singular values kept after SVD\n            - obsvarsave: save the observations as a file (default false)\n            - restart: restart optimization from a restart file (default false)\n            - restartsave: save a restart file after each successful iteration (defalut false)\n            - analysisdebug: specify which class variables to save to the result files\n            - truedataindex: order of the simulated data (for timeseries this is points in time)\n            - obsname: unit for truedataindex (for timeseries this is days or hours or seconds, etc.)\n            - truedata: the data, e.g., provided as a .csv file\n            - assimindex: index for the data that will be used for assimilation\n            - datatype: list with the name of the datatypes\n            - staticvar: name of the static variables\n            - datavar: data variance, e.g., provided as a .csv file\n\n        keys_en : dict\n            Options for the ensemble class\n\n            - ne: number of perturbations used to compute the gradient\n            - state: name of state variables passed to the .mako file\n            - prior_&lt;name&gt;: the prior information the state variables, including mean, variance and variable limits\n\n        sim : callable\n            The forward simulator (e.g. flow)\n        \n</pre> <p>Example using ESMDA. The input and available options are given below. During assimilation, useful information is written to the screen. The same information is also written to a log-file named pet_logger.log.</p> In\u00a0[7]: Copied! <pre>analysis = pipt_init.init_da(kd, ke, sim)\n\nprint(analysis.__init__.__doc__)\nassimilation = Assimilate(analysis)\nassimilation.run()\n</pre> analysis = pipt_init.init_da(kd, ke, sim)  print(analysis.__init__.__doc__) assimilation = Assimilate(analysis) assimilation.run() <pre>Single entry for VARIO will be copied to all 2 layers\nSingle entry for VAR will be copied to all 2 layers\nSingle entry for ANISO will be copied to all 2 layers\nSingle entry for ANGLE will be copied to all 2 layers\nSingle entry for RANGE will be copied to all 2 layers\n\n        The class is initialized by passing the keywords and simulator object upwards in the hierarchy.\n\n        Parameters\n        ----------\n        keys_da['mda']: list\n            - tot_assim_steps: total number of iterations in MDA, e.g., 3\n            - inflation_param: covariance inflation factors, e.g., [2, 4, 4]\n\n        keys_en : dict\n\n        sim : callable\n</pre> <pre>\rIterations (Obj. func. val: ):   0%|                                                                                                                                                                                             | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/50 [00:00&lt;?, ?it/s]</pre> <pre>\rIterations (Obj. func. val: ):  25%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                       | 1/4 [00:06&lt;00:19,  6.58s/it]</pre> <pre>  0%|          | 0/50 [00:00&lt;?, ?it/s]</pre> <pre>Iterations (Obj. func. val:749926.2 Reduced: 95 %):  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                | 2/4 [00:13&lt;00:13,  6.71s/it]</pre> <pre>  0%|          | 0/50 [00:00&lt;?, ?it/s]</pre> <pre>Iterations (Obj. func. val:163815.9 Reduced: 78 %):  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                        | 3/4 [00:20&lt;00:06,  6.88s/it]</pre> <pre>  0%|          | 0/50 [00:00&lt;?, ?it/s]</pre> <pre>Iterations (Obj. func. val:14396.2 Reduced: 91 %): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:27&lt;00:00,  6.76s/it]</pre> <pre>Convergence was met. Obj. function reduced from 14994770.0 to 14396.2\n</pre> <pre>\n</pre> <p>Plot the data mismatch:</p> In\u00a0[8]: Copied! <pre>combined()\n</pre> combined() <p>Plot the prior and posterior permeability in the upper layer:</p> In\u00a0[9]: Copied! <pre>export_to_grid('permx')\nplot_layer('permx', [2, 10, 10])\n</pre> export_to_grid('permx') plot_layer('permx', [2, 10, 10]) In\u00a0[10]: Copied! <pre>plot_prod()\n</pre> plot_prod() <pre>WOPR PRO1\n</pre> <pre>WOPR PRO2\n</pre> <pre>WOPR PRO3\n</pre> <pre>WWPR PRO1\n</pre> <pre>WWPR PRO2\n</pre> <pre>WWPR PRO3\n</pre> <pre>WWIR INJ1\n</pre> <pre>WWIR INJ2\n</pre> <pre>WWIR INJ3\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/pipt/tutorial_pipt/#tutorial-for-running-the-python-inverse-problem-toolbox-pipt","title":"Tutorial for running the Python Inverse Problem Toolbox (PIPT)\u00b6","text":"<p>As an illustrative example we choose a small 3D-field with three producers and three (water) injectors. The figure below shows the true (data generating) permeability field and the well positions. The grid is 10x10x2, and the porosity is 0.2. The inverse problem is to find the permeability for the reservoir by assimilation produced water and oil and injected water.</p> <p> The first step is to load neccessary external and local modules.</p>"},{"location":"tutorials/pipt/tutorial_pipt/#setting-up-the-mako-file","title":"Setting up the .mako file\u00b6","text":"<p>The data assimilation relies on a .mako file for writing the current state variables to the flow simulator input. In this case, the flow simulator is opm-flow opm-projects.org, and the input file is provided as a text file (.DATA file). The .mako file is created by replacing the keywords PERMX in the .DATA file with:</p> <pre><code>PERMX\n% for i in range(0, len(permx)):\n% if permx[i] &lt; 6:\n${\"%.3f\" %(np.exp(permx[i]))}\n% else:\n${\"%.3f\" %(np.exp(6))}\n% endif\n% endfor\n/</code></pre>"},{"location":"tutorials/pipt/tutorial_pipt/#running-locally","title":"Running locally\u00b6","text":"<p>It is recommended to run the notebook from a virtual environment. Follow these steps to run this notebook on your own computer:</p> <p>Step 1: Create virtual environment as normal</p> <pre><code>python3 -m venv pet_venv</code></pre> <p>Then activate the environment using:</p> <pre><code>source pet_venv/bin/activate</code></pre> <p>Step 2: Install Jupyter Notebook into virtual environment</p> <pre><code>python3 -m pip install ipykernel</code></pre> <p>Step 3: Install PET in the virtual environment, see PET installation</p> <p>Step 4: Install Plotting in the virtual environment, see Plotting installation</p> <p>Step 5: Allow Jupyter access to the kernel within the virtual environment</p> <pre><code>python3 -m ipykernel install --user --name=pet_venv</code></pre> <p>Start jupyter notebook, and load tutorial_popt.ipynb (this file). On the jupyter notebook toolbar, select \u2018Kernel\u2019 and \u2018Change Kernel\u2019. The new kernel is now be available in the list for selection:</p> <p></p>"},{"location":"tutorials/popt/tutorial_popt/","title":"Tutorial for running the Python Optimization Toolbox (POPT)","text":"In\u00a0[10]: Copied! <pre># Set width\nfrom IPython.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.container { width:50% !important; }&lt;/style&gt;\"))\n\n# Import global modules\nimport os\nimport glob\nimport shutil\nimport logging\nfrom glob import glob\nfrom copy import deepcopy\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt  \nfrom scipy.stats import expon\n\n# Import local modules\nfrom input_output import read_config # functions for reading input\nfrom popt.misc_tools import optim_tools as ot # help functions for optimization\nfrom popt.loop.optimize import Optimize # this class contains the iterative loop\nfrom popt.loop.ensemble import Ensemble # this class contains the control pertrubations and gradient calculations\nfrom simulator.opm import flow # the simulator we want to use\nfrom popt.update_schemes.enopt import EnOpt # the standard EnOpt method\nfrom popt.update_schemes.smcopt import SmcOpt # the sequential Monte Carlo method\nfrom popt.cost_functions.npv import npv # the cost function\n</pre> # Set width from IPython.display import display, HTML display(HTML(\"\"))  # Import global modules import os import glob import shutil import logging from glob import glob from copy import deepcopy import numpy as np from scipy.optimize import minimize import matplotlib.pyplot as plt   from scipy.stats import expon  # Import local modules from input_output import read_config # functions for reading input from popt.misc_tools import optim_tools as ot # help functions for optimization from popt.loop.optimize import Optimize # this class contains the iterative loop from popt.loop.ensemble import Ensemble # this class contains the control pertrubations and gradient calculations from simulator.opm import flow # the simulator we want to use from popt.update_schemes.enopt import EnOpt # the standard EnOpt method from popt.update_schemes.smcopt import SmcOpt # the sequential Monte Carlo method from popt.cost_functions.npv import npv # the cost function <p>Set the random seed:</p> In\u00a0[11]: Copied! <pre>np.random.seed(101122)\n</pre> np.random.seed(101122)     <p>The plottting function is used to display the objective function vs. iterations for different methods:</p> In\u00a0[12]: Copied! <pre>def plot_obj_func():\n    \n    # Collect all results\n    path_to_files = './'\n    path_to_figures = './'  # Save here\n    if not os.path.exists(path_to_figures):\n        os.mkdir(path_to_figures)\n    files = os.listdir(path_to_files)\n    results = [name for name in files if \"optimize_result\" in name]\n    num_iter = len(results)\n\n    mm = []\n    for iter in range(num_iter):\n        info = np.load(str(path_to_files) + 'optimize_result_{}.npz'.format(iter))\n        if 'best_func' in info:\n            mm.append(-info['best_func'])\n        else:\n            mm.append(-info['obj_func_values'])\n\n    f = plt.figure()\n    plt.plot(mm, 'bs-')\n    plt.xticks(range(num_iter))\n    plt.xticks(fontsize = 14)\n    plt.yticks(fontsize = 14)\n    plt.xlabel('Iteration no.', size=14)\n    plt.ylabel('NPV', size=14)\n    plt.title('Objective function', size=14)\n    f.tight_layout(pad=2.0)\n    plt.show()\n</pre> def plot_obj_func():          # Collect all results     path_to_files = './'     path_to_figures = './'  # Save here     if not os.path.exists(path_to_figures):         os.mkdir(path_to_figures)     files = os.listdir(path_to_files)     results = [name for name in files if \"optimize_result\" in name]     num_iter = len(results)      mm = []     for iter in range(num_iter):         info = np.load(str(path_to_files) + 'optimize_result_{}.npz'.format(iter))         if 'best_func' in info:             mm.append(-info['best_func'])         else:             mm.append(-info['obj_func_values'])      f = plt.figure()     plt.plot(mm, 'bs-')     plt.xticks(range(num_iter))     plt.xticks(fontsize = 14)     plt.yticks(fontsize = 14)     plt.xlabel('Iteration no.', size=14)     plt.ylabel('NPV', size=14)     plt.title('Objective function', size=14)     f.tight_layout(pad=2.0)     plt.show() <p>Remove old results and folders, if present:</p> In\u00a0[13]: Copied! <pre>for folder in glob('En_*'):\n    shutil.rmtree(folder)\nfor file in glob('optimize_result_*'):\n    os.remove(file)\n</pre> for folder in glob('En_*'):     shutil.rmtree(folder) for file in glob('optimize_result_*'):     os.remove(file) <p>Read inputfile. In this tutorial the input file is written as a .toml file, and consists of three main keys: ensemble, optim and fwdsim. The first part contains keys related to the ensemble, the second part contains the options for the optimization algorithm and the third part are options related to the forward simulation model and objective function. The description of all keys are provided in the printouts of method docstrings below.</p> In\u00a0[14]: Copied! <pre>!cat init_optim.toml\nko, kf, ke = read_config.read_toml('init_optim.toml')\n</pre> !cat init_optim.toml ko, kf, ke = read_config.read_toml('init_optim.toml') <pre>[ensemble]\r\ndisable_tqdm = true\r\nne = 10\r\nstate = [\"injbhp\",\"prodbhp\"]\r\nprior_injbhp = [\r\n    [\"mean\",\"init_injbhp.npz\"],\r\n    [\"var\",6250.0],\r\n    [\"limits\",100.0,500.0]\r\n]\r\nprior_prodbhp = [\r\n    [\"mean\",\"init_prodbhp.npz\"],\r\n    [\"var\",6250.0,],\r\n    [\"limits\",20.0,300.0]\r\n]\r\nnum_models = 1\r\ntransform = true\r\n\r\n[optim]\r\nmaxiter = 5\r\ntol = 1e-06\r\nalpha = 0.2\r\nbeta = 0.1\r\nalpha_maxiter = 3\r\nresample = 0\r\noptimizer = 'GA'\r\nnesterov = true\r\nrestartsave = true\r\nrestart = false\r\nhessian = false\r\ninflation_factor = 10\r\nsavedata = [\"alpha\",\"obj_func_values\"]\r\n\r\n[fwdsim]\r\nnpv_const = [\r\n    [\"wop\",283.05],\r\n    [\"wgp\",0.0],\r\n    [\"wwp\",37.74],\r\n    [\"wwi\",12.58],\r\n    [\"disc\",0.08],\r\n    [\"obj_scaling\",-1.0e6]\r\n]\r\nparallel = 2\r\nsimoptions = [\r\n    ['mpi', 'mpirun -np 3'],\r\n    ['sim_path', '/usr/bin/'],\r\n    ['sim_flag', '--tolerance-mb=1e-5 --parsing-strictness=low']\r\n]\r\nsim_limit = 5.0\r\nrunfile = \"3well\"\r\nreportpoint = [\r\n    1994-02-09 00:00:00,\r\n    1995-01-01 00:00:00,\r\n    1996-01-01 00:00:00,\r\n    1997-01-01 00:00:00,\r\n    1998-01-01 00:00:00,\r\n    1999-01-01 00:00:00,\r\n]\r\nreporttype = \"dates\"\r\ndatatype = [\"fopt\",\"fgpt\",\"fwpt\",\"fwit\"]\r\n</pre> <p>Set initial pressure (note that the filenames correspond to the names given in the input file above):</p> In\u00a0[15]: Copied! <pre>init_injbhp = np.array([300.0,250.0])\ninit_prodbhp = np.array([100.0])\nnp.savez('init_injbhp.npz', init_injbhp)\nnp.savez('init_prodbhp.npz', init_prodbhp)\n</pre> init_injbhp = np.array([300.0,250.0]) init_prodbhp = np.array([100.0]) np.savez('init_injbhp.npz', init_injbhp) np.savez('init_prodbhp.npz', init_prodbhp) <p>Initialize the simulator with simulator keys.</p> In\u00a0[16]: Copied! <pre>sim = flow(kf)\nprint(flow.__init__.__doc__)\n</pre> sim = flow(kf) print(flow.__init__.__doc__) <pre>\n        The inputs are all optional, but in the same fashion as the other simulators a system must be followed.\n        The input_dict can be utilized as a single input. Here all nescessary info is stored. Alternatively,\n        if input_dict is not defined, all the other input variables must be defined.\n\n        Parameters\n        ----------\n        input_dict : dict, optional\n            Dictionary containing all information required to run the simulator.\n\n                - parallel: number of forward simulations run in parallel\n                - simoptions: options for the simulations\n                    - mpi: option to use mpi (always use &gt; 2 cores)\n                    - sim_path: Path to the simulator\n                    - sim_flag: Flags sent to the simulator (see simulator documentation for all possibilities)\n                - sim_limit: maximum number of seconds a simulation can run before being killed\n                - runfile: name of the simulation input file\n                - reportpoint: these are the dates the simulator reports results\n                - reporttype: this key states that the report poins are given as dates\n                - datatype: the data types the simulator reports\n\n        filename : str, optional\n            Name of the .mako file utilized to generate the ECL input .DATA file. Must be in uppercase for the\n            ECL simulator.\n\n        options : dict, optional\n            Dictionary with options for the simulator.\n\n        Returns\n        -------\n        initial_object : object\n            Initial object from the class ecl_100.\n        \n</pre> <p>Initialize the ensemble with ensemble keys, the simulator, and the chosen objective function. Here we also extract the initial state (x0), the covariance (cov), and the bounds.</p> In\u00a0[17]: Copied! <pre>ensemble = Ensemble(ke, sim, npv)\nprint(Ensemble.__init__.__doc__)\nx0 = ensemble.get_state()\ncov = ensemble.get_cov()\nbounds = ensemble.get_bounds()\n</pre> ensemble = Ensemble(ke, sim, npv) print(Ensemble.__init__.__doc__) x0 = ensemble.get_state() cov = ensemble.get_cov() bounds = ensemble.get_bounds() <pre>\n        Parameters\n        ----------\n        keys_en : dict\n            Options for the ensemble class\n\n            - disable_tqdm: supress tqdm progress bar for clean output in the notebook\n            - ne: number of perturbations used to compute the gradient\n            - state: name of state variables passed to the .mako file\n            - prior_&lt;name&gt;: the prior information the state variables, including mean, variance and variable limits\n            - num_models: number of models (if robust optimization) (default 1)\n            - transform: transform variables to [0,1] if true (default true)\n\n        sim : callable\n            The forward simulator (e.g. flow)\n\n        obj_func : callable\n            The objective function (e.g. npv)\n        \n</pre> <p>Example using EnOpt. The input and available options are given below. During optimization, useful information is written to the screen. The same information is also written to a log-file named popt.log.</p> In\u00a0[18]: Copied! <pre>print(EnOpt.__init__.__doc__)\nEnOpt(ensemble.function, x0, args=(cov,), jac=ensemble.gradient, hess=ensemble.hessian, bounds=bounds, **ko)\n</pre> print(EnOpt.__init__.__doc__) EnOpt(ensemble.function, x0, args=(cov,), jac=ensemble.gradient, hess=ensemble.hessian, bounds=bounds, **ko) <pre>\n        Parameters\n        ----------\n            fun: callable\n                objective function\n\n            x: ndarray\n                Initial state\n\n            args: tuple\n                Initial covariance\n\n            jac: callable\n                Gradient function\n\n            hess: callable\n                Hessian function\n\n            bounds: list, optional\n                (min, max) pairs for each element in x. None is used to specify no bound.\n\n            options: dict\n                Optimization options\n\n                    - maxiter: maximum number of iterations (default 10)\n                    - restart: restart optimization from a restart file (default false)\n                    - restartsave: save a restart file after each successful iteration (defalut false)\n                    - tol: convergence tolerance for the objective function (default 1e-6)\n                    - alpha: step size for the steepest descent method (default 0.1)\n                    - beta: momentum coefficient for running accelerated optimization (default 0.0)\n                    - alpha_maxiter: maximum number of backtracing trials (default 5)\n                    - resample: number indicating how many times resampling is tried if no improvement is found\n                    - optimizer: 'GA' (gradient accent) or Adam (default 'GA')\n                    - nesterov: use Nesterov acceleration if true (default false)\n                    - hessian: use Hessian approximation (if the algorithm permits use of Hessian) (default false)\n                    - normalize: normalize the gradient if true (default true)\n                    - cov_factor: factor used to shrink the covariance for each resampling trial (defalut 0.5)\n                    - savedata: specify which class variables to save to the result files (state, objective\n                                function value, iteration number, number of function evaluations, and number\n                                of gradient evaluations, are always saved)\n        \n</pre> <pre>2023-12-14 10:16:03,832 : INFO : popt.loop.optimize :        ====== Running optimization - EnOpt ======\n2023-12-14 10:16:03,833 : INFO : popt.loop.optimize : \n{'alpha': 0.2,\n 'alpha_maxiter': 3,\n 'beta': 0.1,\n 'datatype': ['fopt', 'fgpt', 'fwpt', 'fwit'],\n 'hessian': False,\n 'inflation_factor': 10,\n 'maxiter': 5,\n 'nesterov': True,\n 'optimizer': 'GA',\n 'resample': 0,\n 'restart': False,\n 'restartsave': True,\n 'savedata': ['alpha', 'obj_func_values'],\n 'tol': 1e-06}\n2023-12-14 10:16:03,833 : INFO : popt.loop.optimize :        iter       alpha_iter obj_func        step-size       cov[0,0]        \n2023-12-14 10:16:03,834 : INFO : popt.loop.optimize :        0                     -1.9083e-01    \n2023-12-14 10:16:13,088 : INFO : popt.loop.optimize :        1          0          -3.1499e-01     2.00e-01        3.97e-02       \n2023-12-14 10:16:21,543 : INFO : popt.loop.optimize :        2          0          -3.7002e-01     2.00e-01        3.97e-02       \n2023-12-14 10:16:33,583 : INFO : popt.loop.optimize :        3          0          -3.7252e-01     2.00e-01        3.95e-02       \n2023-12-14 10:16:43,758 : INFO : popt.loop.optimize :        4          0          -3.7253e-01     2.00e-01        3.93e-02       \n2023-12-14 10:16:53,204 : INFO : popt.loop.optimize :        5          0          -3.7260e-01     2.00e-01        3.97e-02       \n2023-12-14 10:16:53,211 : INFO : popt.loop.optimize :        Optimization converged in 5 iterations \n2023-12-14 10:16:53,213 : INFO : popt.loop.optimize :        Optimization converged with final obj_func = -0.3726\n2023-12-14 10:16:53,216 : INFO : popt.loop.optimize :        Total number of function evaluations = 6\n2023-12-14 10:16:53,218 : INFO : popt.loop.optimize :        Total number of jacobi evaluations = 5\n2023-12-14 10:16:53,221 : INFO : popt.loop.optimize :        Total elapsed time = 0.84 minutes\n2023-12-14 10:16:53,227 : INFO : popt.loop.optimize :        ============================================\n</pre> Out[18]: <pre>&lt;popt.update_schemes.enopt.EnOpt at 0x7f44d18aae80&gt;</pre> <p>Finally, plot the objective function using the function defined above:</p> In\u00a0[19]: Copied! <pre>plot_obj_func()\n</pre> plot_obj_func() <p>Example using the sequential Monte Carlo method. Here we also save the best state (which is not the mean of the ensemble simulations) and the corresponding best objective function value:</p> In\u00a0[20]: Copied! <pre>ko_smc = deepcopy(ko)\nko_smc['savedata'] += [\"best_state\", \"best_func\"]\nprint(SmcOpt.__init__.__doc__)\nSmcOpt(ensemble.function, x0, args=(cov,), sens=ensemble.calc_ensemble_weights, bounds=bounds, **ko_smc)\n</pre> ko_smc = deepcopy(ko) ko_smc['savedata'] += [\"best_state\", \"best_func\"] print(SmcOpt.__init__.__doc__) SmcOpt(ensemble.function, x0, args=(cov,), sens=ensemble.calc_ensemble_weights, bounds=bounds, **ko_smc) <pre>\n        Parameters\n        ----------\n            fun: callable\n                objective function\n\n            x: ndarray\n                Initial state\n\n            sens: callable\n                Ensemble sensitivity\n\n            bounds: list, optional\n                (min, max) pairs for each element in x. None is used to specify no bound.\n\n            options: dict\n                Optimization options\n\n                - maxiter: maximum number of iterations (default 10)\n                - restart: restart optimization from a restart file (default false)\n                - restartsave: save a restart file after each successful iteration (defalut false)\n                - tol: convergence tolerance for the objective function (default 1e-6)\n                - alpha: weight between previous and new step (default 0.1)\n                - alpha_maxiter: maximum number of backtracing trials (default 5)\n                - resample: number indicating how many times resampling is tried if no improvement is found\n                - cov_factor: factor used to shrink the covariance for each resampling trial (defalut 0.5)\n                - inflation_factor: term used to weight down prior influence (defalult 1)\n                - savedata: specify which class variables to save to the result files (state, objective function\n                            value, iteration number, number of function evaluations, and number of gradient\n                            evaluations, are always saved)\n        \n</pre> <pre>2023-12-14 10:17:21,181 : INFO : popt.loop.optimize :        ====== Running optimization - SmcOpt ======\n2023-12-14 10:17:21,182 : INFO : popt.loop.optimize : \n{'alpha': 0.2,\n 'alpha_maxiter': 3,\n 'beta': 0.1,\n 'datatype': ['fopt', 'fgpt', 'fwpt', 'fwit'],\n 'hessian': False,\n 'inflation_factor': 10,\n 'maxiter': 5,\n 'nesterov': True,\n 'optimizer': 'GA',\n 'resample': 0,\n 'restart': False,\n 'restartsave': True,\n 'savedata': ['alpha', 'obj_func_values', 'best_state', 'best_func'],\n 'tol': 1e-06}\n2023-12-14 10:17:21,182 : INFO : popt.loop.optimize :        iter       alpha_iter obj_func        step-size       \n2023-12-14 10:17:21,183 : INFO : popt.loop.optimize :        0                     -1.9083e-01    \n2023-12-14 10:17:30,338 : INFO : popt.loop.optimize :        1          0          -3.6047e-01     2.00e-01       \n2023-12-14 10:17:39,446 : INFO : popt.loop.optimize :        2          0          -3.7190e-01     2.00e-01       \n2023-12-14 10:17:49,220 : INFO : popt.loop.optimize :        3          0          -3.7443e-01     2.00e-01       \n2023-12-14 10:17:58,141 : INFO : popt.loop.optimize :        4          0          -3.7443e-01     2.00e-01       \n2023-12-14 10:18:10,110 : INFO : popt.loop.optimize :        5          0          -3.7443e-01     2.00e-01       \n2023-12-14 10:18:10,112 : INFO : popt.loop.optimize :        Optimization converged in 5 iterations \n2023-12-14 10:18:10,113 : INFO : popt.loop.optimize :        Optimization converged with final obj_func = -0.3672\n2023-12-14 10:18:10,113 : INFO : popt.loop.optimize :        Total number of function evaluations = 6\n2023-12-14 10:18:10,114 : INFO : popt.loop.optimize :        Total number of jacobi evaluations = 5\n2023-12-14 10:18:10,114 : INFO : popt.loop.optimize :        Total elapsed time = 0.83 minutes\n2023-12-14 10:18:10,114 : INFO : popt.loop.optimize :        ============================================\n</pre> Out[20]: <pre>&lt;popt.update_schemes.smcopt.SmcOpt at 0x7f453c9638b0&gt;</pre> <p>Plot the objective function using the function defined above:</p> In\u00a0[21]: Copied! <pre>plot_obj_func()\n</pre> plot_obj_func() <p>Example using ensemble gradient approximation with the conjugate gradient (CG) method from scipy.minimize:</p> In\u00a0[22]: Copied! <pre>res = minimize(ensemble.function, x0, args=(cov,), method='CG', jac=ensemble.gradient, tol=ko['tol'],\n               callback=ot.save_optimize_results, bounds=bounds, options=ko)\nprint(res)\n</pre> res = minimize(ensemble.function, x0, args=(cov,), method='CG', jac=ensemble.gradient, tol=ko['tol'],                callback=ot.save_optimize_results, bounds=bounds, options=ko) print(res) <pre> message: Maximum number of iterations has been exceeded.\n success: False\n  status: 1\n     fun: -0.37443170946723164\n       x: [-1.023e-01 -1.007e-01 -2.088e-01]\n     nit: 5\n     jac: [ 0.000e+00  2.207e-06  0.000e+00]\n    nfev: 21\n    njev: 21\n</pre> <p>Example calling EnOpt through scipy.minimize (this does exactly the same as running EnOpt, but with a different random seed since we do not reset the seed):</p> In\u00a0[23]: Copied! <pre>for file in glob('optimize_result_*'):\n    os.remove(file)\nminimize(ensemble.function, x0, args=(cov,), method=EnOpt, jac=ensemble.gradient, hess=ensemble.hessian,\n         bounds=bounds, options=ko)\n</pre> for file in glob('optimize_result_*'):     os.remove(file) minimize(ensemble.function, x0, args=(cov,), method=EnOpt, jac=ensemble.gradient, hess=ensemble.hessian,          bounds=bounds, options=ko) <pre>2023-12-14 10:22:29,796 : INFO : popt.loop.optimize :        ====== Running optimization - EnOpt ======\n2023-12-14 10:22:29,797 : INFO : popt.loop.optimize : \n{'alpha': 0.2,\n 'alpha_maxiter': 3,\n 'beta': 0.1,\n 'callback': None,\n 'constraints': (),\n 'datatype': ['fopt', 'fgpt', 'fwpt', 'fwit'],\n 'hessian': False,\n 'hessp': None,\n 'inflation_factor': 10,\n 'maxiter': 5,\n 'nesterov': True,\n 'optimizer': 'GA',\n 'resample': 0,\n 'restart': False,\n 'restartsave': True,\n 'savedata': ['alpha', 'obj_func_values'],\n 'tol': 1e-06}\n2023-12-14 10:22:29,797 : INFO : popt.loop.optimize :        iter       alpha_iter obj_func        step-size       cov[0,0]        \n2023-12-14 10:22:29,798 : INFO : popt.loop.optimize :        0                     -1.9083e-01    \n2023-12-14 10:22:38,680 : INFO : popt.loop.optimize :        1          0          -3.0921e-01     2.00e-01        3.90e-02       \n2023-12-14 10:22:47,943 : INFO : popt.loop.optimize :        2          0          -3.6664e-01     2.00e-01        3.90e-02       \n2023-12-14 10:23:00,146 : INFO : popt.loop.optimize :        Optimization converged in 2 iterations \n2023-12-14 10:23:00,147 : INFO : popt.loop.optimize :        Optimization converged with final obj_func = -0.3666\n2023-12-14 10:23:00,147 : INFO : popt.loop.optimize :        Total number of function evaluations = 3\n2023-12-14 10:23:00,147 : INFO : popt.loop.optimize :        Total number of jacobi evaluations = 2\n2023-12-14 10:23:00,148 : INFO : popt.loop.optimize :        Total elapsed time = 0.52 minutes\n2023-12-14 10:23:00,148 : INFO : popt.loop.optimize :        ============================================\n</pre> Out[23]: <pre>&lt;popt.update_schemes.enopt.EnOpt at 0x7f453c96d310&gt;</pre> <p>Plot the objective function using the function defined above:</p> In\u00a0[24]: Copied! <pre>plot_obj_func()\n</pre> plot_obj_func() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/popt/tutorial_popt/#tutorial-for-running-the-python-optimization-toolbox-popt","title":"Tutorial for running the Python Optimization Toolbox (POPT)\u00b6","text":"<p>As an illustrative example we choose a 2D-field with one producer and two (water) injectors. The figure below shows the permeability field and the well positions. The grid is 100x100, and the porosity is 0.18. The optimization problem is to find the pressure control for the wells in order to maximize net present value.</p> <p></p> The first step is to load neccessary external and local modules."},{"location":"tutorials/popt/tutorial_popt/#setting-up-the-mako-file","title":"Setting up the .mako file\u00b6","text":"<p>The optimization relies on a .mako file for writing the current control variables to the flow simulator input. In this case, the flow simulator is opm-flow opm-projects.org, and the input file is provided as a text file (.DATA file). The .mako file is created by replacing the keywords WCONINJE and WCONPROD in the .DATA file with:</p> <pre><code>WCONINJE\n'INJ-1'  WATER 'OPEN' BHP 2* ${injbhp[0]} /\n'INJ-2'  WATER 'OPEN' BHP 2* ${injbhp[1]} /\n/\n\nWCONPROD\n 'PRO-1' 'OPEN' BHP 5* ${prodbhp[0]} /\n/</code></pre>"},{"location":"tutorials/popt/tutorial_popt/#running-locally","title":"Running locally\u00b6","text":"<p>It is recommended to run the notebook from a virtual environment. Follow these steps to run this notebook on your own computer:</p> <p>Step 1: Create virtual environment as normal</p> <pre><code>python3 -m venv pet_venv</code></pre> <p>Then activate the environment using:</p> <pre><code>source pet_venv/bin/activate</code></pre> <p>Step 2: Install Jupyter Notebook into virtual environment</p> <pre><code>python3 -m pip install ipykernel</code></pre> <p>Step 3: Install PET in the virtual environment, see PET installation</p> <p>Step 4: Allow Jupyter access to the kernel within the virtual environment</p> <pre><code>python3 -m ipykernel install --user --name=pet_venv</code></pre> <p>Start jupyter notebook, and load tutorial_popt.ipynb (this file). On the jupyter notebook toolbar, select \u2018Kernel\u2019 and \u2018Change Kernel\u2019. The new kernel is now be available in the list for selection:</p> <p></p>"}]}